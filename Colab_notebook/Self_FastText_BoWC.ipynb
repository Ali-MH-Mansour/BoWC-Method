{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Self_FastText_BoWC.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ali-MH-Mansour/BoWC-Method/blob/main/Colab_notebook/Self_FastText_BoWC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe8E2O_7iyQX"
      },
      "source": [
        "## **Connect Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_I0ZTb-sqxH7",
        "outputId": "65369c64-7ca6-457b-d7d9-4a35a272d5eb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gge2NxpEmGsT"
      },
      "source": [
        "# **Requirements  || Определение библиотек**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kp6vCScYjWfp",
        "outputId": "e6542f8c-2d3a-4bcd-9964-25a948ff460a"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yPskImTLE1Q",
        "outputId": "f2a1ed29-6ccc-4759-9c64-75bcf2ecfbc6"
      },
      "source": [
        "################################ new\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk import word_tokenize, sent_tokenize, pos_tag\n",
        "\n",
        "def final_preprocess_text(text):\n",
        "    # 1. Tokenise to alphabetic tokens\n",
        "    text = remove_numbers(text)\n",
        "    text = remove_punctuation(text)\n",
        "    text = convert_to_lower(text)\n",
        "    text = remove_white_space(text)\n",
        "    text = remove_short_words(text)\n",
        "    tokens = toknizing(text)\n",
        "    #tokeniser = RegexpTokenizer(r'[A-Za-z]+')\n",
        "    #tokens = tokeniser.tokenize(text)\n",
        "    \n",
        "    # 2. POS tagging\n",
        "    pos_map = {'J': 'a', 'N': 'n', 'R': 'r', 'V': 'v'}\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    \n",
        "    # 3. Lowercase and lemmatise \n",
        "    lemmatiser = WordNetLemmatizer()\n",
        "    tokens = [lemmatiser.lemmatize(t.lower(), pos=pos_map.get(p[0], 'v')) for t, p in pos_tags]\n",
        "    return tokens\n",
        "\n",
        "#############\n",
        "#########################################################################\n",
        "def new_pre_processing_dataset(data):\n",
        "  All_tokens = [];\n",
        "  for i in range(len(data)):\n",
        "    All_tokens.append(final_preprocess_text(data.iloc[i][1]))\n",
        "  All_tokens = Convert_list_of_list_to_list(All_tokens)\n",
        "  print('the number of words in Crorpus' , len(All_tokens))\n",
        "\n",
        "  return All_tokens\n",
        "\n",
        "  ###########################################\n",
        "my_stopwords= [\"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\",\"aab\",'aaab', \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"affects\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"ain't\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appear\", \"appreciate\", \"appropriate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"aren't\", \"arise\", \"around\", \"as\", \"a's\", \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"can't\", \"cause\", \"causes\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"changes\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"c'mon\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"could\", \"couldn\", \"couldnt\", \"couldn't\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"c's\", \"ct\", \"cu\", \"currently\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"didn't\", \"different\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"done\", \"don't\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"effect\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"empty\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"h2\", \"h3\", \"had\", \"hadn\", \"hadn't\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"hed\", \"he'd\", \"he'll\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"here's\", \"hereupon\", \"hers\", \"herself\", \"hes\", \"he's\", \"hh\", \"hi\", \"hid\", \"him\", \"himself\", \"his\", \"hither\", \"hj\", \"ho\", \"home\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"how's\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"i'd\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"i'll\", \"im\", \"i'm\", \"immediate\", \"immediately\", \"importance\", \"important\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"invention\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"isn't\", \"it\", \"itd\", \"it'd\", \"it'll\", \"its\", \"it's\", \"itself\", \"iv\", \"i've\", \"ix\", \"iy\", \"iz\", \"j\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"know\", \"known\", \"knows\", \"ko\", \"l\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"let's\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mightn't\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"n\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needn\", \"needn't\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"nothing\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"other\", \"others\", \"otherwise\", \"ou\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"present\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"s2\", \"sa\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shan't\", \"she\", \"shed\", \"she'd\", \"she'll\", \"shes\", \"she's\", \"should\", \"shouldn\", \"shouldn't\", \"should've\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"system\", \"sz\", \"t\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"thats\", \"that's\", \"that've\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"there's\", \"thereto\", \"thereupon\", \"there've\", \"these\", \"they\", \"theyd\", \"they'd\", \"they'll\", \"theyre\", \"they're\", \"they've\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"t's\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"ut\", \"v\", \"va\", \"value\", \"various\", \"vd\", \"ve\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"wa\", \"want\", \"wants\", \"was\", \"wasn\", \"wasnt\", \"wasn't\", \"way\", \"we\", \"wed\", \"we'd\", \"welcome\", \"well\", \"we'll\", \"well-b\", \"went\", \"were\", \"we're\", \"weren\", \"werent\", \"weren't\", \"we've\", \"what\", \"whatever\", \"what'll\", \"whats\", \"what's\", \"when\", \"whence\", \"whenever\", \"when's\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"where's\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whom\", \"whomever\", \"whos\", \"who's\", \"whose\", \"why\", \"why's\", \"wi\", \"widely\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"won't\", \"words\", \"world\", \"would\", \"wouldn\", \"wouldnt\", \"wouldn't\", \"www\", \"x\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"you'd\", \"you'll\", \"your\", \"youre\", \"you're\", \"yours\", \"yourself\", \"yourselves\", \"you've\", \"yr\", \"ys\", \"yt\", \"z\", \"zero\", \"zi\", \"zz\"]\n",
        "\n",
        "!pip install sklearn\n",
        "!pip install spherecluster\n",
        "!pip install soyclustering\n",
        "#################################### Importing all Library here  ################################################\n",
        "from soyclustering import SphericalKMeans\n",
        "import time\n",
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from numpy import array \n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords \n",
        "from numpy import array\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import gensim.downloader\n",
        "from collections import Counter \n",
        "import functools\n",
        "import math\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.sparse import csr_matrix\n",
        "#from sklearn_extra.cluster import KMedoids\n",
        "#from spherecluster import SphericalKMeans\n",
        "############ Our Downloads ##########\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "##################################\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "import numpy as np\n",
        "np.random.seed(2018)\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "############################################\n",
        "#machine learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "##################################  Defining Our Methods Here    ##################################################\n",
        "# Save the pre_trained model embeddings.. \n",
        "def save_embeddings(model, file_path):\n",
        "  word_vectors = model.wv\n",
        "  word_vectors.save(file_path)\n",
        "####################################################################################\n",
        "### loading the embeddings\n",
        "def Load_pre_trained_model_embeddings(file_path):\n",
        "  wv = KeyedVectors.load(file_path, mmap='r')\n",
        "  return wv\n",
        "####################################################################################\n",
        "### install pre_trained_model\n",
        "def intall_pre_trained_model(file_path, model_name):\n",
        "  model = gensim.downloader.load(model_name)\n",
        "  save_embeddings(model , file_path)\n",
        "  return model\n",
        "  print('the models are intalled and saved in the \" ' + file_path + '\"')\n",
        "####################################################################################\n",
        "def convert_to_lower(text):\n",
        "  return text.lower()\n",
        "####################################################################################\n",
        "def remove_numbers(text):\n",
        "  text = re.sub(r'\\d+' , '', text)\n",
        "  return text\n",
        "#################################################\n",
        "def remove_short_words(text):\n",
        "  text = re.sub(r'\\b\\w{1,2}\\b', '', text)\n",
        "  return text\n",
        "####################################################################################\n",
        "def remove_punctuation(text):\n",
        "     punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^+&*_~'''\n",
        "     no_punct = \"\"\n",
        "     for char in text:\n",
        "        if char not in punctuations:\n",
        "            no_punct = no_punct + char\n",
        "     return no_punct\n",
        "####################################################################################\n",
        "def remove_white_space(text):\n",
        "  text = text.strip()\n",
        "  return text\n",
        "####################################################################################\n",
        "def toknizing(text):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = word_tokenize(text)\n",
        "  ## Remove Stopwords from tokens\n",
        "  result = [i for i in tokens if not i in stop_words]\n",
        "  return result\n",
        "###################################################################################\n",
        "def remove_duplication(text):\n",
        "  return list(set(text)) \n",
        "####################################################################################\n",
        "def pre_processing(text):\n",
        "  text = remove_numbers(text)\n",
        "  text = remove_punctuation(text)\n",
        "  text = convert_to_lower(text)\n",
        "  text = remove_white_space(text)\n",
        "  text = remove_short_words(text)\n",
        "  #text = preprocess_toklem(text)\n",
        "  text = toknizing(text)\n",
        "  return text \n",
        "#######################################\n",
        "def pre_processing_no_toknize(text):\n",
        "  text = remove_numbers(text)\n",
        "  text = remove_punctuation(text)\n",
        "  text = convert_to_lower(text)\n",
        "  text = remove_white_space(text)\n",
        "  text = remove_short_words(text)\n",
        "  #text = preprocess_toklem(text)\n",
        "  #text = toknizing(text)\n",
        "  return text\n",
        "####################################################################################\n",
        "def lemmatize_stemming(text):\n",
        "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
        "#######\n",
        "def preprocess_toklem(text):\n",
        "    result = []\n",
        "    for token in gensim.utils.simple_preprocess(text):\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) >= 3:\n",
        "            result.append(lemmatize_stemming(token))\n",
        "    return result\n",
        "\n",
        "def plot_pie_distribution(labels , metric, name):\n",
        "      # Create figure and plot pie:\n",
        "      fig = plt.figure(figsize=(4,4))\n",
        "      colors = ['skyblue', 'peru', 'gray' , 'red' , 'blue']\n",
        "      plt.pie(metric, labels = labels , autopct='%d%%', colors=colors)\n",
        "      plt.axis('equal')\n",
        "      plt.title(name + ' Distribution', fontsize='20')\n",
        "      plt.savefig('plot_eight.png')\n",
        "      plt.show()\n",
        "########################\n",
        "#stem single token\n",
        "def stem_token(alltoken):\n",
        "    result = []\n",
        "    for token in alltoken:\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS:\n",
        "            result.append(lemmatize_stemming(token))\n",
        "    return result\n",
        "####################################################################################\n",
        "def Convert_list_of_list_to_list(all_tokens):\n",
        "  tokens = []\n",
        "  for words in all_tokens:\n",
        "    for i in words:\n",
        "      tokens.append(i)\n",
        "  return tokens\n",
        "#########################################################3\n",
        "def pre_processing_dataset(data):\n",
        "  All_tokens = [];\n",
        "  for i in range(len(data)):\n",
        "    All_tokens.append(pre_processing(data.iloc[i][1]))\n",
        "  All_tokens = Convert_list_of_list_to_list(All_tokens)\n",
        "  print('the number of words in Crorpus' , len(All_tokens))\n",
        "\n",
        "  return All_tokens\n",
        "\n",
        "#########################################################################################\n",
        "def Create_embedding_matrix(model , tokens , embedding_dim):\n",
        "    embedding_matrix = []\n",
        "    irregular_words  = []\n",
        "    for words in tokens:\n",
        "          try :\n",
        "            temp = model.wv[words]\n",
        "            embedding_matrix.append(temp)\n",
        "          except KeyError:\n",
        "            # for the words that are not in model vocabulary\n",
        "            # we initilize with random vector\n",
        "            irregular_words.append(words)\n",
        "            #embedding_matrix.append(np.random.normal(0,np.sqrt(0.300),embedding_dim))\n",
        "    return embedding_matrix , irregular_words\n",
        "\n",
        "#################################################################################\n",
        "def Create_embeddingTFIDF_matrix(model , tokens , embedding_dim, df_tfidf):\n",
        "    embedding_matrix = []\n",
        "    irregular_words  = []\n",
        "    words_tfidf =[]\n",
        "    for word in tokens:\n",
        "          try :\n",
        "            temp = model.wv[word]\n",
        "            embedding_matrix.append(temp)\n",
        "            words_tfidf.append(max(df_tfidf[word]))\n",
        "          except KeyError:\n",
        "            # for the words that are not in model vocabulary\n",
        "            irregular_words.append(word)\n",
        "    return embedding_matrix , irregular_words, words_tfidf\n",
        "\n",
        "###########################################################################################\n",
        "# K-means Clustering the embeddings #\n",
        "def embeddings_k_means_clustering(embeddings_vectors, N_clusters):\n",
        "  #kmedoids = KMedoids(n_clusters=5, random_state=0).fit(embeddings_vectors)\n",
        "  #SphericalKMeans\n",
        "  kmeans = KMeans(n_clusters= N_clusters , random_state=0).fit(embeddings_vectors)\n",
        "  return kmeans\n",
        "\n",
        "\n",
        "########################################################################################\n",
        "def spherical_kmeans(n_clusters_, embeddings_matrix,sim_digree):\n",
        "  embeddings_matrix_csr = csr_matrix(embeddings_matrix)\n",
        "  spherical_kmeans = SphericalKMeans( max_similar=sim_digree, init='similar_cut', \n",
        "      n_clusters = n_clusters_)\n",
        "  labels = spherical_kmeans.fit_predict(embeddings_matrix_csr)\n",
        "  centers = spherical_kmeans.cluster_centers_\n",
        "  return labels , centers\n",
        "\n",
        "############################################################################################\n",
        "def dimentionality_reduction(embeddings_vectors):\n",
        "  pca = PCA(3).fit(embeddings_vectors)\n",
        "  pca_data = pd.DataFrame(pca.transform(embeddings_vectors)) \n",
        "  return pca_data\n",
        "\n",
        "#############################################################################################\n",
        "def clusters_visualization( _3D_data , labels):\n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_subplot(111, projection='3d')\n",
        "  ax.scatter(_3D_data[0], _3D_data[1], _3D_data[2], c = labels,  s=50, cmap='viridis')\n",
        "  centers = k_means.cluster_centers_\n",
        "  ax.scatter(centers[0], centers[1], centers[2] ,  c='red', s=200, alpha=0.8);\n",
        "  ax.set_xlabel('X Label')\n",
        "  ax.set_ylabel('Y Label')\n",
        "  ax.set_zlabel('Z Label')\n",
        "  plt.show()\n",
        "#############################################################################################\n",
        "def clusters_properties(k_means):\n",
        "  print(Counter(k_means.labels_))\n",
        "  centers = k_means.cluster_centers_\n",
        "  '''\n",
        "  Return\n",
        "     - Number of samples for each cluter ,\n",
        "     - The Centers of each cluster\n",
        "  '''\n",
        "  return Counter(k_means.labels_) , centers\n",
        "#############################################################################################\n",
        "def Save_words_with_Embeddings(tokens , pre_model):\n",
        "  dic = {}\n",
        "  for words in tokens: \n",
        "      try :\n",
        "        dic.update({words : pre_model.wv[words]})\n",
        "      except: \n",
        "        continue;\n",
        "  return dic\n",
        "##############################################################################################\n",
        "### this function take the vector and return the word that linked with it.\n",
        "def Vector_to_Word(vector , dic):\n",
        "  word = ''\n",
        "  for i in dic:\n",
        "    if functools.reduce(lambda x, y : x and y, map(lambda p, q: p == q,list(dic[i]),vector), True): \n",
        "      word = i\n",
        "  return word\n",
        "##############################################################################################\n",
        "### get list of list array, where each list contain the groub of element the belong to specific cluster\n",
        "### C * n_sample for each cluster\n",
        "\n",
        "# updated\n",
        "def get_cluster_elements(labels , n_clusters , embedding_matrix):\n",
        "  # Nice Pythonic way to get the indices of the points for each corresponding cluster\n",
        "  mydict = {i: np.where(labels == i)[0] for i in range(n_clusters)}\n",
        "  embeddings_samples = []\n",
        "  for i in range(len(mydict)):\n",
        "    temp= []\n",
        "    for j in mydict[i]:\n",
        "      temp.append(embedding_matrix[j])\n",
        "    embeddings_samples.append(temp)\n",
        "    del temp\n",
        "  return embeddings_samples\n",
        "\n",
        "############################################################################################\n",
        "def get_soreted_dictionary(centers , dictionary , M):\n",
        "  dic_with_similarity =[]\n",
        "  for i in range(len(dictionary)):\n",
        "    temp = []\n",
        "    for j in range(len(dictionary[i])):\n",
        "      temp.append([dictionary[i][j] , cosine_similarity([dictionary[i][j]] , [centers[i]])])\n",
        "    dic_with_similarity.append(temp)\n",
        "    del temp\n",
        "      ####### Sort all vectors clusters ############\n",
        "  sorted_list = []\n",
        "  def take_second(elem):\n",
        "      return elem[1]\n",
        "  for i in range(len(dic_with_similarity)):\n",
        "      sorted_list.append(sorted(dic_with_similarity[i], key=take_second , reverse = True))\n",
        "    ########### Get the hightest M features that related to the its centers among whole concepet dictionary#############\n",
        "  Top_M_Concept = [sorted_list[i][:M] for i in range(len(dic_with_similarity))]\n",
        "  return Top_M_Concept\n",
        "\n",
        "############################################################################################\n",
        "# compare each words in documents with all clusters words.\n",
        "def Get_similarity_matrix(v1 , list_of_vectors):\n",
        "  sim = []\n",
        "  s=0\n",
        "  for i in list_of_vectors:\n",
        "    sim.append(cosine_distance(v1, i[0]))\n",
        "  sim = array(sim).astype('float').reshape(-1).tolist()\n",
        "  if len(sim)!=0:\n",
        "    s=sum(sim)/len(sim)\n",
        "  return s\n",
        "\n",
        "### convert document to Vector with size C * M \n",
        "##############################################################################################\n",
        "##############################################################################################\n",
        "def Doc2Vec(embeddeings_document,threshold_sim,alpha,centers,dictionary):\n",
        "  M=2\n",
        "  N=10\n",
        "  concept_freq = 0\n",
        "  ## intilize doc array\n",
        "  doc2vec = np.zeros((len(centers), M)).tolist()\n",
        "  final_vector=np.zeros(len(centers)).tolist()\n",
        "  for cnt in range(len(centers)):\n",
        "    counter = 0;\n",
        "    sim =[]\n",
        "    NN=0\n",
        "    for emb_word in embeddeings_document:\n",
        "      Wsim =cosine_distance(emb_word, centers[cnt])# التشابه مع اقرب كلمة للمركز\n",
        "      if Wsim  > alpha and NN<=N:\n",
        "        NN=NN+1\n",
        "        concept_freq = concept_freq+1 #for tf\n",
        "        Smean =Get_similarity_matrix(emb_word , dictionary[cnt][0:threshold_sim-1]);\n",
        "        sim.append(float((Smean+Wsim)/2))\n",
        "        doc2vec[cnt][M-1] = doc2vec[cnt][M-1] + 1;\n",
        "      elif Wsim  > alpha and NN>N:\n",
        "        concept_freq = concept_freq+1 #for tf\n",
        "        sim.append(Wsim)\n",
        "        doc2vec[cnt][M-1] = doc2vec[cnt][M-1] + 1;\n",
        "      else:\n",
        "        continue;\n",
        "      counter = counter +1;\n",
        "    # end for doc words with one concepts\n",
        "    meanofSim=0#  المتوسط لدرجات تشابه كلمات المستند مع المفهوم الحالي\n",
        "    if(len(sim)!=0):\n",
        "      meanofSim=sum(sim)/len(sim)\n",
        "    doc2vec[cnt][M-2]=meanofSim\n",
        "    del sim\n",
        "#* math.exp(i[M-2])\n",
        "  if concept_freq !=0: # the number of occurence for all concept in this document\n",
        "    for i in doc2vec:\n",
        "      #print(i[M-1], concept_freq, 'sim',i[M-2],'exp',math.exp(i[M-2]))\n",
        "      i[M-1] = i[M-1]* math.exp(i[M-2]) / concept_freq\n",
        "      #print(i[M-1])\n",
        "  doc2vec1=np.array(doc2vec,dtype=object)\n",
        "  final_vector=doc2vec1[:,1].tolist() \n",
        "\n",
        "  return final_vector\n",
        "\n",
        "\n",
        "#############################\n",
        "def Doc2VecTFIDF(embeddeings_tfidf,threshold_sim,alpha,centers,dictionary):\n",
        "  M=2\n",
        "  N=10\n",
        "  concept_freq = 0\n",
        "  ## intilize doc array\n",
        "  doc2vec = np.zeros((len(centers), M)).tolist()\n",
        "  final_vector=np.zeros(len(centers)).tolist()\n",
        "  for cnt in range(len(centers)):\n",
        "    counter = 0;\n",
        "    sim =[]\n",
        "    concept_tf_idf=[]\n",
        "    NN=0\n",
        "    for emb_word in embeddeings_tfidf:    \n",
        "      #emb_word هذه تحتوي تضمين ووزن تفايدف    \n",
        "      #emb_word[0] embedding\n",
        "      #emb_word[1] tfidf       \n",
        "      Wsim =cosine_distance(emb_word[0], dictionary[cnt][0][0])# التشابه مع اقرب كلمة للمركز\n",
        "      if Wsim  > alpha and NN<=N:\n",
        "        NN=NN+1\n",
        "        concept_freq = concept_freq+1 #for tf\n",
        "        #Smean =Get_similarity_matrix(emb_word , dictionary[cnt][1:threshold_sim-1]);\n",
        "        #sim.append(float((Smean+Wsim)/2))\n",
        "        #new\n",
        "        concept_tf_idf.append(emb_word[1])\n",
        "        doc2vec[cnt][M-1] = doc2vec[cnt][M-1] + 1;\n",
        "      elif Wsim  > alpha and NN>N:\n",
        "        concept_freq = concept_freq+1 #for tf\n",
        "        #sim.append(Wsim)\n",
        "        concept_tf_idf.append(emb_word[1])\n",
        "        doc2vec[cnt][M-1] = doc2vec[cnt][M-1] + 1;\n",
        "      else:\n",
        "        continue;\n",
        "      counter = counter +1;\n",
        "    # end for doc words with one concepts\n",
        "    #meanofSim=0#  المتوسط لدرجات تشابه كلمات المستند مع المفهوم الحالي\n",
        "    #if(len(sim)!=0):\n",
        "     # meanofSim=sum(sim)/len(sim)\n",
        "    #mean_of_tf_idf=0\n",
        "    if len(concept_tf_idf)>0:\n",
        "      mean_of_tf_idf=sum(concept_tf_idf)/len(concept_tf_idf)\n",
        "    #doc2vec[cnt][M-2]=meanofSim\n",
        "    doc2vec[cnt][M-2] = mean_of_tf_idf\n",
        "    del concept_tf_idf\n",
        "    #del sim\n",
        "#* math.exp(i[M-2])\n",
        "  if concept_freq !=0: # the number of occurence for all concept in this document\n",
        "    for i in doc2vec:\n",
        "      i[M-1] = i[M-2]  # خزن محل تف متوسط اوزان تفات كلمات المفهوم\n",
        "      #i[M-1] = i[M-1] * math.exp(i[M-2]) / concept_freq   #calculte tf\n",
        "\n",
        "      \n",
        "  doc2vec1=np.array(doc2vec,dtype=object)\n",
        "  final_vector=doc2vec1[:,1].tolist() \n",
        "\n",
        "  return final_vector\n",
        "#################################\n",
        "def Doc2Vec2D(embeddeings_document,threshold_sim,alpha,centers,dictionary):\n",
        "  M=2\n",
        "  concept_freq = 0\n",
        "  ## intilize doc array\n",
        "  doc2vec = np.zeros((len(centers), M)).tolist()\n",
        "  final_vector=np.zeros(len(centers)).tolist()\n",
        "  for cnt in range(len(centers)):\n",
        "    counter = 0;\n",
        "    sim =[]\n",
        "    for emb_word in embeddeings_document:\n",
        "      Smean =Get_similarity_matrix(emb_word , dictionary[cnt][0:threshold_sim-1]);\n",
        "      if Smean  > alpha:\n",
        "        concept_freq = concept_freq+1 #for tf\n",
        "        sim.append(float(Smean))\n",
        "        doc2vec[cnt][M-1] = doc2vec[cnt][M-1] + 1;\n",
        "      else:\n",
        "        continue;\n",
        "      counter = counter +1;\n",
        "    # end for doc words with one concepts\n",
        "    meanofSim=0#  المتوسط لدرجات تشابه كلمات المستند مع المفهوم الحالي\n",
        "    if(len(sim)!=0):\n",
        "      meanofSim=sum(sim)/len(sim)\n",
        "    doc2vec[cnt][M-2]=meanofSim\n",
        "    del sim\n",
        "#* math.exp(i[M-2])\n",
        "  if concept_freq !=0: # the number of occurence for all concept in this document\n",
        "    for i in doc2vec:\n",
        "      #print(i[M-1], concept_freq, 'sim',i[M-2],'exp',math.exp(i[M-2]))\n",
        "      i[M-1] = i[M-1] / concept_freq\n",
        "      #print(i[M-1])\n",
        "  doc2vec1=np.array(doc2vec)\n",
        "  final_vector=doc2vec1[:,1].tolist() \n",
        "\n",
        "  return doc2vec\n",
        "  #######################\n",
        "def DDDoc2Vec(embeddeings_document,threshold_sim,alpha,centers,dictionary):\n",
        "  M=2\n",
        "  concept_freq = 0\n",
        "  ## intilize doc array\n",
        "  doc2vec = np.zeros((len(centers), M)).tolist()\n",
        "  final_vector=np.zeros(len(centers)).tolist()\n",
        "  for cnt in range(len(centers)):\n",
        "    counter = 0;\n",
        "    sim =[]\n",
        "    for emb_word in embeddeings_document:\n",
        "      Wsim1 =cosine_similarity([emb_word], [dictionary[cnt][0][0]])\n",
        "      Wsim2 =cosine_similarity([emb_word], [dictionary[cnt][1][0]])\n",
        "      Wsim3 =cosine_similarity([emb_word], [dictionary[cnt][2][0]])\n",
        "      Smean=float((Wsim1+Wsim2+Wsim3)/3)\n",
        "      #print('Smean', Smean)\n",
        "      if Smean  > alpha:\n",
        "        concept_freq = concept_freq+1 #for tf\n",
        "        sim.append(Smean)\n",
        "        doc2vec[cnt][M-1] = doc2vec[cnt][M-1] + 1;\n",
        "      else:\n",
        "        continue;\n",
        "      counter = counter +1;\n",
        "    # end for doc words with one concepts\n",
        "    meanofSim=0#  المتوسط لدرجات تشابه كلمات المستند مع المفهوم الحالي\n",
        "    lsim=len(sim)\n",
        "    if lsim>0:\n",
        "      meanofSim=sum(sim)/lsim\n",
        "    #print('Mean of sum',meanofSim,'sim',len(sim),doc2vec[cnt][M-1])\n",
        "    doc2vec[cnt][M-2]=meanofSim\n",
        "    del sim\n",
        "\n",
        "  if concept_freq !=0: # the number of occurence for all concept in this document\n",
        "    for i in doc2vec:\n",
        "      #print(i[M-1], concept_freq, 'sim',i[M-2],'exp',math.exp(i[M-2]))\n",
        "      i[M-1] = i[M-1]* math.exp(i[M-2]) / concept_freq\n",
        "  doc2vec1=np.array(doc2vec)\n",
        "  final_vector=doc2vec1[:,1].tolist() \n",
        "\n",
        "  return final_vector\n",
        "#####################################################################################\n",
        "\n",
        "\n",
        "\n",
        "#######################################################################################\n",
        "## convert vectors to words for each cluster (M=10 words)\n",
        "# K number of clsuters\n",
        "def dictAsText(K,M,sordted_concepts,dictionary):\n",
        "  textclusters = np.zeros((K, M)).tolist()\n",
        "  for i in range(K):\n",
        "    num_of_words=len(sorted_cons[i][0:M])\n",
        "    for j in range(num_of_words):\n",
        "      textclusters[i][j]=Vector_to_Word(sordted_concepts[i][j][0],dictionary)\n",
        "  return textclusters\n",
        "\n",
        "#######################################\n",
        "# WMD we need to use whole cluster not only 10 words\n",
        "# dictionarAsstring as input to Doc2WMD\n",
        "\n",
        "def Doc2WMD(model,text_document,embeddeings_document , M , alpha , centers , textclusters,dic):\n",
        "  #doc array\n",
        "  doc2WMD_only = np.zeros(len(centers)).tolist()\n",
        "  doc2WMD_with_Concept = np.zeros(( len(centers), M)).tolist()\n",
        "  counter=0;\n",
        "  #start for conepts\n",
        "  for cnt in range(len(centers)):\n",
        "    for emb_word in embeddeings_document:\n",
        "      s=cosine_similarity([emb_word] , [centers[cnt]])\n",
        "      if ((s  > alpha)):\n",
        "        doc2WMD_with_Concept[cnt][M-1] = doc2WMD_with_Concept[cnt][M-1] + 1 #frequency++\n",
        "      else:\n",
        "        continue;\n",
        "    counter=counter+1;\n",
        "    #end for \n",
        "    # get WMD ((words) dictionary[cnt])\n",
        "    doc1=text_document\n",
        "    doc2 = ' '.join(textclusters[cnt])\n",
        "    #print('counter', counter ,'doc1' ,doc1,' \\n doc2', doc2 ,'\\n')\n",
        "    distance = model.wmdistance(doc1.split(), doc2.split())\n",
        "    doc2WMD_with_Concept[cnt][M-2]= distance\n",
        "    doc2WMD_only[cnt]=distance\n",
        "    #empty strings\n",
        "    doc2=\"\"\n",
        "  #claculate tf-idf\n",
        "  get_concept_freq(doc2WMD_with_Concept)\n",
        "  # normalize vector\n",
        "  WMD_only,bysum=normalize_vector(doc2WMD_only)\n",
        "  doc2WMD_with_Concept=normalize_coloumn_in_2Darray(doc2WMD_with_Concept, 0)\n",
        "  return doc2WMD_with_Concept,WMD_only\n",
        "\n",
        "###################################################################\n",
        "def cosine_distance(u, v):\n",
        "  return np.dot(u, v) / (math.sqrt(np.dot(u, u)) * math.sqrt(np.dot(v, v)))\n",
        "\n",
        "#####################################################################################\n",
        "#TF-idf\n",
        "def get_concept_freq(array2D):\n",
        "  s=0;\n",
        "  for row in array2D:            \n",
        "    s=s+row[len(row)-1]\n",
        "  if s !=0:\n",
        "    for row in array2D:\n",
        "      row[len(row)-1]=row[len(row)-1]/s\n",
        "  return array2D\n",
        "#########################################33\n",
        "def normalize_vector(vect):\n",
        "  bymax=[float(i)/max(vect) for i in vect]\n",
        "  bysum=[float(i)/sum(vect) for i in vect]\n",
        "  return bymax,bysum\n",
        "\n",
        "#####################################################\n",
        "def normalize_coloumn_in_2Darray(array2D, index_column):\n",
        "  arr=np.array(array2D)\n",
        "  list_col=arr[:,index_column]\n",
        "  s=max(list_col) # or sum\n",
        "  #print(' col ', list_col , ' max ' , s)\n",
        "  if s !=0:\n",
        "    for row in array2D:\n",
        "      row[index_column]=float(row[index_column])/s\n",
        "  return array2D\n",
        "\n",
        "\n",
        "####################################################################\n",
        "\n",
        "\n",
        "####################################################33333\n",
        "def self_embedding_all_documents(df , model,N):\n",
        "  documents_embeddings = []\n",
        "  for i in range(len(df)):\n",
        "    documents_embeddings.append(Self_convert_doc_to_embedding(df['document'][i] , model,N))\n",
        "  return documents_embeddings\n",
        "\n",
        "########################################################################################################\n",
        "def Self_convert_doc_to_embedding(text , model,N):\n",
        "  tokens = final_preprocess_text(text)\n",
        "  if N!=0:\n",
        "    tokens=removeElements(tokens,N)\n",
        "  embeddings_matrix  , irrugular_words = Create_embedding_matrix(model , tokens , 300)\n",
        "  return embeddings_matrix\n",
        "#########################################################################################################\n",
        "####################################################33333\n",
        "def embedding_all_documents(df , model,N):\n",
        "  documents_embeddings = []\n",
        "  for i in range(len(df)):\n",
        "    documents_embeddings.append(convert_doc_to_embedding(df['document'][i] , model,N))\n",
        "  return documents_embeddings\n",
        "\n",
        "########################################################################################################\n",
        "def convert_doc_to_embedding(text , model,N):\n",
        "  tokens = pre_processing(text)\n",
        "  if N!=0:\n",
        "    tokens=removeElements(tokens,N)\n",
        "  embeddings_matrix  , irrugular_words = Fast_Create_embedding_matrix(model , tokens , 300)\n",
        "  return embeddings_matrix\n",
        "#########################################################################################################\n",
        "def RepresentationNew(documents_embeddings , sim_val , alpha , centers , dictionary):\n",
        "  embeddings = []\n",
        "  for i in documents_embeddings: \n",
        "    v = Doc2Vec(i ,sim_val , alpha , centers , dictionary)\n",
        "    embeddings.append(v)\n",
        "  return embeddings\n",
        "##########################################\n",
        "\n",
        "def calculate_IDF(doc_vectors):\n",
        "  docL=len(doc_vectors[0])\n",
        "  N=len(doc_vectors) #collection size\n",
        "  IDf = np.zeros(docL).tolist() # بطول العناقيد\n",
        "  for doc in doc_vectors:\n",
        "    counter=0 # counting concepts\n",
        "    for concept_freature in doc:\n",
        "      if concept_freature > 0:\n",
        "        IDf[counter] = IDf[counter] + 1 #  i[M-1]\n",
        "      counter = counter+1\n",
        "  \n",
        "  #ضرب حساب التردد\n",
        "  newIDF=[]\n",
        "  for i in IDf:\n",
        "    if i!=0:\n",
        "      newIDF.append( math.exp(-1*(i/N)))\n",
        "    else:\n",
        "      newIDF.append(0)\n",
        "  return IDf, newIDF\n",
        "\n",
        "########################################### \n",
        "#Calculate Cf-IDF\n",
        "def CF_IDF(doc_vectors,IDF):\n",
        "  docL=len(doc_vectors[0])\n",
        "  for i in range(len(doc_vectors)):\n",
        "    for j in range(docL):\n",
        "       doc_vectors[i][j] = doc_vectors[i][j] * IDF[j]\n",
        "  return doc_vectors\n",
        "\n",
        "#################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################333\n",
        "def IDFbySimNoLog(documents_embeddings , sim_val , alpha , centers , dictionary):\n",
        "  embeddings = []\n",
        "  counter = 0\n",
        "  IDf = np.zeros(len(centers)).tolist() # بطول العناقيد\n",
        "  for i in documents_embeddings: \n",
        "    v = Doc2Vec2D(i ,sim_val , alpha , centers , dictionary)\n",
        "    ######## حساب عدد مرات ظهور المفهوم في كل نص\n",
        "    for i in v:   # i is concept line , i need class of doc v\n",
        "      if i > 0:\n",
        "        IDf[counter] = IDf[counter] + 1 #  i[M-1]\n",
        "      counter = counter+1\n",
        "    counter = 0\n",
        "    embeddings.append(v)\n",
        "  print(IDf)\n",
        "  ######## Calculate Cf-IDF\n",
        "  print(len(embeddings))\n",
        "  for i in range(len(embeddings)):\n",
        "    l=len(embeddings[i])\n",
        "    for j in range(l):\n",
        "    #for doc i: muti cell j with idf j \n",
        "       #print('i', i,' j ', j, ' = ', embeddings[i][j] * newIDF[j])\n",
        "       embeddings[i][j] = embeddings[i][j] * IDf[j]\n",
        "  \n",
        "  return embeddings\n",
        "#########################################################################################################\n",
        "\n",
        "def RepresentationNormNoLog(documents_embeddings , sim_val , alpha , centers , dictionary):\n",
        "  embeddings = []\n",
        "  counter = 0\n",
        "  IDf = np.zeros(len(centers)).tolist() # بطول العناقيد\n",
        "  for i in documents_embeddings: \n",
        "    v = Doc2Vec(i ,sim_val , alpha , centers , dictionary)\n",
        "    ######## حساب عدد مرات ظهور المفهوم في كل نص\n",
        "    for i in v:   # i is concept line , i need class of doc v\n",
        "      if i > 0:\n",
        "        IDf[counter] = IDf[counter] + 1 #  i[M-1]\n",
        "      counter = counter+1\n",
        "    counter = 0\n",
        "    embeddings.append(v)\n",
        "  print(IDf)\n",
        "  ######## Calculate Cf-IDF\n",
        "  print(len(embeddings))\n",
        "  for i in range(len(embeddings)):\n",
        "    l=len(embeddings[i])\n",
        "    for j in range(l):\n",
        "    #for doc i: muti cell j with idf j \n",
        "       #print('i', i,' j ', j, ' = ', embeddings[i][j] * newIDF[j])\n",
        "       embeddings[i][j] = embeddings[i][j] * IDf[j]\n",
        "  \n",
        "  return embeddings\n",
        "###############\n",
        "\n",
        "  ##################################3\n",
        "\n",
        "def RepresentationAccNoLog(documents_embeddings , sim_val , alpha , centers , dictionary):\n",
        "  embeddings = []\n",
        "  counter = 0\n",
        "  IDf = np.zeros(len(centers)).tolist() # بطول العناقيد\n",
        "  for i in documents_embeddings: \n",
        "    v = Doc2Vec(i ,sim_val , alpha , centers , dictionary)\n",
        "    ######## حساب عدد مرات ظهور المفهوم في كل نص\n",
        "    for i in v:   # i is concept line , i need class of doc v\n",
        "      if i > 0:\n",
        "        IDf[counter] = IDf[counter] + i #  i[M-1]\n",
        "      counter = counter+1\n",
        "    counter = 0\n",
        "    embeddings.append(v)\n",
        "  \n",
        "  ######## Calculate Cf-IDF\n",
        "  print(len(embeddings))\n",
        "  for i in range(len(embeddings)):\n",
        "    l=len(embeddings[i])\n",
        "    for j in range(l):\n",
        "    #for doc i: muti cell j with idf j \n",
        "       #print('i', i,' j ', j, ' = ', embeddings[i][j] * newIDF[j])\n",
        "       embeddings[i][j] = embeddings[i][j] * IDf[j]\n",
        "  \n",
        "  return embeddings\n",
        "###########################################\n",
        "\n",
        "def RepresentationAccLog(documents_embeddings , sim_val , alpha , centers , dictionary):\n",
        "  embeddings = []\n",
        "  counter = 0\n",
        "  IDf = np.zeros(len(centers)).tolist() # بطول العناقيد\n",
        "  for i in documents_embeddings: \n",
        "    v = Doc2Vec(i ,sim_val , alpha , centers , dictionary)\n",
        "    ######## حساب عدد مرات ظهور المفهوم في كل نص\n",
        "    for i in v:   # i is concept line , i need class of doc v\n",
        "      if i > 0:\n",
        "        IDf[counter] = IDf[counter] + i #  i[M-1]\n",
        "      counter = counter+1\n",
        "    counter = 0\n",
        "    embeddings.append(v)\n",
        "  #print('oldidf',IDf,'\\n')\n",
        "  #########  calculate IDF\n",
        "  #newIDF=[]\n",
        "  for i in IDf:\n",
        "    if i!=0:\n",
        "      N=len(embeddings)\n",
        "      newIDF.append( math.log10( (N+1) /i ) )\n",
        "    else:\n",
        "      newIDF.append(0)\n",
        "  #print('newidf',len(newIDF),'\\n')\n",
        "  \n",
        "  ######## Calculate Cf-IDF\n",
        "  print(len(embeddings))\n",
        "  for i in range(len(embeddings)):\n",
        "    l=len(embeddings[i])\n",
        "    for j in range(l):\n",
        "    #for doc i: muti cell j with idf j \n",
        "       #print('i', i,' j ', j, ' = ', embeddings[i][j] * newIDF[j])\n",
        "       embeddings[i][j] = embeddings[i][j] * newIDF[j]\n",
        "  \n",
        "  return embeddings\n",
        "\n",
        "  #######################################\n",
        "def saveList(myList,filename):\n",
        "    # the filename should mention the extension 'npy'\n",
        "    np.save(filename,myList)\n",
        "    print(\"Saved successfully!\")\n",
        "def loadList(filename):\n",
        "    # the filename should mention the extension 'npy'\n",
        "    tempNumpyArray=np.load(filename)\n",
        "    return tempNumpyArray.tolist()\n",
        "\n",
        "###############################################\n",
        "#######################################################\n",
        "# number of doc in each class \n",
        "def DataBalance(data):\n",
        "  data['class'] = pd.factorize(data['class'])[0]\n",
        "  uniqueValues = data['class'].unique()\n",
        "  l=len(uniqueValues)\n",
        "  labels_freq = np.zeros(l).tolist()\n",
        "  for i in range(len(data)):\n",
        "    label= data.iloc[i][0]\n",
        "    labels_freq[label]=labels_freq[label]+1\n",
        "  return labels_freq \n",
        "\n",
        "#BBC_data['vector']=BBC_data['text'].apply(BoET,args=(model,centers , sorted_cons,0.44,2,10))\n",
        "\n",
        "\n",
        "###############################\n",
        "def document_class_of_concept(orginalDF,data_DF, doc_vectors): \n",
        "  #number of docs in each class\n",
        "  doc_in_class=DataBalance(orginalDF)\n",
        "  # number of classes\n",
        "  L_labels=len(doc_in_class)\n",
        "  K_clusters=len(doc_vectors[0])\n",
        "  #array for freq  \n",
        "  freq_array = np.zeros((L_labels, K_clusters)).tolist() \n",
        "  i=0 # doc counter\n",
        "  for d in doc_vectors:\n",
        "    curren_class=data_DF['class'][i]\n",
        "    counter=0 #idf counter within doc\n",
        "    for concept_feature in d:\n",
        "      if concept_feature > 0:\n",
        "        freq_array[curren_class][counter]=freq_array[curren_class][counter]+1\n",
        "      counter=counter+1\n",
        "    i=i+1\n",
        "  arr=np.array(freq_array)\n",
        "  #نسبة عدد مستندات مفهوم ما من فئة ما على عدد مستندات الفئة\n",
        "  for i in range(L_labels):\n",
        "    arr[i][:]= arr[i][:]/doc_in_class[i]\n",
        "  return arr\n",
        "####################################\n",
        "def class_weighting(saved_embaddings,data_DF,freqarray):\n",
        "  lv=len(saved_embaddings)\n",
        "  K_clusters=len(saved_embaddings[0])\n",
        "  for i in range(lv):\n",
        "    curren_class=data_DF['class'][i]\n",
        "    for j in range(K_clusters):\n",
        "      saved_embaddings[i][j] = float(saved_embaddings[i][j])*freqarray[curren_class][j]\n",
        "  return saved_embaddings\n",
        "\n",
        "############################################\n",
        "from collections import Counter \n",
        "def removeElements(lst, k): \n",
        "    counted = Counter(lst) \n",
        "    temp_lst = [] \n",
        "    for el in counted: \n",
        "        if counted[el] < k: \n",
        "            temp_lst.append(el) \n",
        "    res_lst = [] \n",
        "    for el in lst: \n",
        "        if el not in temp_lst: \n",
        "            res_lst.append(el) \n",
        "              \n",
        "    return(res_lst)\n",
        "\n",
        "######################\n",
        "from collections import Counter\n",
        "\n",
        "def rare_words(text):\n",
        "\t\"\"\"\n",
        "\tReturn :- Most Rare words\n",
        "\tInput :- string\n",
        "\tOutput :- list of rare words\n",
        "\t\"\"\"\n",
        "\t# tokenization\n",
        "\ttokens = word_tokenize(text)\n",
        "\tfor word in tokens:\n",
        "\t\tcounter[word]= +1\n",
        "\n",
        "\tRareWords = []\n",
        "\tnumber_rare_words = 10\n",
        "\t# take top 10 frequent words\n",
        "\tfrequentWords = counter.most_common()\n",
        "\tfor (word, word_count) in frequentWords[:-number_rare_words:-1]:\n",
        "\t\tRareWords.append(word)\n",
        "\n",
        "\treturn RareWords\n",
        "\n",
        "###****************\n",
        "def remove_rw(text, RareWords):\n",
        "\t\"\"\"\n",
        "\tReturn :- String after removing frequent words\n",
        "\tInput :- String\n",
        "\tOutput :- String\n",
        "\t\"\"\"\n",
        "\n",
        "\ttokens = word_tokenize(text)\n",
        "\twithout_rw = []\n",
        "\tfor word in tokens:\n",
        "\t\tif word not in RareWords:\n",
        "\t\t\twithout_rw.append(word)\n",
        "\n",
        "\twithout_rw = ' '.join(without_rw)\n",
        "\treturn without_rw\n",
        "### expample\n",
        "###\n",
        "##########################################33\n",
        "#delet last weght\n",
        "def deletFeatures(Feature_index,saved_embaddings):\n",
        "  for d in saved_embaddings:\n",
        "    del d[Feature_index-1]\n",
        "  return saved_embaddings\n",
        "\n",
        "############################################################\n",
        "#تابع لحساب تردد المستند العكسي بناء على بيانات التدريب والاختبار المقسومة\n",
        "def RepresentationGNAG(train_docs,test_docs , sim_val , alpha , centers , dictionary):\n",
        "  train_embeddings = []\n",
        "  test_embeddings = []\n",
        "\n",
        "  counter = 0\n",
        "  IDf = np.zeros(len(centers)).tolist() # بطول العناقيد\n",
        "#idf from train docs  \n",
        "  for i in train_docs: \n",
        "    v = Doc2Vec(i ,sim_val , alpha , centers , dictionary)\n",
        "    ######## حساب عدد مرات ظهور المفهوم في كل نص\n",
        "    for i in v:   # i is concept line , i need class of doc v\n",
        "      if i > 0:\n",
        "        IDf[counter] = IDf[counter] + 1 #  i[M-1]\n",
        "      counter = counter+1\n",
        "    counter = 0\n",
        "    train_embeddings.append(v)\n",
        "#idf from test docs \n",
        "  for i in test_docs: \n",
        "    v = Doc2Vec(i ,sim_val , alpha , centers , dictionary)\n",
        "    ######## حساب عدد مرات ظهور المفهوم في كل نص\n",
        "    for i in v:   # i is concept line , i need class of doc v\n",
        "      if i > 0:\n",
        "        IDf[counter] = IDf[counter] + 1 #  i[M-1]\n",
        "      counter = counter+1\n",
        "    counter = 0\n",
        "    test_embeddings.append(v)\n",
        "\n",
        "  print(IDf)\n",
        "  ######## Calculate Cf-IDF\n",
        "  for i in range(len(train_embeddings)):\n",
        "    l=len(train_embeddings[i])\n",
        "    for j in range(l):\n",
        "    #for doc i: muti cell j with idf j \n",
        "       #print('i', i,' j ', j, ' = ', embeddings[i][j] * newIDF[j])\n",
        "       train_embeddings[i][j] = train_embeddings[i][j] * IDf[j]\n",
        "\n",
        "  for i in range(len(test_embeddings)):\n",
        "    l=len(test_embeddings[i])\n",
        "    for j in range(l):\n",
        "       test_embeddings[i][j] = test_embeddings[i][j] * IDf[j]\n",
        "  return train_embeddings,test_embeddings\n",
        "###############\n",
        "################################################################################\n",
        "#thershold for words in document\n",
        "def delete_empty_docs(documents_embeddings,labels,thershold):\n",
        "  doc_indexes=[]\n",
        "  new_emb=[]\n",
        "  new_label=[]\n",
        "  j=0\n",
        "  #find empty docs\n",
        "  for d in documents_embeddings:\n",
        "    l=len(d)\n",
        "    if l>thershold:\n",
        "      new_label.append(labels[j])\n",
        "      new_emb.append(documents_embeddings[j])\n",
        "    else:\n",
        "      doc_indexes.append(j)\n",
        "    j=j+1\n",
        "  print(doc_indexes)\n",
        "  return new_emb,new_label\n",
        "#################################\n",
        "#used for self embeddings \n",
        "def get_documents_as_list(df):\n",
        "  documents_embeddings = []\n",
        "  for i in range(len(df)):\n",
        "    documents_embeddings.append(final_preprocess_text(df['document'][i]))\n",
        "  return documents_embeddings\n",
        "\n",
        "def documents_as_list_of_sentences(df):\n",
        "  documents_embeddings = []\n",
        "  for i in range(len(df)):\n",
        "    tokens = final_preprocess_text(df['document'][i])\n",
        "    sentens= ' '.join(tokens)\n",
        "    documents_embeddings.append(sentens)\n",
        "  return documents_embeddings\n",
        "\n",
        "\n",
        "#######################################\n",
        "def word_count(documents_embeddings):\n",
        "  word_count=[]\n",
        "  #find empty docs\n",
        "  for d in documents_embeddings:\n",
        "    l=len(d)\n",
        "    word_count.append(l)\n",
        "    \n",
        "  return word_count\n",
        "##########################################\n",
        "def document_biggerThan(data_DF, doc_vectors,th): \n",
        "  #number of docs in each class\n",
        "  doc_in_class=DataBalance(data_DF)\n",
        "  # number of classes\n",
        "  L_labels=len(doc_in_class)\n",
        "  K_clusters=len(doc_vectors[0])\n",
        "  #array for freq  \n",
        "  freq_array = np.zeros(L_labels).tolist()\n",
        "  mean_array = np.zeros(L_labels).tolist()  \n",
        "  i=0 # doc counter\n",
        "  for d in doc_vectors:\n",
        "    curren_class=data_DF['class'][i]\n",
        "    docL= len(d)\n",
        "    if docL>th:\n",
        "      freq_array[curren_class]=freq_array[curren_class]+1\n",
        "    i=i+1\n",
        "  #متوسط اطوال مستندات كل فئة=عدد مستنداتها الاكبر من حد معين على عدد مستندات الفئة\n",
        "  mean_array = [i / j for i, j in zip(freq_array, doc_in_class)] \n",
        "\n",
        "  return freq_array,mean_array\n",
        "\n",
        "#####################################################\n",
        "def clac_new_Tf_idf(to_delete,tf_similarity,idfcount,idfExp):\n",
        "  i=0\n",
        "  j=0\n",
        "  newIDFexp=[]\n",
        "  newIDFcount=[]\n",
        "  newTf_similarity=[]\n",
        "  num_of_concepts=len(idfexp)\n",
        "\n",
        "  \n",
        "  for d in tf_similarity:\n",
        "    tempD=[]\n",
        "    for j in range(num_of_concepts):\n",
        "      if j not in to_delete:\n",
        "        tempD.append(d[j])\n",
        "    newTf_similarity.append(tempD)\n",
        "    del tempD\n",
        "  ###\n",
        "  for i in range(num_of_concepts):\n",
        "    if i not in todelete:\n",
        "      newIDFexp.append(idfexp[i])\n",
        "      newIDFcount.append(idfcount[i])\n",
        "  return newTf_similarity,newIDFexp,newIDFcount\n",
        "\n",
        "\n",
        "##############################################################\n",
        "def concept_to_delete(min_idf,max_idf,idfcount):\n",
        "  to_delete_List=[]\n",
        "  j=0\n",
        "  for i in idfcount: #30\n",
        "    if (i > max_idf) or (i < min_idf):\n",
        "      to_delete_List.append(j)\n",
        "    j+=1\n",
        "  return to_delete_List\n",
        "\n",
        "######################################################\n",
        "def delete_NaN_docs(documents_embeddings,labels):\n",
        "  doc_indexes=[]\n",
        "  new_emb=[]\n",
        "  new_label=[]\n",
        "  j=0\n",
        "  #find empty docs\n",
        "  for d in documents_embeddings:\n",
        "    if math.isnan(d[0])==False:\n",
        "      new_label.append(labels[j])\n",
        "      new_emb.append(documents_embeddings[j])\n",
        "    else:\n",
        "      doc_indexes.append(j)\n",
        "    j=j+1\n",
        "  print(doc_indexes)\n",
        "  return new_emb,new_label\n",
        "\n",
        "###################################3\n",
        "def docs_word_count(data_DF):\n",
        "  word_count=[]\n",
        "  #find empty docs\n",
        "  for d in data_DF['document']:\n",
        "    l=len(d)\n",
        "    word_count.append(l)\n",
        "    \n",
        "  return word_count\n",
        "\n",
        "#############################################\n",
        "def data_details(documents_embeddings,data_DF,th):\n",
        "  fr,men=document_biggerThan(data_DF,documents_embeddings,th)\n",
        "  no_docs = len(data_DF)\n",
        "  doc_per_class = DataBalance(data_DF)\n",
        "  wc=word_count(documents_embeddings) #embedded word counts\n",
        "  o_wc= docs_word_count(data_DF) # orginal word count\n",
        "\n",
        "  print('№ of all doc=',len(data_DF),\"\\n\")\n",
        "  print(\"Befor embeddings: \\n\",\n",
        "        'the mean length of docs', sum(o_wc)/len(o_wc), \"\\n\",\n",
        "        \"The max length of docs\", max(o_wc),\"\\n\",\n",
        "        \"The min length of docs\", min(o_wc),\"\\n\", \n",
        "        '№ classes ', ' =', len(doc_per_class),\"\\n\",\n",
        "        \"№ docs per class \", doc_per_class )\n",
        "  print(\"After embeddings: \\n\",\n",
        "        \"the mean length of embeded docs\", sum(wc)/len(wc), \"\\n\",\n",
        "        \"The max length of embeded docs\", max(wc),\"\\n\",\n",
        "        \"The min length of embeded docs\", min(wc),\"\\n\", \n",
        "        \"№ Documents with length > \",th, \"=\" ,sum(fr))\n",
        "  \n",
        "\n",
        "##################################################################333\n",
        "#خاص بحقيبة الكلمات الاصلية\n",
        "def BOC(embeddeings_document,threshold_sim,alpha,centers,dictionary):\n",
        "  M=2\n",
        "  concept_freq = 0\n",
        "  ## intilize doc array\n",
        "  doc2vec = np.zeros(len(centers)).tolist()\n",
        "\n",
        "  for cnt in range(len(centers)):\n",
        "    counter = 0;\n",
        "    for emb_word in embeddeings_document:\n",
        "      Wsim= cosine_distance(emb_word , centers[cnt])\n",
        "      if Wsim  > alpha:\n",
        "        concept_freq = concept_freq+1 #for tf\n",
        "        doc2vec[cnt] = doc2vec[cnt] + 1;\n",
        "      else:\n",
        "        continue;\n",
        "      counter = counter +1;\n",
        "    # end for doc words with one concepts\n",
        "  if concept_freq !=0: # the number of occurence for all concept in this document\n",
        "    for i in doc2vec:\n",
        "      i = i / concept_freq  \n",
        "  return doc2vec\n",
        "#####################################\n",
        "def CF_IDF_BOC(documents_embeddings , sim_val , alpha , centers , dictionary):\n",
        "  embeddings = []\n",
        "  counter = 0\n",
        "  IDf = np.zeros(len(centers)).tolist() # بطول العناقيد\n",
        "  for i in documents_embeddings: \n",
        "    bag_of_concepts = BOC(i ,sim_val , alpha , centers , dictionary)\n",
        "    ######## حساب عدد مرات ظهور المفهوم في كل نص\n",
        "    for i in bag_of_concepts:   # i is concept line , i need class of doc v\n",
        "      if i > 0:\n",
        "        IDf[counter] = IDf[counter] + 1 #  i[M-1]\n",
        "      counter = counter+1\n",
        "    counter = 0\n",
        "    embeddings.append(bag_of_concepts)\n",
        "  #########  calculate IDF\n",
        "  newIDF=[]\n",
        "  for i in IDf:\n",
        "    if i!=0:\n",
        "      N=len(embeddings)\n",
        "      newIDF.append( math.log10( (N+1) /i ) )\n",
        "    else:\n",
        "      newIDF.append(0)  \n",
        "  ######## Calculate Cf-IDF\n",
        "  print(len(embeddings))\n",
        "  for i in range(len(embeddings)):\n",
        "    l=len(embeddings[i])\n",
        "    for j in range(l):\n",
        "    #for doc i: muti cell j with idf j \n",
        "       #print('i', i,' j ', j, ' = ', embeddings[i][j] * newIDF[j])\n",
        "       embeddings[i][j] = embeddings[i][j] * newIDF[j]\n",
        "  \n",
        "  return embeddings\n",
        "\n",
        "def text_file_to_list(train_path,test_path):\n",
        "  labels=[]\n",
        "  docs=[]\n",
        "  ###fill train\n",
        "  with open(train_path) as f:\n",
        "    reader = csv.reader(f, delimiter=\"\\t\")\n",
        "    d = list(reader)\n",
        "  for line in d:\n",
        "    docs.append(line[1])\n",
        "    labels.append(line[0])\n",
        "  ####test\n",
        "  with open(test_path) as f:\n",
        "    reader = csv.reader(f, delimiter=\"\\t\")\n",
        "    d = list(reader)\n",
        "  for line in d:\n",
        "    docs.append(line[1])\n",
        "    labels.append(line[0])\n",
        "  return docs,labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Collecting spherecluster\n",
            "  Downloading spherecluster-0.1.7-py3-none-any.whl (14 kB)\n",
            "Collecting nose\n",
            "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[K     |████████████████████████████████| 154 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from spherecluster) (1.19.5)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from spherecluster) (3.6.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from spherecluster) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from spherecluster) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->spherecluster) (1.0.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spherecluster) (1.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spherecluster) (1.15.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spherecluster) (1.10.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spherecluster) (8.10.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spherecluster) (21.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->spherecluster) (57.4.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->spherecluster) (0.7.1)\n",
            "Installing collected packages: nose, spherecluster\n",
            "Successfully installed nose-1.3.7 spherecluster-0.1.7\n",
            "Collecting soyclustering\n",
            "  Downloading soyclustering-0.2.0-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: numpy>=1.1 in /usr/local/lib/python3.7/dist-packages (from soyclustering) (1.19.5)\n",
            "Installing collected packages: soyclustering\n",
            "Successfully installed soyclustering-0.2.0\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj8ww4vWj5BX"
      },
      "source": [
        "# Загрузка Датасетов (Data sets Downolad from Drive)\n",
        "ВВС & 20NG & Reuters & OHSUMED & WebKB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "rnb1TFbGkhct",
        "outputId": "2a43f4bd-be71-4981-a873-c2b7b568177c"
      },
      "source": [
        "Reuters_file_path=\"/content/drive/MyDrive/PHD Work/Data/Reuters88.csv\"\n",
        "\n",
        "data_DF = pd.read_csv(Reuters_file_path)\n",
        "\n",
        "# rename columns\n",
        "data_DF.columns.values[0] = \"class\"\n",
        "data_DF.columns.values[1] = \"document\"\n",
        "\n",
        "# Factorize class names to numbers\n",
        "data_DF['class'] = pd.factorize(data_DF['class'])[0]\n",
        "data_DF.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>document</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>THAI TRADE DEFICIT WIDENS IN FIRST QUARTER\\n  ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>SRI LANKA GETS USDA APPROVAL FOR WHEAT PRICE\\n...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   class                                           document\n",
              "0      0  ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...\n",
              "1      1  CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO...\n",
              "2      2  JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA...\n",
              "3      3  THAI TRADE DEFICIT WIDENS IN FIRST QUARTER\\n  ...\n",
              "4      1  SRI LANKA GETS USDA APPROVAL FOR WHEAT PRICE\\n..."
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlYA9AxDkRzQ"
      },
      "source": [
        "###**Предварительная обработка**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwKMpUBNjsKT"
      },
      "source": [
        "## load or extract tokens from dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMUdMyA0U_wp"
      },
      "source": [
        "# extract token from dataset\n",
        "#all_tokens = new_pre_processing_dataset(data_DF)\n",
        "\n",
        "#\n",
        "#all_tokens = pre_processing_dataset(data_DF)\n",
        "\n",
        "#load the saved tokens\n",
        "token_path=\"your path/Reuters_r20_data.txt.npy\"\n",
        "all_tokens=loadList(token_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7lLmzIzp2iv"
      },
      "source": [
        "## Select self or pretrained embedding model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6UbrjtHp0iy"
      },
      "source": [
        "# Train fast text model on your dataset\n",
        "documents_as_list = get_documents_as_list(data_DF)\n",
        "from gensim.models import FastText\n",
        "model = FastText(documents_as_list, size=300, window=15, min_count=5, workers=4,sg=1)\n",
        "\n",
        "# Or Load pre trained model embeddings\n",
        "#model = Load_pre_trained_model_embeddings(model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3vs_X3xlJGb"
      },
      "source": [
        "## Extract Dictionary of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpeOXCKvlIox",
        "outputId": "2cb7459a-5b84-42e8-a3fc-981943ed568b"
      },
      "source": [
        "# Remove duplication and get the unique words \n",
        "unique_words = remove_duplication(all_tokens)\n",
        "print(\"unique words: \" ,len(unique_words),\"\\n\")\n",
        "\n",
        "# make dictionary {token : emnbedding vector} for \n",
        "dic = Save_words_with_Embeddings (unique_words , model)\n",
        "print(\"length of dictionary: \",len(dic),\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unique words:  3174 \n",
            "\n",
            "length of dictionary:  3174 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSThZ7Xwlk8D"
      },
      "source": [
        "## Clustering the Dictionary of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzqFHaaMlkCy"
      },
      "source": [
        "dim = 300 #model.vector_size\n",
        "\n",
        "# Save (word:embedding) for each word in each document\n",
        "embeddings_matrix  , irrugular_words = Create_embedding_matrix(model , unique_words , dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9y9uRZfH5fY",
        "outputId": "8bb0116e-801e-4b42-a536-8767f4a461f5"
      },
      "source": [
        "# number of clusters\n",
        "K_clusters= 50\n",
        "sim_digree=0.3\n",
        "number_of_words_per_cluster=5\n",
        "\n",
        "######### Clustering\n",
        "labels , centers = spherical_kmeans(K_clusters , embeddings_matrix, sim_digree)\n",
        "print(\"The clusters details\", Counter(labels), \"\\n\")\n",
        "\n",
        "# dictionary cluster index: values are its words embedding\n",
        "concept_dictionary = get_cluster_elements(labels, K_clusters , embeddings_matrix)\n",
        "\n",
        "start_time = time.time()\n",
        "sorted_cons = get_soreted_dictionary(centers , concept_dictionary , number_of_words_per_cluster)\n",
        "print(\"Clustering excution time --- %s seconds ---\" % (time.time() - start_time), \"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The clusters details Counter({38: 127, 3: 127, 2: 124, 33: 113, 11: 102, 42: 101, 0: 101, 47: 98, 39: 95, 10: 92, 45: 88, 8: 87, 4: 87, 21: 83, 18: 74, 40: 74, 16: 73, 41: 72, 30: 71, 15: 71, 31: 69, 44: 69, 26: 66, 5: 64, 32: 63, 37: 63, 14: 60, 25: 59, 46: 56, 49: 53, 20: 53, 17: 53, 29: 51, 12: 50, 9: 49, 35: 48, 7: 48, 13: 46, 24: 44, 36: 40, 23: 39, 19: 36, 6: 30, 22: 29, 48: 24, 27: 23, 28: 18, 1: 5, 43: 3, 34: 3}) \n",
            "\n",
            "Clustering excution time --- 1.099527359008789 seconds --- \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5l32D8ymV1V"
      },
      "source": [
        "### Represent each document as list of its words vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksFb5NBa1AdU",
        "outputId": "be19c874-0658-4d9f-9963-f35b7be8d3e6"
      },
      "source": [
        "N=0 # \n",
        "min_doc= 0 # الحد الادني لعدد الكلمات في المستند\n",
        "\n",
        "documents_embeddings = self_embedding_all_documents(data_DF , model,N)\n",
        "labels =data_DF['class'].tolist()\n",
        "\n",
        "print(\"the  len of labels and docs \", len(labels),len(documents_embeddings),\"\\n\")\n",
        "\n",
        "print(\"deleting docs that have not embedded words or its length less\", min_doc , \"\\n\")\n",
        "\n",
        "#deleting empty document if exist\n",
        "documents_embeddings ,labels = delete_empty_docs(documents_embeddings,labels,min_doc)\n",
        "print(\"the  New len of labels and docs \", len(labels),len(documents_embeddings),\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the  len of labels and docs  8491 8491 \n",
            "\n",
            "deleting docs that have not embedded words or its length less 0 \n",
            "\n",
            "[]\n",
            "the  New len of labels and docs  8491 8491 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NA1crRuZT5E"
      },
      "source": [
        "# Bag of weighted Concepts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOb8CGAlxIrU"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "\n",
        "def data_preperation(dataset , labels, sample):\n",
        "  x_train , x_test , y_train , y_test = train_test_split(dataset , labels , test_size = sample , random_state = 0)\n",
        "  return array(x_train) , array(x_test) , array(y_train) , array(y_test)\n",
        "\n",
        "def train_ML(x_train , y_train, krenell):\n",
        "  #kernell = linear', 'poly', 'rbf', 'sigmoid',\n",
        "  svm =  SVC(kernel=kernell, random_state=44).fit(x_train , y_train)   \n",
        "  return svm\n",
        "\n",
        "def evaluation_ML(x_test , y_test , svm_model):\n",
        "  ev=svm_model.score(x_test , y_test)\n",
        "  return ev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "869bhrJcim0p"
      },
      "source": [
        "### Documents vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ci2bgMM3NiDu",
        "outputId": "5820af50-da0c-4381-ac51-6aad6be4a2a6"
      },
      "source": [
        "# Thershold similarity values\n",
        "#Theta = [0.2,0.3,0.4,0.5,0.6, 0.7,0.8]\n",
        "Theta = [0.2]\n",
        "for alpha in Theta:\n",
        "  \n",
        "  min_idf = 0\n",
        "  max_idf_r=1\n",
        "  kernell='poly' \n",
        "  max_idf=max_idf_r* len(documents_embeddings)\n",
        "\n",
        "  print(\"====================================================== start represnation:\\n\")\n",
        "  start_time = time.time()\n",
        "  tf_similarity = RepresentationNew(documents_embeddings, Th , alpha , centers , sorted_cons)\n",
        "\n",
        "  print(\"Counting IDF to select concepts with IDF less than \",max_idf, \":\\n\")\n",
        "\n",
        "  idfcount,idfexp=calculate_IDF(tf_similarity)\n",
        "  todelete=concept_to_delete(min_idf, max_idf,idfcount)\n",
        "  print(len(todelete),\" clusters will be deleted:  \", todelete ,\" \\n\")\n",
        "\n",
        "  new_tf_similarity,idfexp,idfcount= clac_new_Tf_idf(todelete,tf_similarity,idfcount,idfexp)\n",
        "  K_clusters=len(idfexp)\n",
        "  print(\"The new number of clusters: \",K_clusters, \":\\n\")\n",
        "\n",
        "  print(\"Calculating BoEC...\\n\")\n",
        "\n",
        "  final_embeddings=CF_IDF(new_tf_similarity,idfexp)\n",
        "\n",
        "  represnationtime=time.time() - start_time\n",
        "\n",
        "\n",
        "  print(\"time for representation: \", represnationtime,\" seconds\")\n",
        "\n",
        "  print(\"build new data frame: \\n\")\n",
        "\n",
        "  embedding_DF = pd.DataFrame({'class': labels, 'document': final_embeddings})\n",
        "\n",
        "\n",
        "  print(\"Start training ............................................ \",alpha, \"\\n\")\n",
        "\n",
        "\n",
        "  final_embeddings = [array(i).reshape(-1) for i in final_embeddings]\n",
        "  labels =embedding_DF['class'].tolist() # ConvertTextLabelToNeumerical(data_DF)\n",
        "\n",
        "  kernell='poly'\n",
        "\n",
        "  x_train , x_test , y_train , y_test = data_preperation(final_embeddings , labels, 0.30)\n",
        "  print(\"resluts without calss info.............\")\n",
        "  svm = train_ML(x_train , y_train,kernell)\n",
        "  y_pred=svm.predict(x_test)\n",
        "  F1 = f1_score(y_test,y_pred, average=\"micro\")\n",
        "\n",
        "  print(\"average-weighted f1 score\", F1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resluts without calss info.............\n",
            "average-weighted f1 score 0.9249858256923642\n"
          ]
        }
      ]
    }
  ]
}