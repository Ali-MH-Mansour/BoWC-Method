{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Self_FastText_BoWC.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ali-MH-Mansour/BoWC-Method/blob/main/Bag-of-weighted-concepts/Self_FastText_BoWC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe8E2O_7iyQX"
      },
      "source": [
        "## **Connect Drive**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_I0ZTb-sqxH7",
        "outputId": "5c2ec7cd-ab55-441d-df90-a6077aa46c7e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gge2NxpEmGsT"
      },
      "source": [
        "# **Requirements  || Определение библиотек**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kp6vCScYjWfp",
        "outputId": "3c0c2ba4-f051-4b61-a9b7-06273b2f40c6"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOH5gupuRUiH",
        "outputId": "26fbda8d-376c-4a33-fbf5-46a0c9819e99"
      },
      "source": [
        "!pip install sklearn\n",
        "!pip install spherecluster\n",
        "!pip install soyclustering\n",
        "#################################### Importing all Library here  ################################################\n",
        "from soyclustering import SphericalKMeans\n",
        "import time\n",
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from numpy import array \n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords \n",
        "from numpy import array\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import gensim.downloader\n",
        "from collections import Counter \n",
        "import functools\n",
        "import math\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.sparse import csr_matrix\n",
        "#from sklearn_extra.cluster import KMedoids\n",
        "#from spherecluster import SphericalKMeans\n",
        "############ Our Downloads ##########\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "##################################\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "import numpy as np\n",
        "np.random.seed(2018)\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "################################ \n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk import word_tokenize, sent_tokenize, pos_tag\n",
        "\n",
        "##############################################################################################\n",
        "def final_preprocess_text(text):\n",
        "    # 1. Tokenise to alphabetic tokens\n",
        "    text = remove_numbers(text)\n",
        "    text = remove_punctuation(text)\n",
        "    text = convert_to_lower(text)\n",
        "    text = remove_white_space(text)\n",
        "    text = remove_short_words(text)\n",
        "    tokens = toknizing(text)\n",
        "\n",
        "    # 2. POS tagging\n",
        "    pos_map = {'J': 'a', 'N': 'n', 'R': 'r', 'V': 'v'}\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    \n",
        "    # 3. Lowercase and lemmatise \n",
        "    lemmatiser = WordNetLemmatizer()\n",
        "    tokens = [lemmatiser.lemmatize(t.lower(), pos=pos_map.get(p[0], 'v')) for t, p in pos_tags]\n",
        "    return tokens\n",
        "\n",
        "#--------------------------------------------------------------------------------------------\n",
        "\n",
        "def convert_to_lower(text):\n",
        "  return text.lower()\n",
        "#--------------------------------------------------------------------------------------------\n",
        "def remove_numbers(text):\n",
        "  text = re.sub(r'\\d+' , '', text)\n",
        "  return text\n",
        "#--------------------------------------------------------------------------------------------\n",
        "\n",
        "def remove_short_words(text):\n",
        "  text = re.sub(r'\\b\\w{1,2}\\b', '', text)\n",
        "  return text\n",
        "#--------------------------------------------------------------------------------------------\n",
        "\n",
        "def remove_punctuation(text):\n",
        "     punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^+&*_~'''\n",
        "     no_punct = \"\"\n",
        "     for char in text:\n",
        "        if char not in punctuations:\n",
        "            no_punct = no_punct + char\n",
        "     return no_punct\n",
        "#--------------------------------------------------------------------------------------------\n",
        "def remove_white_space(text):\n",
        "  text = text.strip()\n",
        "  return text\n",
        "#--------------------------------------------------------------------------------------------\n",
        "def toknizing(text):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = word_tokenize(text)\n",
        "  ## Remove Stopwords from tokens\n",
        "  result = [i for i in tokens if not i in stop_words]\n",
        "  return result\n",
        "#--------------------------------------------------------------------------------------------\n",
        "def remove_duplication(text):\n",
        "  return list(set(text)) \n",
        "####################################################################################\n",
        "def pre_processing_dataset(data):\n",
        "  All_tokens = [];\n",
        "  for i in range(len(data)):\n",
        "    All_tokens.append(final_preprocess_text(data.iloc[i][1]))\n",
        "  All_tokens = Convert_list_of_list_to_list(All_tokens)\n",
        "  print('the number of words in Crorpus' , len(All_tokens))\n",
        "\n",
        "  return All_tokens\n",
        "\n",
        "####################################################################################\n",
        "def Convert_list_of_list_to_list(all_tokens):\n",
        "  tokens = []\n",
        "  for words in all_tokens:\n",
        "    for i in words:\n",
        "      tokens.append(i)\n",
        "  return tokens\n",
        "\n",
        "#########################################################################################\n",
        "def Create_embedding_matrix(model , tokens , embedding_dim):\n",
        "    embedding_matrix = []\n",
        "    irregular_words  = []\n",
        "    for words in tokens:\n",
        "          try :\n",
        "            temp = model.wv[words]\n",
        "            embedding_matrix.append(temp)\n",
        "          except KeyError:\n",
        "            # for the words that are not in model vocabulary move to irregular_words\n",
        "            irregular_words.append(words)\n",
        "    return embedding_matrix , irregular_words\n",
        "\n",
        "#####################################################"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.1.0)\n",
            "Collecting spherecluster\n",
            "  Downloading spherecluster-0.1.7-py3-none-any.whl (14 kB)\n",
            "Collecting nose\n",
            "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[K     |████████████████████████████████| 154 kB 5.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from spherecluster) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from spherecluster) (1.4.1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from spherecluster) (3.6.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from spherecluster) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->spherecluster) (1.1.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spherecluster) (8.10.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spherecluster) (1.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->spherecluster) (57.4.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spherecluster) (1.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spherecluster) (1.15.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spherecluster) (21.2.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->spherecluster) (0.7.1)\n",
            "Installing collected packages: nose, spherecluster\n",
            "Successfully installed nose-1.3.7 spherecluster-0.1.7\n",
            "Collecting soyclustering\n",
            "  Downloading soyclustering-0.2.0-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: numpy>=1.1 in /usr/local/lib/python3.7/dist-packages (from soyclustering) (1.19.5)\n",
            "Installing collected packages: soyclustering\n",
            "Successfully installed soyclustering-0.2.0\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPyMVCSNRf81"
      },
      "source": [
        "# list of stop words from many resources\n",
        "my_stopwords= [\"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\",\"aab\",'aaab', \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"affects\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"ain't\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appear\", \"appreciate\", \"appropriate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"aren't\", \"arise\", \"around\", \"as\", \"a's\", \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"can't\", \"cause\", \"causes\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"changes\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"c'mon\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"could\", \"couldn\", \"couldnt\", \"couldn't\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"c's\", \"ct\", \"cu\", \"currently\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"didn't\", \"different\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"done\", \"don't\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"effect\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"empty\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"h2\", \"h3\", \"had\", \"hadn\", \"hadn't\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"hed\", \"he'd\", \"he'll\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"here's\", \"hereupon\", \"hers\", \"herself\", \"hes\", \"he's\", \"hh\", \"hi\", \"hid\", \"him\", \"himself\", \"his\", \"hither\", \"hj\", \"ho\", \"home\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"how's\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"i'd\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"i'll\", \"im\", \"i'm\", \"immediate\", \"immediately\", \"importance\", \"important\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"invention\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"isn't\", \"it\", \"itd\", \"it'd\", \"it'll\", \"its\", \"it's\", \"itself\", \"iv\", \"i've\", \"ix\", \"iy\", \"iz\", \"j\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"know\", \"known\", \"knows\", \"ko\", \"l\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"let's\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mightn't\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"n\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needn\", \"needn't\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"nothing\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"other\", \"others\", \"otherwise\", \"ou\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"present\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"s2\", \"sa\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shan't\", \"she\", \"shed\", \"she'd\", \"she'll\", \"shes\", \"she's\", \"should\", \"shouldn\", \"shouldn't\", \"should've\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"system\", \"sz\", \"t\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"thats\", \"that's\", \"that've\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"there's\", \"thereto\", \"thereupon\", \"there've\", \"these\", \"they\", \"theyd\", \"they'd\", \"they'll\", \"theyre\", \"they're\", \"they've\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"t's\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"ut\", \"v\", \"va\", \"value\", \"various\", \"vd\", \"ve\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"wa\", \"want\", \"wants\", \"was\", \"wasn\", \"wasnt\", \"wasn't\", \"way\", \"we\", \"wed\", \"we'd\", \"welcome\", \"well\", \"we'll\", \"well-b\", \"went\", \"were\", \"we're\", \"weren\", \"werent\", \"weren't\", \"we've\", \"what\", \"whatever\", \"what'll\", \"whats\", \"what's\", \"when\", \"whence\", \"whenever\", \"when's\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"where's\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whom\", \"whomever\", \"whos\", \"who's\", \"whose\", \"why\", \"why's\", \"wi\", \"widely\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"won't\", \"words\", \"world\", \"would\", \"wouldn\", \"wouldnt\", \"wouldn't\", \"www\", \"x\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"you'd\", \"you'll\", \"your\", \"youre\", \"you're\", \"yours\", \"yourself\", \"yourselves\", \"you've\", \"yr\", \"ys\", \"yt\", \"z\", \"zero\", \"zi\", \"zz\"]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yPskImTLE1Q"
      },
      "source": [
        "####################################################################################\n",
        "### loading the embeddings\n",
        "def Load_pre_trained_model_embeddings(file_path):\n",
        "  wv = KeyedVectors.load(file_path, mmap='r')\n",
        "  return wv\n",
        "###########################################################################################\n",
        "# K-means Clustering the embeddings #\n",
        "def embeddings_k_means_clustering(embeddings_vectors, N_clusters):\n",
        "  #kmedoids = KMedoids(n_clusters=5, random_state=0).fit(embeddings_vectors)\n",
        "  #SphericalKMeans\n",
        "  kmeans = KMeans(n_clusters= N_clusters , random_state=0).fit(embeddings_vectors)\n",
        "  return kmeans\n",
        "########################################################################################\n",
        "def spherical_kmeans(n_clusters_, embeddings_matrix,sim_digree):\n",
        "  embeddings_matrix_csr = csr_matrix(embeddings_matrix)\n",
        "  spherical_kmeans = SphericalKMeans( max_similar=sim_digree, init='similar_cut', \n",
        "      n_clusters = n_clusters_)\n",
        "  labels = spherical_kmeans.fit_predict(embeddings_matrix_csr)\n",
        "  centers = spherical_kmeans.cluster_centers_\n",
        "  return labels , centers\n",
        "\n",
        "############################################################################################\n",
        "\n",
        "#############################################################################################\n",
        "def clusters_properties(k_means):\n",
        "  print(Counter(k_means.labels_))\n",
        "  centers = k_means.cluster_centers_\n",
        "  '''\n",
        "  Return\n",
        "     - Number of samples for each cluter ,\n",
        "     - The Centers of each cluster\n",
        "  '''\n",
        "  return Counter(k_means.labels_) , centers\n",
        "#############################################################################################\n",
        "def Save_words_with_Embeddings(tokens , pre_model):\n",
        "  dic = {}\n",
        "  for words in tokens: \n",
        "      try :\n",
        "        dic.update({words : pre_model.wv[words]})\n",
        "      except: \n",
        "        continue;\n",
        "  return dic\n",
        "##############################################################################################\n",
        "### this function take the vector and return the word that linked with it.\n",
        "def Vector_to_Word(vector , dic):\n",
        "  word = ''\n",
        "  for i in dic:\n",
        "    if functools.reduce(lambda x, y : x and y, map(lambda p, q: p == q,list(dic[i]),vector), True): \n",
        "      word = i\n",
        "  return word\n",
        "##############################################################################################\n",
        "\n",
        "def get_cluster_elements(labels , n_clusters , embedding_matrix):\n",
        "  # Nice Pythonic way to get the indices of the points for each corresponding cluster\n",
        "  mydict = {i: np.where(labels == i)[0] for i in range(n_clusters)}\n",
        "  embeddings_samples = []\n",
        "  for i in range(len(mydict)):\n",
        "    temp= []\n",
        "    for j in mydict[i]:\n",
        "      temp.append(embedding_matrix[j])\n",
        "    embeddings_samples.append(temp)\n",
        "    del temp\n",
        "  return embeddings_samples\n",
        "\n",
        "############################################################################################\n",
        "def get_soreted_dictionary(centers , dictionary , M):\n",
        "  dic_with_similarity =[]\n",
        "  for i in range(len(dictionary)):\n",
        "    temp = []\n",
        "    for j in range(len(dictionary[i])):\n",
        "      temp.append([dictionary[i][j] , cosine_similarity([dictionary[i][j]] , [centers[i]])])\n",
        "    dic_with_similarity.append(temp)\n",
        "    del temp\n",
        "      ####### Sort all vectors clusters ############\n",
        "  sorted_list = []\n",
        "  def take_second(elem):\n",
        "      return elem[1]\n",
        "  for i in range(len(dic_with_similarity)):\n",
        "      sorted_list.append(sorted(dic_with_similarity[i], key=take_second , reverse = True))\n",
        "    ########### Get the hightest M features that related to the its centers among whole concepet dictionary#############\n",
        "  Top_M_Concept = [sorted_list[i][:M] for i in range(len(dic_with_similarity))]\n",
        "  return Top_M_Concept\n",
        "\n",
        "############################################################################################\n",
        "# compare each words in documents with all clusters words.\n",
        "def Get_similarity_matrix(v1 , list_of_vectors):\n",
        "  sim = []\n",
        "  s=0\n",
        "  for i in list_of_vectors:\n",
        "    sim.append(cosine_distance(v1, i[0]))\n",
        "  sim = array(sim).astype('float').reshape(-1).tolist()\n",
        "  if len(sim)!=0:\n",
        "    s=sum(sim)/len(sim)\n",
        "  return s\n",
        "\n",
        "### convert document to Vector with size C * M \n",
        "##############################################################################################\n",
        "##############################################################################################\n",
        "def Doc2Vec(embeddeings_document,number_of_concept_words,alpha,centers,dictionary):\n",
        "  M=2 # concept array elements\n",
        "  N=5 # cluster words\n",
        "  concept_freq = 0\n",
        "  ## intilize doc array\n",
        "  doc2vec = np.zeros((len(centers), M)).tolist()\n",
        "  final_vector=np.zeros(len(centers)).tolist()\n",
        "  for cnt in range(len(centers)):\n",
        "    counter = 0;\n",
        "    sim =[]\n",
        "    NN=0\n",
        "    for emb_word in embeddeings_document:\n",
        "      Wsim =cosine_distance(emb_word, centers[cnt])# التشابه مع اقرب كلمة للمركز\n",
        "      if Wsim  > alpha and NN<=N:\n",
        "        NN=NN+1\n",
        "        concept_freq = concept_freq+1 #for tf\n",
        "        Smean =Get_similarity_matrix(emb_word , dictionary[cnt][0:number_of_concept_words-1]);\n",
        "        sim.append(float((Smean+Wsim)/2))\n",
        "        doc2vec[cnt][M-1] = doc2vec[cnt][M-1] + 1;\n",
        "      elif Wsim  > alpha and NN>N:\n",
        "        concept_freq = concept_freq+1 #for tf\n",
        "        sim.append(Wsim)\n",
        "        doc2vec[cnt][M-1] = doc2vec[cnt][M-1] + 1;\n",
        "      else:\n",
        "        continue;\n",
        "      counter = counter +1;\n",
        "    # end for doc words with one concepts\n",
        "    meanofSim=0# mean of sim for current doc\n",
        "    if(len(sim)!=0):\n",
        "      meanofSim=sum(sim)/len(sim)\n",
        "    doc2vec[cnt][M-2]=meanofSim\n",
        "    del sim\n",
        "\n",
        "  if concept_freq !=0: # the number of occurence for all concept in this document\n",
        "    for i in doc2vec:\n",
        "      i[M-1] = i[M-1]* math.exp(i[M-2]) / concept_freq\n",
        "  \n",
        "  doc2vec1=np.array(doc2vec,dtype=object)\n",
        "  final_vector=doc2vec1[:,1].tolist() \n",
        "\n",
        "  return final_vector\n",
        "###################################################################\n",
        "def cosine_distance(u, v):\n",
        "  return np.dot(u, v) / (math.sqrt(np.dot(u, u)) * math.sqrt(np.dot(v, v)))\n",
        "#########################################33\n",
        "def normalize_vector(vect):\n",
        "  bymax=[float(i)/max(vect) for i in vect]\n",
        "  bysum=[float(i)/sum(vect) for i in vect]\n",
        "  return bymax,bysum\n",
        "\n",
        "\n",
        "####################################################33333\n",
        "def self_embedding_all_documents(df , model,N):\n",
        "  documents_embeddings = []\n",
        "  for i in range(len(df)):\n",
        "    documents_embeddings.append(Self_convert_doc_to_embedding(df['document'][i] , model,N))\n",
        "  return documents_embeddings\n",
        "\n",
        "########################################################################################################\n",
        "def Self_convert_doc_to_embedding(text , model,N):\n",
        "  tokens = final_preprocess_text(text)\n",
        "  if N!=0:\n",
        "    tokens=removeElements(tokens,N)\n",
        "  embeddings_matrix  , irrugular_words = Create_embedding_matrix(model , tokens , 300)\n",
        "  return embeddings_matrix\n",
        "#########################################################################################################\n",
        "\n",
        "\n",
        "####################################################33333\n",
        "def embedding_all_documents(df , model,N):\n",
        "  documents_embeddings = []\n",
        "  for i in range(len(df)):\n",
        "    documents_embeddings.append(convert_doc_to_embedding(df['document'][i] , model,N))\n",
        "  return documents_embeddings\n",
        "\n",
        "#########################################################################################################\n",
        "def BoWC_representation(documents_embeddings , sim_val , alpha , centers , dictionary):\n",
        "  embeddings = []\n",
        "  for i in documents_embeddings: \n",
        "    v = Doc2Vec(i ,sim_val , alpha , centers , dictionary)\n",
        "    embeddings.append(v)\n",
        "  return embeddings\n",
        "##########################################\n",
        "\n",
        "def calculate_IDF(doc_vectors):\n",
        "  docL=len(doc_vectors[0])\n",
        "  N=len(doc_vectors) #collection size\n",
        "  IDf = np.zeros(docL).tolist() # بطول العناقيد\n",
        "  for doc in doc_vectors:\n",
        "    counter=0 # counting concepts\n",
        "    for concept_freature in doc:\n",
        "      if concept_freature > 0:\n",
        "        IDf[counter] = IDf[counter] + 1 #  i[M-1]\n",
        "      counter = counter+1\n",
        "  \n",
        "  #ضرب حساب التردد\n",
        "  newIDF=[]\n",
        "  for i in IDf:\n",
        "    if i!=0:\n",
        "      newIDF.append( math.exp(-1*(i/N)))\n",
        "    else:\n",
        "      newIDF.append(0)\n",
        "  return IDf, newIDF\n",
        "\n",
        "########################################### \n",
        "#Calculate Cf-IDF\n",
        "def CF_IDF(doc_vectors,IDF):\n",
        "  docL=len(doc_vectors[0])\n",
        "  for i in range(len(doc_vectors)):\n",
        "    for j in range(docL):\n",
        "       doc_vectors[i][j] = doc_vectors[i][j] * IDF[j]\n",
        "  return doc_vectors\n",
        "\n",
        "#####################################################\n",
        "def clac_new_Tf_idf(to_delete,tf_similarity,idfcount,idfExp):\n",
        "  i=0\n",
        "  j=0\n",
        "  newIDFexp=[]\n",
        "  newIDFcount=[]\n",
        "  newTf_similarity=[]\n",
        "  num_of_concepts=len(idfexp)\n",
        "\n",
        "  \n",
        "  for d in tf_similarity:\n",
        "    tempD=[]\n",
        "    for j in range(num_of_concepts):\n",
        "      if j not in to_delete:\n",
        "        tempD.append(d[j])\n",
        "    newTf_similarity.append(tempD)\n",
        "    del tempD\n",
        "  ###\n",
        "  for i in range(num_of_concepts):\n",
        "    if i not in todelete:\n",
        "      newIDFexp.append(idfexp[i])\n",
        "      newIDFcount.append(idfcount[i])\n",
        "  return newTf_similarity,newIDFexp,newIDFcount\n",
        "#########################################################################################################\n",
        "\n",
        "def saveList(myList,filename):\n",
        "    # the filename should mention the extension 'npy'\n",
        "    np.save(filename,myList)\n",
        "    print(\"Saved successfully!\")\n",
        "def loadList(filename):\n",
        "    # the filename should mention the extension 'npy'\n",
        "    tempNumpyArray=np.load(filename)\n",
        "    return tempNumpyArray.tolist()\n",
        "\n",
        "#######################################################\n",
        "# number of doc in each class \n",
        "def DataBalance(data):\n",
        "  data['class'] = pd.factorize(data['class'])[0]\n",
        "  uniqueValues = data['class'].unique()\n",
        "  l=len(uniqueValues)\n",
        "  labels_freq = np.zeros(l).tolist()\n",
        "  for i in range(len(data)):\n",
        "    label= data.iloc[i][0]\n",
        "    labels_freq[label]=labels_freq[label]+1\n",
        "  return labels_freq \n",
        "\n",
        "############################################\n",
        "from collections import Counter \n",
        "def removeElements(lst, k): \n",
        "    counted = Counter(lst) \n",
        "    temp_lst = [] \n",
        "    for el in counted: \n",
        "        if counted[el] < k: \n",
        "            temp_lst.append(el) \n",
        "    res_lst = [] \n",
        "    for el in lst: \n",
        "        if el not in temp_lst: \n",
        "            res_lst.append(el) \n",
        "              \n",
        "    return(res_lst)\n",
        "################################################################################\n",
        "#thershold for words in document\n",
        "def delete_empty_docs(documents_embeddings,labels,thershold):\n",
        "  doc_indexes=[]\n",
        "  new_emb=[]\n",
        "  new_label=[]\n",
        "  j=0\n",
        "  #find empty docs\n",
        "  for d in documents_embeddings:\n",
        "    l=len(d)\n",
        "    if l>thershold:\n",
        "      new_label.append(labels[j])\n",
        "      new_emb.append(documents_embeddings[j])\n",
        "    else:\n",
        "      doc_indexes.append(j)\n",
        "    j=j+1\n",
        "  print(doc_indexes)\n",
        "  return new_emb,new_label\n",
        "#################################\n",
        "#used for self embeddings \n",
        "def get_documents_as_list(df):\n",
        "  documents_embeddings = []\n",
        "  for i in range(len(df)):\n",
        "    documents_embeddings.append(final_preprocess_text(df['document'][i]))\n",
        "  return documents_embeddings\n",
        "\n",
        "def documents_as_list_of_sentences(df):\n",
        "  documents_embeddings = []\n",
        "  for i in range(len(df)):\n",
        "    tokens = final_preprocess_text(df['document'][i])\n",
        "    sentens= ' '.join(tokens)\n",
        "    documents_embeddings.append(sentens)\n",
        "  return documents_embeddings\n",
        "#######################################\n",
        "def word_count(documents_embeddings):\n",
        "  word_count=[]\n",
        "  #find empty docs\n",
        "  for d in documents_embeddings:\n",
        "    l=len(d)\n",
        "    word_count.append(l)\n",
        "    \n",
        "  return word_count\n",
        "##########################################\n",
        "def document_biggerThan(data_DF, doc_vectors,th): \n",
        "  #number of docs in each class\n",
        "  doc_in_class=DataBalance(data_DF)\n",
        "  # number of classes\n",
        "  L_labels=len(doc_in_class)\n",
        "  K_clusters=len(doc_vectors[0])\n",
        "  #array for freq  \n",
        "  freq_array = np.zeros(L_labels).tolist()\n",
        "  mean_array = np.zeros(L_labels).tolist()  \n",
        "  i=0 # doc counter\n",
        "  for d in doc_vectors:\n",
        "    curren_class=data_DF['class'][i]\n",
        "    docL= len(d)\n",
        "    if docL>th:\n",
        "      freq_array[curren_class]=freq_array[curren_class]+1\n",
        "    i=i+1\n",
        "  #متوسط اطوال مستندات كل فئة=عدد مستنداتها الاكبر من حد معين على عدد مستندات الفئة\n",
        "  mean_array = [i / j for i, j in zip(freq_array, doc_in_class)] \n",
        "\n",
        "  return freq_array,mean_array\n",
        "\n",
        "##############################################################\n",
        "def concept_to_delete(min_idf,max_idf,idfcount):\n",
        "  to_delete_List=[]\n",
        "  j=0\n",
        "  for i in idfcount: #30\n",
        "    if (i > max_idf) or (i < min_idf):\n",
        "      to_delete_List.append(j)\n",
        "    j+=1\n",
        "  return to_delete_List\n",
        "\n",
        "\n",
        "#############################################\n",
        "def data_details(documents_embeddings,data_DF,th):\n",
        "  fr,men=document_biggerThan(data_DF,documents_embeddings,th)\n",
        "  no_docs = len(data_DF)\n",
        "  doc_per_class = DataBalance(data_DF)\n",
        "  wc=word_count(documents_embeddings) #embedded word counts\n",
        "  o_wc= docs_word_count(data_DF) # orginal word count\n",
        "\n",
        "  print('№ of all doc=',len(data_DF),\"\\n\")\n",
        "  print(\"Befor embeddings: \\n\",\n",
        "        'the mean length of docs', sum(o_wc)/len(o_wc), \"\\n\",\n",
        "        \"The max length of docs\", max(o_wc),\"\\n\",\n",
        "        \"The min length of docs\", min(o_wc),\"\\n\", \n",
        "        '№ classes ', ' =', len(doc_per_class),\"\\n\",\n",
        "        \"№ docs per class \", doc_per_class )\n",
        "  print(\"After embeddings: \\n\",\n",
        "        \"the mean length of embeded docs\", sum(wc)/len(wc), \"\\n\",\n",
        "        \"The max length of embeded docs\", max(wc),\"\\n\",\n",
        "        \"The min length of embeded docs\", min(wc),\"\\n\", \n",
        "        \"№ Documents with length > \",th, \"=\" ,sum(fr))\n",
        "##################################################################"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj8ww4vWj5BX"
      },
      "source": [
        "# Загрузка Датасетов (Data sets Downolad from Drive)\n",
        "ВВС & 20NG & Reuters & OHSUMED & WebKB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "rnb1TFbGkhct",
        "outputId": "6ed0e83a-d9dc-48d1-ab63-0fefbeaf89bd"
      },
      "source": [
        "Reuters_file_path=\"/content/drive/MyDrive/PHD Work/Data/Reuters88.csv\"\n",
        "\n",
        "data_DF = pd.read_csv(Reuters_file_path)\n",
        "\n",
        "# rename columns\n",
        "data_DF.columns.values[0] = \"class\"\n",
        "data_DF.columns.values[1] = \"document\"\n",
        "\n",
        "# Factorize class names to numbers\n",
        "data_DF['class'] = pd.factorize(data_DF['class'])[0]\n",
        "data_DF.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>document</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>THAI TRADE DEFICIT WIDENS IN FIRST QUARTER\\n  ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>SRI LANKA GETS USDA APPROVAL FOR WHEAT PRICE\\n...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   class                                           document\n",
              "0      0  ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...\n",
              "1      1  CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO...\n",
              "2      2  JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA...\n",
              "3      3  THAI TRADE DEFICIT WIDENS IN FIRST QUARTER\\n  ...\n",
              "4      1  SRI LANKA GETS USDA APPROVAL FOR WHEAT PRICE\\n..."
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlYA9AxDkRzQ"
      },
      "source": [
        "###**Предварительная обработка**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwKMpUBNjsKT"
      },
      "source": [
        "## load or extract tokens from dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMUdMyA0U_wp"
      },
      "source": [
        "# extract token from dataset\n",
        "#all_tokens = pre_processing_dataset(data_DF)\n",
        "token_path=\"/content/drive/MyDrive/PHD Work/PreProData/Reuters_r20_data.txt.npy\"\n",
        "\n",
        "all_tokens=loadList(token_path)\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7lLmzIzp2iv"
      },
      "source": [
        "## Select self or pretrained embedding model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U6UbrjtHp0iy"
      },
      "source": [
        "# Train fast text model on your dataset\n",
        "documents_as_list = get_documents_as_list(data_DF)\n",
        "from gensim.models import FastText\n",
        "model = FastText(documents_as_list, size=300, window=15, min_count=5, workers=4,sg=1)\n",
        "\n",
        "# Or Load pre trained model embeddings\n",
        "#model = Load_pre_trained_model_embeddings(model_path)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3vs_X3xlJGb"
      },
      "source": [
        "## Extract Dictionary of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpeOXCKvlIox",
        "outputId": "ec67ff13-0f40-4058-90a9-2e271dc288c0"
      },
      "source": [
        "# Remove duplication and get the unique words \n",
        "unique_words = remove_duplication(all_tokens)\n",
        "print(\"unique words: \" ,len(unique_words),\"\\n\")\n",
        "\n",
        "# make dictionary {token : emnbedding vector} for \n",
        "dic = Save_words_with_Embeddings (unique_words , model)\n",
        "print(\"length of dictionary: \",len(dic),\"\\n\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unique words:  3174 \n",
            "\n",
            "length of dictionary:  3174 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSThZ7Xwlk8D"
      },
      "source": [
        "## Clustering the Dictionary of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzqFHaaMlkCy"
      },
      "source": [
        "dim = 300 #model.vector_size\n",
        "\n",
        "# Save (word:embedding) for each word in each document\n",
        "embeddings_matrix  , irrugular_words = Create_embedding_matrix(model , unique_words , dim)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9y9uRZfH5fY",
        "outputId": "6145e644-6c8f-4047-9b94-c3798b057f0b"
      },
      "source": [
        "# number of clusters\n",
        "K_clusters= 50\n",
        "sim_digree=0.3\n",
        "number_of_words_per_cluster=5\n",
        "\n",
        "######### Clustering\n",
        "labels , centers = spherical_kmeans(K_clusters , embeddings_matrix, sim_digree)\n",
        "print(\"The clusters details\", Counter(labels), \"\\n\")\n",
        "\n",
        "# dictionary cluster index: values are its words embedding\n",
        "concept_dictionary = get_cluster_elements(labels, K_clusters , embeddings_matrix)\n",
        "\n",
        "start_time = time.time()\n",
        "sorted_cons = get_soreted_dictionary(centers , concept_dictionary , number_of_words_per_cluster)\n",
        "print(\"Clustering excution time --- %s seconds ---\" % (time.time() - start_time), \"\\n\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The clusters details Counter({19: 122, 48: 119, 45: 107, 32: 103, 18: 98, 20: 97, 47: 88, 15: 85, 42: 82, 21: 80, 44: 79, 37: 79, 26: 79, 22: 77, 12: 77, 36: 76, 27: 74, 9: 70, 17: 69, 28: 69, 40: 67, 35: 67, 4: 66, 11: 66, 41: 63, 23: 62, 43: 60, 2: 60, 34: 59, 25: 59, 14: 58, 38: 58, 46: 54, 24: 48, 49: 48, 39: 48, 29: 47, 0: 45, 33: 44, 6: 43, 5: 43, 13: 39, 3: 38, 7: 35, 30: 34, 16: 31, 1: 30, 31: 29, 10: 25, 8: 18}) \n",
            "\n",
            "Clustering excution time --- 0.7747592926025391 seconds --- \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5l32D8ymV1V"
      },
      "source": [
        "### Represent each document as list of its words vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksFb5NBa1AdU",
        "outputId": "a0224cb6-6248-4a0d-ddf0-045cd28bd1e2"
      },
      "source": [
        "N=0 # \n",
        "min_doc= 0 # الحد الادني لعدد الكلمات في المستند\n",
        "\n",
        "documents_embeddings = self_embedding_all_documents(data_DF , model,N)\n",
        "labels =data_DF['class'].tolist()\n",
        "\n",
        "print(\"the  len of labels and docs \", len(labels),len(documents_embeddings),\"\\n\")\n",
        "\n",
        "print(\"deleting docs that have not embedded words or its length less\", min_doc , \"\\n\")\n",
        "\n",
        "#deleting empty document if exist\n",
        "documents_embeddings ,labels = delete_empty_docs(documents_embeddings,labels,min_doc)\n",
        "print(\"the  New len of labels and docs \", len(labels),len(documents_embeddings),\"\\n\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the  len of labels and docs  8491 8491 \n",
            "\n",
            "deleting docs that have not embedded words or its length less 0 \n",
            "\n",
            "[]\n",
            "the  New len of labels and docs  8491 8491 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NA1crRuZT5E"
      },
      "source": [
        "# Bag of weighted Concepts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOb8CGAlxIrU"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "\n",
        "def data_preperation(dataset , labels, sample):\n",
        "  x_train , x_test , y_train , y_test = train_test_split(dataset , labels , test_size = sample , random_state = 0)\n",
        "  return array(x_train) , array(x_test) , array(y_train) , array(y_test)\n",
        "\n",
        "def train_ML(x_train , y_train, krenell):\n",
        "  #kernell = linear', 'poly', 'rbf', 'sigmoid',\n",
        "  svm =  SVC(kernel=kernell, random_state=44).fit(x_train , y_train)   \n",
        "  return svm\n",
        "\n",
        "def evaluation_ML(x_test , y_test , svm_model):\n",
        "  ev=svm_model.score(x_test , y_test)\n",
        "  return ev"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "869bhrJcim0p"
      },
      "source": [
        "### Documents vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ci2bgMM3NiDu",
        "outputId": "2d8c9214-bc1f-44d6-8b89-3e19c22e93b9"
      },
      "source": [
        "# Thershold similarity values\n",
        "Theta = [0.2,0.3,0.4,0.5,0.6]\n",
        "\n",
        "number_of_concept_words = 5 # how many words to compare from each cluster\n",
        "for alpha in Theta:\n",
        "  \n",
        "  min_idf = 0\n",
        "  max_idf_r=1\n",
        "  kernell='poly' \n",
        "  max_idf=max_idf_r* len(documents_embeddings)\n",
        "\n",
        "  print(\"----------------------------------------------------------------------------------start represnation with similarity threshild = \", alpha,  \":\\n\")\n",
        "  start_time = time.time()\n",
        "  tf_similarity = BoWC_representation(documents_embeddings, number_of_concept_words , alpha , centers , sorted_cons)\n",
        "\n",
        "  print(\"Counting IDF to select concepts with IDF less than \",max_idf, \":\\n\")\n",
        "\n",
        "  idfcount,idfexp=calculate_IDF(tf_similarity)\n",
        "  todelete=concept_to_delete(min_idf, max_idf,idfcount)\n",
        "  print(len(todelete),\" clusters to be deleted:  \", todelete ,\" \\n\")\n",
        "\n",
        "  new_tf_similarity,idfexp,idfcount= clac_new_Tf_idf(todelete,tf_similarity,idfcount,idfexp)\n",
        "  K_clusters=len(idfexp)\n",
        "  print(\"The new number of clusters: \",K_clusters, \":\\n\")\n",
        "\n",
        "  print(\"Calculating CF-IDF...\\n\")\n",
        "\n",
        "  final_embeddings=CF_IDF(new_tf_similarity,idfexp)\n",
        "\n",
        "  represnationtime=time.time() - start_time\n",
        "\n",
        "\n",
        "  print(\"Time for representation: \", represnationtime,\" seconds\")\n",
        "\n",
        "\n",
        "  print(\"Start training the SVM model with \",kernell, \" kernel ............................ \")\n",
        "\n",
        "\n",
        "  embedding_DF = pd.DataFrame({'class': labels, 'document': final_embeddings})\n",
        "\n",
        "\n",
        "  final_embeddings = [array(i).reshape(-1) for i in final_embeddings]\n",
        "  labels =embedding_DF['class'].tolist()\n",
        "\n",
        "  kernell='poly'\n",
        "\n",
        "  x_train , x_test , y_train , y_test = data_preperation(final_embeddings , labels, 0.30)\n",
        "  svm = train_ML(x_train , y_train,kernell)\n",
        "  y_pred=svm.predict(x_test)\n",
        "  F1 = f1_score(y_test,y_pred, average=\"micro\")\n",
        "\n",
        "  print(\"The micro average-weighted f1 score = \", F1)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------start represnation with similarity threshild =  0.2 :\n",
            "\n",
            "Counting IDF to select concepts with IDF less than  8491 :\n",
            "\n",
            "0  clusters to be deleted:   []  \n",
            "\n",
            "The new number of clusters:  50 :\n",
            "\n",
            "Calculating CF-IDF...\n",
            "\n",
            "Time for representation:  310.42463397979736  seconds\n",
            "Start training the SVM model with  poly  kernel ............................ \n",
            "The micro average-weighted f1 score =  0.9309262166405025\n",
            "----------------------------------------------------------------------------------start represnation with similarity threshild =  0.3 :\n",
            "\n",
            "Counting IDF to select concepts with IDF less than  8491 :\n",
            "\n",
            "0  clusters to be deleted:   []  \n",
            "\n",
            "The new number of clusters:  50 :\n",
            "\n",
            "Calculating CF-IDF...\n",
            "\n",
            "Time for representation:  310.7047817707062  seconds\n",
            "Start training the SVM model with  poly  kernel ............................ \n",
            "The micro average-weighted f1 score =  0.9207221350078493\n",
            "----------------------------------------------------------------------------------start represnation with similarity threshild =  0.4 :\n",
            "\n",
            "Counting IDF to select concepts with IDF less than  8491 :\n",
            "\n",
            "0  clusters to be deleted:   []  \n",
            "\n",
            "The new number of clusters:  50 :\n",
            "\n",
            "Calculating CF-IDF...\n",
            "\n",
            "Time for representation:  298.6129548549652  seconds\n",
            "Start training the SVM model with  poly  kernel ............................ \n",
            "The micro average-weighted f1 score =  0.9313186813186813\n",
            "----------------------------------------------------------------------------------start represnation with similarity threshild =  0.5 :\n",
            "\n",
            "Counting IDF to select concepts with IDF less than  8491 :\n",
            "\n",
            "0  clusters to be deleted:   []  \n",
            "\n",
            "The new number of clusters:  50 :\n",
            "\n",
            "Calculating CF-IDF...\n",
            "\n",
            "Time for representation:  267.68023800849915  seconds\n",
            "Start training the SVM model with  poly  kernel ............................ \n",
            "The micro average-weighted f1 score =  0.9250392464678179\n",
            "----------------------------------------------------------------------------------start represnation with similarity threshild =  0.6 :\n",
            "\n",
            "Counting IDF to select concepts with IDF less than  8491 :\n",
            "\n",
            "0  clusters to be deleted:   []  \n",
            "\n",
            "The new number of clusters:  50 :\n",
            "\n",
            "Calculating CF-IDF...\n",
            "\n",
            "Time for representation:  228.58673119544983  seconds\n",
            "Start training the SVM model with  poly  kernel ............................ \n",
            "The micro average-weighted f1 score =  0.8983516483516484\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bj8LKiLAz0t_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}