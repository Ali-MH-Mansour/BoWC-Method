{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GloVe_BoWC_Danial_21082021.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "AMafckFqxOiU"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ali-MH-Mansour/BoWC-Method/blob/main/codes/oldversions/GloVe_BoWC_Danial_21082021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34aOryuJLoRN"
      },
      "source": [
        "# 20 08 2021 **Glove**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gge2NxpEmGsT"
      },
      "source": [
        "# **Requirements  || Определение библиотек**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_I0ZTb-sqxH7",
        "outputId": "85b8b4f1-2e38-4625-eb3d-9ea3861dda96"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5btWsnJyjkJh"
      },
      "source": [
        "## Importing all Library here "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yPskImTLE1Q",
        "outputId": "74d4a9be-9266-49de-ad13-228877a4464e"
      },
      "source": [
        "################################ new\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk import word_tokenize, sent_tokenize, pos_tag\n",
        "\n",
        "def final_preprocess_text(text):\n",
        "    # 1. Tokenise to alphabetic tokens\n",
        "    text = remove_numbers(text)\n",
        "    text = remove_punctuation(text)\n",
        "    text = convert_to_lower(text)\n",
        "    text = remove_white_space(text)\n",
        "    text = remove_short_words(text)\n",
        "    tokens = toknizing(text)\n",
        "    #tokeniser = RegexpTokenizer(r'[A-Za-z]+')\n",
        "    #tokens = tokeniser.tokenize(text)\n",
        "    \n",
        "    # 2. POS tagging\n",
        "    pos_map = {'J': 'a', 'N': 'n', 'R': 'r', 'V': 'v'}\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    \n",
        "    # 3. Lowercase and lemmatise \n",
        "    lemmatiser = WordNetLemmatizer()\n",
        "    tokens = [lemmatiser.lemmatize(t.lower(), pos=pos_map.get(p[0], 'v')) for t, p in pos_tags]\n",
        "    return tokens\n",
        "#########################################################################\n",
        "def new_pre_processing_dataset(data):\n",
        "  All_tokens = [];\n",
        "  for i in range(len(data)):\n",
        "    All_tokens.append(final_preprocess_text(data.iloc[i][1]))\n",
        "  All_tokens = Convert_list_of_list_to_list(All_tokens)\n",
        "  print('the number of words in Crorpus' , len(All_tokens))\n",
        "\n",
        "  return All_tokens\n",
        "\n",
        "  ###########################################\n",
        "my_stopwords= [\"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\",\"aab\",'aaab', \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"affects\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"ain't\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appear\", \"appreciate\", \"appropriate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"aren't\", \"arise\", \"around\", \"as\", \"a's\", \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"can't\", \"cause\", \"causes\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"changes\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"c'mon\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"could\", \"couldn\", \"couldnt\", \"couldn't\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"c's\", \"ct\", \"cu\", \"currently\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"didn't\", \"different\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"done\", \"don't\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"effect\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"empty\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"h2\", \"h3\", \"had\", \"hadn\", \"hadn't\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"hed\", \"he'd\", \"he'll\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"here's\", \"hereupon\", \"hers\", \"herself\", \"hes\", \"he's\", \"hh\", \"hi\", \"hid\", \"him\", \"himself\", \"his\", \"hither\", \"hj\", \"ho\", \"home\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"how's\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"i'd\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"i'll\", \"im\", \"i'm\", \"immediate\", \"immediately\", \"importance\", \"important\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"invention\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"isn't\", \"it\", \"itd\", \"it'd\", \"it'll\", \"its\", \"it's\", \"itself\", \"iv\", \"i've\", \"ix\", \"iy\", \"iz\", \"j\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"know\", \"known\", \"knows\", \"ko\", \"l\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"let's\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mightn't\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"n\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needn\", \"needn't\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"nothing\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"other\", \"others\", \"otherwise\", \"ou\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"present\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"s2\", \"sa\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shan't\", \"she\", \"shed\", \"she'd\", \"she'll\", \"shes\", \"she's\", \"should\", \"shouldn\", \"shouldn't\", \"should've\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"system\", \"sz\", \"t\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"thats\", \"that's\", \"that've\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"there's\", \"thereto\", \"thereupon\", \"there've\", \"these\", \"they\", \"theyd\", \"they'd\", \"they'll\", \"theyre\", \"they're\", \"they've\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"t's\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"ut\", \"v\", \"va\", \"value\", \"various\", \"vd\", \"ve\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"wa\", \"want\", \"wants\", \"was\", \"wasn\", \"wasnt\", \"wasn't\", \"way\", \"we\", \"wed\", \"we'd\", \"welcome\", \"well\", \"we'll\", \"well-b\", \"went\", \"were\", \"we're\", \"weren\", \"werent\", \"weren't\", \"we've\", \"what\", \"whatever\", \"what'll\", \"whats\", \"what's\", \"when\", \"whence\", \"whenever\", \"when's\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"where's\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whom\", \"whomever\", \"whos\", \"who's\", \"whose\", \"why\", \"why's\", \"wi\", \"widely\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"won't\", \"words\", \"world\", \"would\", \"wouldn\", \"wouldnt\", \"wouldn't\", \"www\", \"x\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"you'd\", \"you'll\", \"your\", \"youre\", \"you're\", \"yours\", \"yourself\", \"yourselves\", \"you've\", \"yr\", \"ys\", \"yt\", \"z\", \"zero\", \"zi\", \"zz\"]\n",
        "\n",
        "!pip install sklearn\n",
        "!pip install spherecluster\n",
        "!pip install soyclustering\n",
        "#################################### Importing all Library here  ################################################\n",
        "from soyclustering import SphericalKMeans\n",
        "import time\n",
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from numpy import array \n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords \n",
        "from numpy import array\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import gensim.downloader\n",
        "from collections import Counter \n",
        "import functools\n",
        "import math\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.sparse import csr_matrix\n",
        "#from sklearn_extra.cluster import KMedoids\n",
        "#from spherecluster import SphericalKMeans\n",
        "############ Our Downloads ##########\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "##################################\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "import numpy as np\n",
        "np.random.seed(2018)\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "############################################\n",
        "#machine learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "##################################  Defining Our Methods Here    ##################################################\n",
        "# Save the pre_trained model embeddings.. \n",
        "def save_embeddings(model, file_path):\n",
        "  word_vectors = model.wv\n",
        "  word_vectors.save(file_path)\n",
        "####################################################################################\n",
        "### loading the embeddings\n",
        "def Load_pre_trained_model_embeddings(file_path):\n",
        "  wv = KeyedVectors.load(file_path, mmap='r')\n",
        "  return wv\n",
        "####################################################################################\n",
        "### install pre_trained_model\n",
        "def intall_pre_trained_model(file_path, model_name):\n",
        "  model = gensim.downloader.load(model_name)\n",
        "  save_embeddings(model , file_path)\n",
        "  return model\n",
        "  print('the models are intalled and saved in the \" ' + file_path + '\"')\n",
        "####################################################################################\n",
        "def convert_to_lower(text):\n",
        "  return text.lower()\n",
        "####################################################################################\n",
        "def remove_numbers(text):\n",
        "  text = re.sub(r'\\d+' , '', text)\n",
        "  return text\n",
        "#################################################\n",
        "def remove_short_words(text):\n",
        "  text = re.sub(r'\\b\\w{1,2}\\b', '', text)\n",
        "  return text\n",
        "####################################################################################\n",
        "def remove_punctuation(text):\n",
        "     punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^+&*_~'''\n",
        "     no_punct = \"\"\n",
        "     for char in text:\n",
        "        if char not in punctuations:\n",
        "            no_punct = no_punct + char\n",
        "     return no_punct\n",
        "####################################################################################\n",
        "def remove_white_space(text):\n",
        "  text = text.strip()\n",
        "  return text\n",
        "####################################################################################\n",
        "def toknizing(text):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = word_tokenize(text)\n",
        "  ## Remove Stopwords from tokens\n",
        "  result = [i for i in tokens if not i in stop_words]\n",
        "  return result\n",
        "###################################################################################\n",
        "def remove_duplication(text):\n",
        "  return list(set(text)) \n",
        "####################################################################################\n",
        "def pre_processing(text):\n",
        "  text = remove_numbers(text)\n",
        "  text = remove_punctuation(text)\n",
        "  text = convert_to_lower(text)\n",
        "  text = remove_white_space(text)\n",
        "  text = remove_short_words(text)\n",
        "  #text = preprocess_toklem(text)\n",
        "  text = toknizing(text)\n",
        "  return text \n",
        "#######################################\n",
        "def pre_processing_no_toknize(text):\n",
        "  text = remove_numbers(text)\n",
        "  text = remove_punctuation(text)\n",
        "  text = convert_to_lower(text)\n",
        "  text = remove_white_space(text)\n",
        "  text = remove_short_words(text)\n",
        "  #text = preprocess_toklem(text)\n",
        "  #text = toknizing(text)\n",
        "  return text\n",
        "####################################################################################\n",
        "def lemmatize_stemming(text):\n",
        "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
        "#######\n",
        "def preprocess_toklem(text):\n",
        "    result = []\n",
        "    for token in gensim.utils.simple_preprocess(text):\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) >= 3:\n",
        "            result.append(lemmatize_stemming(token))\n",
        "    return result\n",
        "\n",
        "def plot_pie_distribution(labels , metric, name):\n",
        "      # Create figure and plot pie:\n",
        "      fig = plt.figure(figsize=(4,4))\n",
        "      colors = ['skyblue', 'peru', 'gray' , 'red' , 'blue']\n",
        "      plt.pie(metric, labels = labels , autopct='%d%%', colors=colors)\n",
        "      plt.axis('equal')\n",
        "      plt.title(name + ' Distribution', fontsize='20')\n",
        "      plt.savefig('plot_eight.png')\n",
        "      plt.show()\n",
        "########################\n",
        "#stem single token\n",
        "def stem_token(alltoken):\n",
        "    result = []\n",
        "    for token in alltoken:\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS:\n",
        "            result.append(lemmatize_stemming(token))\n",
        "    return result\n",
        "####################################################################################\n",
        "def Convert_list_of_list_to_list(all_tokens):\n",
        "  tokens = []\n",
        "  for words in all_tokens:\n",
        "    for i in words:\n",
        "      tokens.append(i)\n",
        "  return tokens\n",
        "#########################################################3\n",
        "def pre_processing_dataset(data):\n",
        "  All_tokens = [];\n",
        "  for i in range(len(data)):\n",
        "    All_tokens.append(pre_processing(data.iloc[i][1]))\n",
        "  All_tokens = Convert_list_of_list_to_list(All_tokens)\n",
        "  print('the number of words in Crorpus' , len(All_tokens))\n",
        "\n",
        "  return All_tokens\n",
        "\n",
        "#########################################################################################\n",
        "def Create_embedding_matrix(model , tokens , embedding_dim):\n",
        "    embedding_matrix = []\n",
        "    irregular_words  = []\n",
        "    for words in tokens:\n",
        "          try :\n",
        "            temp = model.wv[words]\n",
        "            embedding_matrix.append(temp)\n",
        "          except KeyError:\n",
        "            # for the words that are not in model vocabulary\n",
        "            # we initilize with random vector\n",
        "            irregular_words.append(words)\n",
        "            #embedding_matrix.append(np.random.normal(0,np.sqrt(0.300),embedding_dim))\n",
        "    return embedding_matrix , irregular_words\n",
        "\n",
        "\n",
        "#################################################################################\n",
        "def Create_embeddingTFIDF_matrix(model , tokens , embedding_dim, df_tfidf):\n",
        "    embedding_matrix = []\n",
        "    irregular_words  = []\n",
        "    words_tfidf =[]\n",
        "    for word in tokens:\n",
        "          try :\n",
        "            temp = model.wv[word]\n",
        "            embedding_matrix.append(temp)\n",
        "            words_tfidf.append(max(df_tfidf[word]))\n",
        "          except KeyError:\n",
        "            # for the words that are not in model vocabulary\n",
        "            irregular_words.append(word)\n",
        "    return embedding_matrix , irregular_words, words_tfidf\n",
        "\n",
        "###########################################################################################\n",
        "# K-means Clustering the embeddings #\n",
        "def embeddings_k_means_clustering(embeddings_vectors, N_clusters):\n",
        "  #kmedoids = KMedoids(n_clusters=5, random_state=0).fit(embeddings_vectors)\n",
        "  #SphericalKMeans\n",
        "  kmeans = KMeans(n_clusters= N_clusters , random_state=0).fit(embeddings_vectors)\n",
        "  return kmeans\n",
        "\n",
        "\n",
        "########################################################################################\n",
        "def spherical_kmeans(n_clusters_, embeddings_matrix,sim_digree):\n",
        "  embeddings_matrix_csr = csr_matrix(embeddings_matrix)\n",
        "  spherical_kmeans = SphericalKMeans( max_similar=sim_digree, init='similar_cut', \n",
        "      n_clusters = n_clusters_)\n",
        "  labels = spherical_kmeans.fit_predict(embeddings_matrix_csr)\n",
        "  centers = spherical_kmeans.cluster_centers_\n",
        "  return labels , centers\n",
        "\n",
        "############################################################################################\n",
        "def dimentionality_reduction(embeddings_vectors):\n",
        "  pca = PCA(3).fit(embeddings_vectors)\n",
        "  pca_data = pd.DataFrame(pca.transform(embeddings_vectors)) \n",
        "  return pca_data\n",
        "\n",
        "#############################################################################################\n",
        "def clusters_visualization( _3D_data , labels):\n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_subplot(111, projection='3d')\n",
        "  ax.scatter(_3D_data[0], _3D_data[1], _3D_data[2], c = labels,  s=50, cmap='viridis')\n",
        "  centers = k_means.cluster_centers_\n",
        "  ax.scatter(centers[0], centers[1], centers[2] ,  c='red', s=200, alpha=0.8);\n",
        "  ax.set_xlabel('X Label')\n",
        "  ax.set_ylabel('Y Label')\n",
        "  ax.set_zlabel('Z Label')\n",
        "  plt.show()\n",
        "#############################################################################################\n",
        "def clusters_properties(k_means):\n",
        "  print(Counter(k_means.labels_))\n",
        "  centers = k_means.cluster_centers_\n",
        "  '''\n",
        "  Return\n",
        "     - Number of samples for each cluter ,\n",
        "     - The Centers of each cluster\n",
        "  '''\n",
        "  return Counter(k_means.labels_) , centers\n",
        "#############################################################################################\n",
        "def Save_words_with_Embeddings(tokens , pre_model):\n",
        "  dic = {}\n",
        "  for words in tokens: \n",
        "      try :\n",
        "        dic.update({words :  model.word_vectors[glove.dictionary[words]]})\n",
        "      except: \n",
        "        continue;\n",
        "  return dic\n",
        "##############################################################################################\n",
        "### this function take the vector and return the word that linked with it.\n",
        "def Vector_to_Word(vector , dic):\n",
        "  word = ''\n",
        "  for i in dic:\n",
        "    if functools.reduce(lambda x, y : x and y, map(lambda p, q: p == q,list(dic[i]),vector), True): \n",
        "      word = i\n",
        "  return word\n",
        "##############################################################################################\n",
        "### get list of list array, where each list contain the groub of element the belong to specific cluster\n",
        "### C * n_sample for each cluster\n",
        "\n",
        "# updated\n",
        "def get_cluster_elements(labels , n_clusters , embedding_matrix):\n",
        "  # Nice Pythonic way to get the indices of the points for each corresponding cluster\n",
        "  mydict = {i: np.where(labels == i)[0] for i in range(n_clusters)}\n",
        "  embeddings_samples = []\n",
        "  for i in range(len(mydict)):\n",
        "    temp= []\n",
        "    for j in mydict[i]:\n",
        "      temp.append(embedding_matrix[j])\n",
        "    embeddings_samples.append(temp)\n",
        "    del temp\n",
        "  return embeddings_samples\n",
        "\n",
        "############################################################################################\n",
        "def get_soreted_dictionary(centers , dictionary , M):\n",
        "  dic_with_similarity =[]\n",
        "  for i in range(len(dictionary)):\n",
        "    temp = []\n",
        "    for j in range(len(dictionary[i])):\n",
        "      temp.append([dictionary[i][j] , cosine_similarity([dictionary[i][j]] , [centers[i]])])\n",
        "    dic_with_similarity.append(temp)\n",
        "    del temp\n",
        "      ####### Sort all vectors clusters ############\n",
        "  sorted_list = []\n",
        "  def take_second(elem):\n",
        "      return elem[1]\n",
        "  for i in range(len(dic_with_similarity)):\n",
        "      sorted_list.append(sorted(dic_with_similarity[i], key=take_second , reverse = True))\n",
        "    ########### Get the hightest M features that related to the its centers among whole concepet dictionary#############\n",
        "  Top_M_Concept = [sorted_list[i][:M] for i in range(len(dic_with_similarity))]\n",
        "  return Top_M_Concept\n",
        "\n",
        "############################################################################################\n",
        "# compare each words in documents with all clusters words.\n",
        "def Get_similarity_matrix(v1 , list_of_vectors):\n",
        "  sim = []\n",
        "  s=0\n",
        "  for i in list_of_vectors:\n",
        "    sim.append(cosine_distance(v1, i[0]))\n",
        "  sim = array(sim).astype('float').reshape(-1).tolist()\n",
        "  if len(sim)!=0:\n",
        "    s=sum(sim)/len(sim)\n",
        "  return s\n",
        "\n",
        "### convert document to Vector with size C * M \n",
        "##############################################################################################\n",
        "##############################################################################################\n",
        "def Doc2Vec(embeddeings_document,threshold_sim,alpha,centers,dictionary):\n",
        "  M=2\n",
        "  N=10\n",
        "  concept_freq = 0\n",
        "  ## intilize doc array\n",
        "  doc2vec = np.zeros((len(centers), M)).tolist()\n",
        "  final_vector=np.zeros(len(centers)).tolist()\n",
        "  for cnt in range(len(centers)):\n",
        "    counter = 0;\n",
        "    sim =[]\n",
        "    NN=0\n",
        "    for emb_word in embeddeings_document:\n",
        "      Wsim =cosine_distance(emb_word, centers[cnt])# التشابه مع اقرب كلمة للمركز\n",
        "      if Wsim  > alpha and NN<=N:\n",
        "        NN=NN+1\n",
        "        concept_freq = concept_freq+1 #for tf\n",
        "        Smean =Get_similarity_matrix(emb_word , dictionary[cnt][0:threshold_sim-1]);\n",
        "        sim.append(float((Smean+Wsim)/2))\n",
        "        doc2vec[cnt][M-1] = doc2vec[cnt][M-1] + 1;\n",
        "      elif Wsim  > alpha and NN>N:\n",
        "        concept_freq = concept_freq+1 #for tf\n",
        "        sim.append(Wsim)\n",
        "        doc2vec[cnt][M-1] = doc2vec[cnt][M-1] + 1;\n",
        "      else:\n",
        "        continue;\n",
        "      counter = counter +1;\n",
        "    # end for doc words with one concepts\n",
        "    meanofSim=0#  المتوسط لدرجات تشابه كلمات المستند مع المفهوم الحالي\n",
        "    if(len(sim)!=0):\n",
        "      meanofSim=sum(sim)/len(sim)\n",
        "    doc2vec[cnt][M-2]=meanofSim\n",
        "    del sim\n",
        "#* math.exp(i[M-2])\n",
        "  if concept_freq !=0: # the number of occurence for all concept in this document\n",
        "    for i in doc2vec:\n",
        "      #print(i[M-1], concept_freq, 'sim',i[M-2],'exp',math.exp(i[M-2]))\n",
        "      i[M-1] = i[M-1]* math.exp(i[M-2]) / concept_freq\n",
        "      #print(i[M-1])\n",
        "  doc2vec1=np.array(doc2vec,dtype=object)\n",
        "  final_vector=doc2vec1[:,1].tolist() \n",
        "\n",
        "  return final_vector\n",
        "\n",
        "\n",
        "#############################\n",
        "def Doc2VecTFIDF(embeddeings_tfidf,threshold_sim,alpha,centers,dictionary):\n",
        "  M=2\n",
        "  N=10\n",
        "  concept_freq = 0\n",
        "  ## intilize doc array\n",
        "  doc2vec = np.zeros((len(centers), M)).tolist()\n",
        "  final_vector=np.zeros(len(centers)).tolist()\n",
        "  for cnt in range(len(centers)):\n",
        "    counter = 0;\n",
        "    sim =[]\n",
        "    concept_tf_idf=[]\n",
        "    NN=0\n",
        "    for emb_word in embeddeings_tfidf:    \n",
        "      #emb_word هذه تحتوي تضمين ووزن تفايدف    \n",
        "      #emb_word[0] embedding\n",
        "      #emb_word[1] tfidf       \n",
        "      Wsim =cosine_distance(emb_word[0], dictionary[cnt][0][0])# التشابه مع اقرب كلمة للمركز\n",
        "      if Wsim  > alpha and NN<=N:\n",
        "        NN=NN+1\n",
        "        concept_freq = concept_freq+1 #for tf\n",
        "        #Smean =Get_similarity_matrix(emb_word , dictionary[cnt][1:threshold_sim-1]);\n",
        "        #sim.append(float((Smean+Wsim)/2))\n",
        "        #new\n",
        "        concept_tf_idf.append(emb_word[1])\n",
        "        doc2vec[cnt][M-1] = doc2vec[cnt][M-1] + 1;\n",
        "      elif Wsim  > alpha and NN>N:\n",
        "        concept_freq = concept_freq+1 #for tf\n",
        "        #sim.append(Wsim)\n",
        "        concept_tf_idf.append(emb_word[1])\n",
        "        doc2vec[cnt][M-1] = doc2vec[cnt][M-1] + 1;\n",
        "      else:\n",
        "        continue;\n",
        "      counter = counter +1;\n",
        "    # end for doc words with one concepts\n",
        "    #meanofSim=0#  المتوسط لدرجات تشابه كلمات المستند مع المفهوم الحالي\n",
        "    #if(len(sim)!=0):\n",
        "     # meanofSim=sum(sim)/len(sim)\n",
        "    #mean_of_tf_idf=0\n",
        "    if len(concept_tf_idf)>0:\n",
        "      mean_of_tf_idf=sum(concept_tf_idf)/len(concept_tf_idf)\n",
        "    #doc2vec[cnt][M-2]=meanofSim\n",
        "    doc2vec[cnt][M-2] = mean_of_tf_idf\n",
        "    del concept_tf_idf\n",
        "    #del sim\n",
        "#* math.exp(i[M-2])\n",
        "  if concept_freq !=0: # the number of occurence for all concept in this document\n",
        "    for i in doc2vec:\n",
        "      i[M-1] = i[M-2]  # خزن محل تف متوسط اوزان تفات كلمات المفهوم\n",
        "      #i[M-1] = i[M-1] * math.exp(i[M-2]) / concept_freq   #calculte tf\n",
        "\n",
        "      \n",
        "  doc2vec1=np.array(doc2vec,dtype=object)\n",
        "  final_vector=doc2vec1[:,1].tolist() \n",
        "\n",
        "  return final_vector\n",
        "#################################\n",
        "def Doc2Vec2D(embeddeings_document,threshold_sim,alpha,centers,dictionary):\n",
        "  M=2\n",
        "  concept_freq = 0\n",
        "  ## intilize doc array\n",
        "  doc2vec = np.zeros((len(centers), M)).tolist()\n",
        "  final_vector=np.zeros(len(centers)).tolist()\n",
        "  for cnt in range(len(centers)):\n",
        "    counter = 0;\n",
        "    sim =[]\n",
        "    for emb_word in embeddeings_document:\n",
        "      Smean =Get_similarity_matrix(emb_word , dictionary[cnt][0:threshold_sim-1]);\n",
        "      if Smean  > alpha:\n",
        "        concept_freq = concept_freq+1 #for tf\n",
        "        sim.append(float(Smean))\n",
        "        doc2vec[cnt][M-1] = doc2vec[cnt][M-1] + 1;\n",
        "      else:\n",
        "        continue;\n",
        "      counter = counter +1;\n",
        "    # end for doc words with one concepts\n",
        "    meanofSim=0#  المتوسط لدرجات تشابه كلمات المستند مع المفهوم الحالي\n",
        "    if(len(sim)!=0):\n",
        "      meanofSim=sum(sim)/len(sim)\n",
        "    doc2vec[cnt][M-2]=meanofSim\n",
        "    del sim\n",
        "#* math.exp(i[M-2])\n",
        "  if concept_freq !=0: # the number of occurence for all concept in this document\n",
        "    for i in doc2vec:\n",
        "      #print(i[M-1], concept_freq, 'sim',i[M-2],'exp',math.exp(i[M-2]))\n",
        "      i[M-1] = i[M-1] / concept_freq\n",
        "      #print(i[M-1])\n",
        "  doc2vec1=np.array(doc2vec)\n",
        "  final_vector=doc2vec1[:,1].tolist() \n",
        "\n",
        "  return doc2vec\n",
        "  #######################\n",
        "def DDDoc2Vec(embeddeings_document,threshold_sim,alpha,centers,dictionary):\n",
        "  M=2\n",
        "  concept_freq = 0\n",
        "  ## intilize doc array\n",
        "  doc2vec = np.zeros((len(centers), M)).tolist()\n",
        "  final_vector=np.zeros(len(centers)).tolist()\n",
        "  for cnt in range(len(centers)):\n",
        "    counter = 0;\n",
        "    sim =[]\n",
        "    for emb_word in embeddeings_document:\n",
        "      Wsim1 =cosine_similarity([emb_word], [dictionary[cnt][0][0]])\n",
        "      Wsim2 =cosine_similarity([emb_word], [dictionary[cnt][1][0]])\n",
        "      Wsim3 =cosine_similarity([emb_word], [dictionary[cnt][2][0]])\n",
        "      Smean=float((Wsim1+Wsim2+Wsim3)/3)\n",
        "      #print('Smean', Smean)\n",
        "      if Smean  > alpha:\n",
        "        concept_freq = concept_freq+1 #for tf\n",
        "        sim.append(Smean)\n",
        "        doc2vec[cnt][M-1] = doc2vec[cnt][M-1] + 1;\n",
        "      else:\n",
        "        continue;\n",
        "      counter = counter +1;\n",
        "    # end for doc words with one concepts\n",
        "    meanofSim=0#  المتوسط لدرجات تشابه كلمات المستند مع المفهوم الحالي\n",
        "    lsim=len(sim)\n",
        "    if lsim>0:\n",
        "      meanofSim=sum(sim)/lsim\n",
        "    #print('Mean of sum',meanofSim,'sim',len(sim),doc2vec[cnt][M-1])\n",
        "    doc2vec[cnt][M-2]=meanofSim\n",
        "    del sim\n",
        "\n",
        "  if concept_freq !=0: # the number of occurence for all concept in this document\n",
        "    for i in doc2vec:\n",
        "      #print(i[M-1], concept_freq, 'sim',i[M-2],'exp',math.exp(i[M-2]))\n",
        "      i[M-1] = i[M-1]* math.exp(i[M-2]) / concept_freq\n",
        "  doc2vec1=np.array(doc2vec)\n",
        "  final_vector=doc2vec1[:,1].tolist() \n",
        "\n",
        "  return final_vector\n",
        "#####################################################################################\n",
        "\n",
        "\n",
        "\n",
        "#######################################################################################\n",
        "## convert vectors to words for each cluster (M=10 words)\n",
        "# K number of clsuters\n",
        "def dictAsText(K,M,sordted_concepts,dictionary):\n",
        "  textclusters = np.zeros((K, M)).tolist()\n",
        "  for i in range(K):\n",
        "    num_of_words=len(sorted_cons[i][0:M])\n",
        "    for j in range(num_of_words):\n",
        "      textclusters[i][j]=Vector_to_Word(sordted_concepts[i][j][0],dictionary)\n",
        "  return textclusters\n",
        "\n",
        "#######################################\n",
        "# WMD we need to use whole cluster not only 10 words\n",
        "# dictionarAsstring as input to Doc2WMD\n",
        "\n",
        "def Doc2WMD(model,text_document,embeddeings_document , M , alpha , centers , textclusters,dic):\n",
        "  #doc array\n",
        "  doc2WMD_only = np.zeros(len(centers)).tolist()\n",
        "  doc2WMD_with_Concept = np.zeros(( len(centers), M)).tolist()\n",
        "  counter=0;\n",
        "  #start for conepts\n",
        "  for cnt in range(len(centers)):\n",
        "    for emb_word in embeddeings_document:\n",
        "      s=cosine_similarity([emb_word] , [centers[cnt]])\n",
        "      if ((s  > alpha)):\n",
        "        doc2WMD_with_Concept[cnt][M-1] = doc2WMD_with_Concept[cnt][M-1] + 1 #frequency++\n",
        "      else:\n",
        "        continue;\n",
        "    counter=counter+1;\n",
        "    #end for \n",
        "    # get WMD ((words) dictionary[cnt])\n",
        "    doc1=text_document\n",
        "    doc2 = ' '.join(textclusters[cnt])\n",
        "    #print('counter', counter ,'doc1' ,doc1,' \\n doc2', doc2 ,'\\n')\n",
        "    distance = model.wmdistance(doc1.split(), doc2.split())\n",
        "    doc2WMD_with_Concept[cnt][M-2]= distance\n",
        "    doc2WMD_only[cnt]=distance\n",
        "    #empty strings\n",
        "    doc2=\"\"\n",
        "  #claculate tf-idf\n",
        "  get_concept_freq(doc2WMD_with_Concept)\n",
        "  # normalize vector\n",
        "  WMD_only,bysum=normalize_vector(doc2WMD_only)\n",
        "  doc2WMD_with_Concept=normalize_coloumn_in_2Darray(doc2WMD_with_Concept, 0)\n",
        "  return doc2WMD_with_Concept,WMD_only\n",
        "\n",
        "###################################################################\n",
        "def cosine_distance(u, v):\n",
        "  return np.dot(u, v) / (math.sqrt(np.dot(u, u)) * math.sqrt(np.dot(v, v)))\n",
        "\n",
        "#####################################################################################\n",
        "#TF-idf\n",
        "def get_concept_freq(array2D):\n",
        "  s=0;\n",
        "  for row in array2D:            \n",
        "    s=s+row[len(row)-1]\n",
        "  if s !=0:\n",
        "    for row in array2D:\n",
        "      row[len(row)-1]=row[len(row)-1]/s\n",
        "  return array2D\n",
        "#########################################33\n",
        "def normalize_vector(vect):\n",
        "  bymax=[float(i)/max(vect) for i in vect]\n",
        "  bysum=[float(i)/sum(vect) for i in vect]\n",
        "  return bymax,bysum\n",
        "\n",
        "#####################################################\n",
        "def normalize_coloumn_in_2Darray(array2D, index_column):\n",
        "  arr=np.array(array2D)\n",
        "  list_col=arr[:,index_column]\n",
        "  s=max(list_col) # or sum\n",
        "  #print(' col ', list_col , ' max ' , s)\n",
        "  if s !=0:\n",
        "    for row in array2D:\n",
        "      row[index_column]=float(row[index_column])/s\n",
        "  return array2D\n",
        "\n",
        "\n",
        "\n",
        "####################################################33333\n",
        "def embedding_all_documents(df , model,N):\n",
        "  documents_embeddings = []\n",
        "  for i in range(len(df)):\n",
        "    documents_embeddings.append(convert_doc_to_embedding(df['document'][i] , model,N))\n",
        "  return documents_embeddings\n",
        "\n",
        "########################################################################################################\n",
        "def convert_doc_to_embedding(text , model,N):\n",
        "  tokens = final_preprocess_text(text)\n",
        "  if N!=0:\n",
        "    tokens=removeElements(tokens,N)\n",
        "  embeddings_matrix  , irrugular_words = Create_embedding_matrix(model , tokens , 300)\n",
        "  return embeddings_matrix\n",
        "#########################################################################################################\n",
        "def RepresentationNew(documents_embeddings , sim_val , alpha , centers , dictionary):\n",
        "  embeddings = []\n",
        "  for i in documents_embeddings: \n",
        "    v = Doc2Vec(i ,sim_val , alpha , centers , dictionary)\n",
        "    embeddings.append(v)\n",
        "  return embeddings\n",
        "##########################################\n",
        "\n",
        "def calculate_IDF(doc_vectors):\n",
        "  docL=len(doc_vectors[0])\n",
        "  N=len(doc_vectors) #collection size\n",
        "  IDf = np.zeros(docL).tolist() # بطول العناقيد\n",
        "  for doc in doc_vectors:\n",
        "    counter=0 # counting concepts\n",
        "    for concept_freature in doc:\n",
        "      if concept_freature > 0:\n",
        "        IDf[counter] = IDf[counter] + 1 #  i[M-1]\n",
        "      counter = counter+1\n",
        "  \n",
        "  #ضرب حساب التردد\n",
        "  newIDF=[]\n",
        "  for i in IDf:\n",
        "    if i!=0:\n",
        "      newIDF.append( math.exp(-1*(i/N)))\n",
        "    else:\n",
        "      newIDF.append(0)\n",
        "  return IDf, newIDF\n",
        "\n",
        "########################################### \n",
        "#Calculate Cf-IDF\n",
        "def CF_IDF(doc_vectors,IDF):\n",
        "  docL=len(doc_vectors[0])\n",
        "  for i in range(len(doc_vectors)):\n",
        "    for j in range(docL):\n",
        "       doc_vectors[i][j] = doc_vectors[i][j] * IDF[j]\n",
        "  return doc_vectors\n",
        "\n",
        "#################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################333\n",
        "def IDFbySimNoLog(documents_embeddings , sim_val , alpha , centers , dictionary):\n",
        "  embeddings = []\n",
        "  counter = 0\n",
        "  IDf = np.zeros(len(centers)).tolist() # بطول العناقيد\n",
        "  for i in documents_embeddings: \n",
        "    v = Doc2Vec2D(i ,sim_val , alpha , centers , dictionary)\n",
        "    ######## حساب عدد مرات ظهور المفهوم في كل نص\n",
        "    for i in v:   # i is concept line , i need class of doc v\n",
        "      if i > 0:\n",
        "        IDf[counter] = IDf[counter] + 1 #  i[M-1]\n",
        "      counter = counter+1\n",
        "    counter = 0\n",
        "    embeddings.append(v)\n",
        "  print(IDf)\n",
        "  ######## Calculate Cf-IDF\n",
        "  print(len(embeddings))\n",
        "  for i in range(len(embeddings)):\n",
        "    l=len(embeddings[i])\n",
        "    for j in range(l):\n",
        "    #for doc i: muti cell j with idf j \n",
        "       #print('i', i,' j ', j, ' = ', embeddings[i][j] * newIDF[j])\n",
        "       embeddings[i][j] = embeddings[i][j] * IDf[j]\n",
        "  \n",
        "  return embeddings\n",
        "#########################################################################################################\n",
        "\n",
        "def RepresentationNormNoLog(documents_embeddings , sim_val , alpha , centers , dictionary):\n",
        "  embeddings = []\n",
        "  counter = 0\n",
        "  IDf = np.zeros(len(centers)).tolist() # بطول العناقيد\n",
        "  for i in documents_embeddings: \n",
        "    v = Doc2Vec(i ,sim_val , alpha , centers , dictionary)\n",
        "    ######## حساب عدد مرات ظهور المفهوم في كل نص\n",
        "    for i in v:   # i is concept line , i need class of doc v\n",
        "      if i > 0:\n",
        "        IDf[counter] = IDf[counter] + 1 #  i[M-1]\n",
        "      counter = counter+1\n",
        "    counter = 0\n",
        "    embeddings.append(v)\n",
        "  print(IDf)\n",
        "  ######## Calculate Cf-IDF\n",
        "  print(len(embeddings))\n",
        "  for i in range(len(embeddings)):\n",
        "    l=len(embeddings[i])\n",
        "    for j in range(l):\n",
        "    #for doc i: muti cell j with idf j \n",
        "       #print('i', i,' j ', j, ' = ', embeddings[i][j] * newIDF[j])\n",
        "       embeddings[i][j] = embeddings[i][j] * IDf[j]\n",
        "  \n",
        "  return embeddings\n",
        "###############\n",
        "\n",
        "  ##################################3\n",
        "\n",
        "def RepresentationAccNoLog(documents_embeddings , sim_val , alpha , centers , dictionary):\n",
        "  embeddings = []\n",
        "  counter = 0\n",
        "  IDf = np.zeros(len(centers)).tolist() # بطول العناقيد\n",
        "  for i in documents_embeddings: \n",
        "    v = Doc2Vec(i ,sim_val , alpha , centers , dictionary)\n",
        "    ######## حساب عدد مرات ظهور المفهوم في كل نص\n",
        "    for i in v:   # i is concept line , i need class of doc v\n",
        "      if i > 0:\n",
        "        IDf[counter] = IDf[counter] + i #  i[M-1]\n",
        "      counter = counter+1\n",
        "    counter = 0\n",
        "    embeddings.append(v)\n",
        "  \n",
        "  ######## Calculate Cf-IDF\n",
        "  print(len(embeddings))\n",
        "  for i in range(len(embeddings)):\n",
        "    l=len(embeddings[i])\n",
        "    for j in range(l):\n",
        "    #for doc i: muti cell j with idf j \n",
        "       #print('i', i,' j ', j, ' = ', embeddings[i][j] * newIDF[j])\n",
        "       embeddings[i][j] = embeddings[i][j] * IDf[j]\n",
        "  \n",
        "  return embeddings\n",
        "###########################################\n",
        "\n",
        "def RepresentationAccLog(documents_embeddings , sim_val , alpha , centers , dictionary):\n",
        "  embeddings = []\n",
        "  counter = 0\n",
        "  IDf = np.zeros(len(centers)).tolist() # بطول العناقيد\n",
        "  for i in documents_embeddings: \n",
        "    v = Doc2Vec(i ,sim_val , alpha , centers , dictionary)\n",
        "    ######## حساب عدد مرات ظهور المفهوم في كل نص\n",
        "    for i in v:   # i is concept line , i need class of doc v\n",
        "      if i > 0:\n",
        "        IDf[counter] = IDf[counter] + i #  i[M-1]\n",
        "      counter = counter+1\n",
        "    counter = 0\n",
        "    embeddings.append(v)\n",
        "  #print('oldidf',IDf,'\\n')\n",
        "  #########  calculate IDF\n",
        "  #newIDF=[]\n",
        "  for i in IDf:\n",
        "    if i!=0:\n",
        "      N=len(embeddings)\n",
        "      newIDF.append( math.log10( (N+1) /i ) )\n",
        "    else:\n",
        "      newIDF.append(0)\n",
        "  #print('newidf',len(newIDF),'\\n')\n",
        "  \n",
        "  ######## Calculate Cf-IDF\n",
        "  print(len(embeddings))\n",
        "  for i in range(len(embeddings)):\n",
        "    l=len(embeddings[i])\n",
        "    for j in range(l):\n",
        "    #for doc i: muti cell j with idf j \n",
        "       #print('i', i,' j ', j, ' = ', embeddings[i][j] * newIDF[j])\n",
        "       embeddings[i][j] = embeddings[i][j] * newIDF[j]\n",
        "  \n",
        "  return embeddings\n",
        "\n",
        "  #######################################\n",
        "def saveList(myList,filename):\n",
        "    # the filename should mention the extension 'npy'\n",
        "    np.save(filename,myList)\n",
        "    print(\"Saved successfully!\")\n",
        "def loadList(filename):\n",
        "    # the filename should mention the extension 'npy'\n",
        "    tempNumpyArray=np.load(filename)\n",
        "    return tempNumpyArray.tolist()\n",
        "\n",
        "###############################################\n",
        "#######################################################\n",
        "# number of doc in each class \n",
        "def DataBalance(data):\n",
        "  data['class'] = pd.factorize(data['class'])[0]\n",
        "  uniqueValues = data['class'].unique()\n",
        "  l=len(uniqueValues)\n",
        "  labels_freq = np.zeros(l).tolist()\n",
        "  for i in range(len(data)):\n",
        "    label= data.iloc[i][0]\n",
        "    labels_freq[label]=labels_freq[label]+1\n",
        "  return labels_freq \n",
        "\n",
        "#BBC_data['vector']=BBC_data['text'].apply(BoET,args=(model,centers , sorted_cons,0.44,2,10))\n",
        "\n",
        "\n",
        "###############################\n",
        "def document_class_of_concept(orginalDF,data_DF, doc_vectors): \n",
        "  #number of docs in each class\n",
        "  doc_in_class=DataBalance(orginalDF)\n",
        "  # number of classes\n",
        "  L_labels=len(doc_in_class)\n",
        "  K_clusters=len(doc_vectors[0])\n",
        "  #array for freq  \n",
        "  freq_array = np.zeros((L_labels, K_clusters)).tolist() \n",
        "  i=0 # doc counter\n",
        "  for d in doc_vectors:\n",
        "    curren_class=data_DF['class'][i]\n",
        "    counter=0 #idf counter within doc\n",
        "    for concept_feature in d:\n",
        "      if concept_feature > 0:\n",
        "        freq_array[curren_class][counter]=freq_array[curren_class][counter]+1\n",
        "      counter=counter+1\n",
        "    i=i+1\n",
        "  arr=np.array(freq_array)\n",
        "  #نسبة عدد مستندات مفهوم ما من فئة ما على عدد مستندات الفئة\n",
        "  for i in range(L_labels):\n",
        "    arr[i][:]= arr[i][:]/doc_in_class[i]\n",
        "  return arr\n",
        "####################################\n",
        "def class_weighting(saved_embaddings,data_DF,freqarray):\n",
        "  lv=len(saved_embaddings)\n",
        "  K_clusters=len(saved_embaddings[0])\n",
        "  for i in range(lv):\n",
        "    curren_class=data_DF['class'][i]\n",
        "    for j in range(K_clusters):\n",
        "      saved_embaddings[i][j] = float(saved_embaddings[i][j])*freqarray[curren_class][j]\n",
        "  return saved_embaddings\n",
        "\n",
        "############################################\n",
        "from collections import Counter \n",
        "def removeElements(lst, k): \n",
        "    counted = Counter(lst) \n",
        "    temp_lst = [] \n",
        "    for el in counted: \n",
        "        if counted[el] < k: \n",
        "            temp_lst.append(el) \n",
        "    res_lst = [] \n",
        "    for el in lst: \n",
        "        if el not in temp_lst: \n",
        "            res_lst.append(el) \n",
        "              \n",
        "    return(res_lst)\n",
        "\n",
        "######################\n",
        "from collections import Counter\n",
        "\n",
        "def rare_words(text):\n",
        "\t\"\"\"\n",
        "\tReturn :- Most Rare words\n",
        "\tInput :- string\n",
        "\tOutput :- list of rare words\n",
        "\t\"\"\"\n",
        "\t# tokenization\n",
        "\ttokens = word_tokenize(text)\n",
        "\tfor word in tokens:\n",
        "\t\tcounter[word]= +1\n",
        "\n",
        "\tRareWords = []\n",
        "\tnumber_rare_words = 10\n",
        "\t# take top 10 frequent words\n",
        "\tfrequentWords = counter.most_common()\n",
        "\tfor (word, word_count) in frequentWords[:-number_rare_words:-1]:\n",
        "\t\tRareWords.append(word)\n",
        "\n",
        "\treturn RareWords\n",
        "\n",
        "###****************\n",
        "def remove_rw(text, RareWords):\n",
        "\t\"\"\"\n",
        "\tReturn :- String after removing frequent words\n",
        "\tInput :- String\n",
        "\tOutput :- String\n",
        "\t\"\"\"\n",
        "\n",
        "\ttokens = word_tokenize(text)\n",
        "\twithout_rw = []\n",
        "\tfor word in tokens:\n",
        "\t\tif word not in RareWords:\n",
        "\t\t\twithout_rw.append(word)\n",
        "\n",
        "\twithout_rw = ' '.join(without_rw)\n",
        "\treturn without_rw\n",
        "### expample\n",
        "###\n",
        "##########################################33\n",
        "#delet last weght\n",
        "def deletFeatures(Feature_index,saved_embaddings):\n",
        "  for d in saved_embaddings:\n",
        "    del d[Feature_index-1]\n",
        "  return saved_embaddings\n",
        "\n",
        "############################################################\n",
        "#تابع لحساب تردد المستند العكسي بناء على بيانات التدريب والاختبار المقسومة\n",
        "def RepresentationGNAG(train_docs,test_docs , sim_val , alpha , centers , dictionary):\n",
        "  train_embeddings = []\n",
        "  test_embeddings = []\n",
        "\n",
        "  counter = 0\n",
        "  IDf = np.zeros(len(centers)).tolist() # بطول العناقيد\n",
        "#idf from train docs  \n",
        "  for i in train_docs: \n",
        "    v = Doc2Vec(i ,sim_val , alpha , centers , dictionary)\n",
        "    ######## حساب عدد مرات ظهور المفهوم في كل نص\n",
        "    for i in v:   # i is concept line , i need class of doc v\n",
        "      if i > 0:\n",
        "        IDf[counter] = IDf[counter] + 1 #  i[M-1]\n",
        "      counter = counter+1\n",
        "    counter = 0\n",
        "    train_embeddings.append(v)\n",
        "#idf from test docs \n",
        "  for i in test_docs: \n",
        "    v = Doc2Vec(i ,sim_val , alpha , centers , dictionary)\n",
        "    ######## حساب عدد مرات ظهور المفهوم في كل نص\n",
        "    for i in v:   # i is concept line , i need class of doc v\n",
        "      if i > 0:\n",
        "        IDf[counter] = IDf[counter] + 1 #  i[M-1]\n",
        "      counter = counter+1\n",
        "    counter = 0\n",
        "    test_embeddings.append(v)\n",
        "\n",
        "  print(IDf)\n",
        "  ######## Calculate Cf-IDF\n",
        "  for i in range(len(train_embeddings)):\n",
        "    l=len(train_embeddings[i])\n",
        "    for j in range(l):\n",
        "    #for doc i: muti cell j with idf j \n",
        "       #print('i', i,' j ', j, ' = ', embeddings[i][j] * newIDF[j])\n",
        "       train_embeddings[i][j] = train_embeddings[i][j] * IDf[j]\n",
        "\n",
        "  for i in range(len(test_embeddings)):\n",
        "    l=len(test_embeddings[i])\n",
        "    for j in range(l):\n",
        "       test_embeddings[i][j] = test_embeddings[i][j] * IDf[j]\n",
        "  return train_embeddings,test_embeddings\n",
        "###############\n",
        "################################################################################\n",
        "#thershold for words in document\n",
        "def delete_empty_docs(documents_embeddings,labels,thershold):\n",
        "  doc_indexes=[]\n",
        "  new_emb=[]\n",
        "  new_label=[]\n",
        "  j=0\n",
        "  #find empty docs\n",
        "  for d in documents_embeddings:\n",
        "    l=len(d)\n",
        "    if l>thershold:\n",
        "      new_label.append(labels[j])\n",
        "      new_emb.append(documents_embeddings[j])\n",
        "    else:\n",
        "      doc_indexes.append(j)\n",
        "    j=j+1\n",
        "  print(doc_indexes)\n",
        "  return new_emb,new_label\n",
        "#################################\n",
        "#used for self embeddings \n",
        "def make_documents_as_list(df):\n",
        "  documents_embeddings = []\n",
        "  for i in range(len(df)):\n",
        "    documents_embeddings.append(final_preprocess_text(df['document'][i]))\n",
        "  return documents_embeddings\n",
        "\n",
        "def documents_as_list_of_sentences(df):\n",
        "  documents_embeddings = []\n",
        "  for i in range(len(df)):\n",
        "    documents_embeddings.append(pre_processing_no_toknize(df['document'][i]))\n",
        "  return documents_embeddings\n",
        "\n",
        "\n",
        "#######################################\n",
        "def word_count(documents_embeddings):\n",
        "  word_count=[]\n",
        "  #find empty docs\n",
        "  for d in documents_embeddings:\n",
        "    l=len(d)\n",
        "    word_count.append(l)\n",
        "    \n",
        "  return word_count\n",
        "##########################################\n",
        "def document_biggerThan(data_DF, doc_vectors,th): \n",
        "  #number of docs in each class\n",
        "  doc_in_class=DataBalance(data_DF)\n",
        "  # number of classes\n",
        "  L_labels=len(doc_in_class)\n",
        "  K_clusters=len(doc_vectors[0])\n",
        "  #array for freq  \n",
        "  freq_array = np.zeros(L_labels).tolist()\n",
        "  mean_array = np.zeros(L_labels).tolist()  \n",
        "  i=0 # doc counter\n",
        "  for d in doc_vectors:\n",
        "    curren_class=data_DF['class'][i]\n",
        "    docL= len(d)\n",
        "    if docL>th:\n",
        "      freq_array[curren_class]=freq_array[curren_class]+1\n",
        "    i=i+1\n",
        "  #متوسط اطوال مستندات كل فئة=عدد مستنداتها الاكبر من حد معين على عدد مستندات الفئة\n",
        "  mean_array = [i / j for i, j in zip(freq_array, doc_in_class)] \n",
        "\n",
        "  return freq_array,mean_array\n",
        "\n",
        "#####################################################\n",
        "def clac_new_Tf_idf(to_delete,tf_similarity,idfcount,idfExp):\n",
        "  i=0\n",
        "  j=0\n",
        "  newIDFexp=[]\n",
        "  newIDFcount=[]\n",
        "  newTf_similarity=[]\n",
        "  num_of_concepts=len(idfexp)\n",
        "\n",
        "  \n",
        "  for d in tf_similarity:\n",
        "    tempD=[]\n",
        "    for j in range(num_of_concepts):\n",
        "      if j not in to_delete:\n",
        "        tempD.append(d[j])\n",
        "    newTf_similarity.append(tempD)\n",
        "    del tempD\n",
        "  ###\n",
        "  for i in range(num_of_concepts):\n",
        "    if i not in todelete:\n",
        "      newIDFexp.append(idfexp[i])\n",
        "      newIDFcount.append(idfcount[i])\n",
        "  return newTf_similarity,newIDFexp,newIDFcount\n",
        "\n",
        "\n",
        "##############################################################\n",
        "def concept_to_delete(min_idf,max_idf,idfcount):\n",
        "  to_delete_List=[]\n",
        "  j=0\n",
        "  for i in idfcount: #30\n",
        "    if (i > max_idf) or (i < min_idf):\n",
        "      to_delete_List.append(j)\n",
        "    j+=1\n",
        "  return to_delete_List\n",
        "\n",
        "######################################################\n",
        "def delete_NaN_docs(documents_embeddings,labels):\n",
        "  doc_indexes=[]\n",
        "  new_emb=[]\n",
        "  new_label=[]\n",
        "  j=0\n",
        "  #find empty docs\n",
        "  for d in documents_embeddings:\n",
        "    if math.isnan(d[0])==False:\n",
        "      new_label.append(labels[j])\n",
        "      new_emb.append(documents_embeddings[j])\n",
        "    else:\n",
        "      doc_indexes.append(j)\n",
        "    j=j+1\n",
        "  print(doc_indexes)\n",
        "  return new_emb,new_label\n",
        "\n",
        "###################################3\n",
        "def docs_word_count(data_DF):\n",
        "  word_count=[]\n",
        "  #find empty docs\n",
        "  for d in data_DF['document']:\n",
        "    l=len(d)\n",
        "    word_count.append(l)\n",
        "    \n",
        "  return word_count\n",
        "\n",
        "#############################################\n",
        "def data_details(documents_embeddings,data_DF,th):\n",
        "  fr,men=document_biggerThan(data_DF,documents_embeddings,th)\n",
        "  no_docs = len(data_DF)\n",
        "  doc_per_class = DataBalance(data_DF)\n",
        "  wc=word_count(documents_embeddings) #embedded word counts\n",
        "  o_wc= docs_word_count(data_DF) # orginal word count\n",
        "\n",
        "  print('№ of all doc=',len(data_DF),\"\\n\")\n",
        "  print(\"Befor embeddings: \\n\",\n",
        "        'the mean length of docs', sum(o_wc)/len(o_wc), \"\\n\",\n",
        "        \"The max length of docs\", max(o_wc),\"\\n\",\n",
        "        \"The min length of docs\", min(o_wc),\"\\n\", \n",
        "        '№ classes ', ' =', len(doc_per_class),\"\\n\",\n",
        "        \"№ docs per class \", doc_per_class )\n",
        "  print(\"After embeddings: \\n\",\n",
        "        \"the mean length of embeded docs\", sum(wc)/len(wc), \"\\n\",\n",
        "        \"The max length of embeded docs\", max(wc),\"\\n\",\n",
        "        \"The min length of embeded docs\", min(wc),\"\\n\", \n",
        "        \"№ Documents with length > \",th, \"=\" ,sum(fr))\n",
        "  \n",
        "\n",
        "##################################################################333\n",
        "#خاص بحقيبة الكلمات الاصلية\n",
        "def BOC(embeddeings_document,threshold_sim,alpha,centers,dictionary):\n",
        "  M=2\n",
        "  concept_freq = 0\n",
        "  ## intilize doc array\n",
        "  doc2vec = np.zeros(len(centers)).tolist()\n",
        "\n",
        "  for cnt in range(len(centers)):\n",
        "    counter = 0;\n",
        "    for emb_word in embeddeings_document:\n",
        "      Wsim= cosine_distance(emb_word , centers[cnt])\n",
        "      if Wsim  > alpha:\n",
        "        concept_freq = concept_freq+1 #for tf\n",
        "        doc2vec[cnt] = doc2vec[cnt] + 1;\n",
        "      else:\n",
        "        continue;\n",
        "      counter = counter +1;\n",
        "    # end for doc words with one concepts\n",
        "  if concept_freq !=0: # the number of occurence for all concept in this document\n",
        "    for i in doc2vec:\n",
        "      i = i / concept_freq  \n",
        "  return doc2vec\n",
        "#####################################\n",
        "def CF_IDF_BOC(documents_embeddings , sim_val , alpha , centers , dictionary):\n",
        "  embeddings = []\n",
        "  counter = 0\n",
        "  IDf = np.zeros(len(centers)).tolist() # بطول العناقيد\n",
        "  for i in documents_embeddings: \n",
        "    bag_of_concepts = BOC(i ,sim_val , alpha , centers , dictionary)\n",
        "    ######## حساب عدد مرات ظهور المفهوم في كل نص\n",
        "    for i in bag_of_concepts:   # i is concept line , i need class of doc v\n",
        "      if i > 0:\n",
        "        IDf[counter] = IDf[counter] + 1 #  i[M-1]\n",
        "      counter = counter+1\n",
        "    counter = 0\n",
        "    embeddings.append(bag_of_concepts)\n",
        "  #########  calculate IDF\n",
        "  newIDF=[]\n",
        "  for i in IDf:\n",
        "    if i!=0:\n",
        "      N=len(embeddings)\n",
        "      newIDF.append( math.log10( (N+1) /i ) )\n",
        "    else:\n",
        "      newIDF.append(0)  \n",
        "  ######## Calculate Cf-IDF\n",
        "  print(len(embeddings))\n",
        "  for i in range(len(embeddings)):\n",
        "    l=len(embeddings[i])\n",
        "    for j in range(l):\n",
        "    #for doc i: muti cell j with idf j \n",
        "       #print('i', i,' j ', j, ' = ', embeddings[i][j] * newIDF[j])\n",
        "       embeddings[i][j] = embeddings[i][j] * newIDF[j]\n",
        "  \n",
        "  return embeddings\n",
        "\n",
        "def text_file_to_list(train_path,test_path):\n",
        "  labels=[]\n",
        "  docs=[]\n",
        "  ###fill train\n",
        "  with open(train_path) as f:\n",
        "    reader = csv.reader(f, delimiter=\"\\t\")\n",
        "    d = list(reader)\n",
        "  for line in d:\n",
        "    docs.append(line[1])\n",
        "    labels.append(line[0])\n",
        "  ####test\n",
        "  with open(test_path) as f:\n",
        "    reader = csv.reader(f, delimiter=\"\\t\")\n",
        "    d = list(reader)\n",
        "  for line in d:\n",
        "    docs.append(line[1])\n",
        "    labels.append(line[0])\n",
        "  return docs,labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Collecting spherecluster\n",
            "  Downloading spherecluster-0.1.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from spherecluster) (1.4.1)\n",
            "Collecting nose\n",
            "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[K     |████████████████████████████████| 154 kB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from spherecluster) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from spherecluster) (0.22.2.post1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from spherecluster) (3.6.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->spherecluster) (1.0.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->spherecluster) (57.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spherecluster) (21.2.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spherecluster) (8.8.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spherecluster) (1.10.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spherecluster) (1.15.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spherecluster) (1.4.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->spherecluster) (0.7.1)\n",
            "Installing collected packages: nose, spherecluster\n",
            "Successfully installed nose-1.3.7 spherecluster-0.1.7\n",
            "Collecting soyclustering\n",
            "  Downloading soyclustering-0.2.0-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: numpy>=1.1 in /usr/local/lib/python3.7/dist-packages (from soyclustering) (1.19.5)\n",
            "Installing collected packages: soyclustering\n",
            "Successfully installed soyclustering-0.2.0\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj8ww4vWj5BX"
      },
      "source": [
        "# Загрузка Датасетов\n",
        "ВВС & GN & Reuters & OHSUMED & WebKB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "PkuKDatzK8lQ",
        "outputId": "c618d055-e201-4d21-9a2d-876a94a7bbe7"
      },
      "source": [
        "# text doc dataset\n",
        "#استخراج الاسطر من الملف وفصل الفة عن نص المستند\n",
        "import csv\n",
        "\n",
        "GN_train_no_stop=\"/content/drive/MyDrive/PHD/Data/GNnostop/20ng-train-no-stop.txt\"\n",
        "GN_test_no_stop=\"/content/drive/MyDrive/PHD/Data/GNnostop/20ng-test-no-stop.txt\"\n",
        "\n",
        "\n",
        "docs, labels = text_file_to_list(GN_train_no_stop, GN_test_no_stop)\n",
        "data_DF = pd.DataFrame({'class': labels, \n",
        "                        'document': docs})\n",
        "data_DF['class'] = pd.factorize(data_DF['class'])[0]\n",
        "\n",
        "data_DF.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>document</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>alt atheism faq atheist resources archive name...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>alt atheism faq introduction atheism archive n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>gospel dating article mimsy umd edu mangoe umd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>university violating separation church state d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>soc motss princeton axes matching funds for bo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   class                                           document\n",
              "0      0  alt atheism faq atheist resources archive name...\n",
              "1      0  alt atheism faq introduction atheism archive n...\n",
              "2      0  gospel dating article mimsy umd edu mangoe umd...\n",
              "3      0  university violating separation church state d...\n",
              "4      0  soc motss princeton axes matching funds for bo..."
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "-JgNxcnaPsBj",
        "outputId": "cefe8d78-492e-4e54-e626-f3274aadd73e"
      },
      "source": [
        "########### WebKB\n",
        "with open('/content/drive/MyDrive/PHD/Data/WebKB/web_docs.txt', \"r\") as f:\n",
        "    docs = f.readlines()\n",
        "with open('/content/drive/MyDrive/PHD/Data/WebKB/web_labels.txt') as f:\n",
        "    labels=[line for line in f]\n",
        "data_DF = pd.DataFrame({'class': labels, \n",
        "                        'document': docs})\n",
        "\n",
        "data_DF['class'] = pd.factorize(data_DF['class'])[0]\n",
        "\n",
        "data_DF.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>document</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>brian comput scienc depart univers wisconsin d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>denni swanson web page mail pop uki offic hour...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>russel impagliazzo depart comput scienc engin ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>dave phd student depart comput scienc univers ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>center lifelong learn design univers colorado ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   class                                           document\n",
              "0      0  brian comput scienc depart univers wisconsin d...\n",
              "1      0  denni swanson web page mail pop uki offic hour...\n",
              "2      1  russel impagliazzo depart comput scienc engin ...\n",
              "3      0  dave phd student depart comput scienc univers ...\n",
              "4      2  center lifelong learn design univers colorado ..."
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "rnb1TFbGkhct",
        "outputId": "febda465-7860-4564-8378-b2ef76ceafc8"
      },
      "source": [
        "Reuters_file_path=\"/content/drive/MyDrive/PHD/Data/Reuters88.csv\"\n",
        "BBC_file_path=\"/content/drive/MyDrive/PHD/Data/bbc-text.csv\"\n",
        "ohsumed_file_path=\"/content/drive/MyDrive/PHD/Data/ohsumed-allcats.csv\"\n",
        "\n",
        "data_DF = pd.read_csv(ohsumed_file_path)\n",
        "\n",
        "data_DF.columns.values[0] = \"class\"\n",
        "data_DF.columns.values[1] = \"document\"\n",
        "\n",
        "data_DF['class'] = pd.factorize(data_DF['class'])[0]\n",
        "data_DF.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>document</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Laparoscopic treatment of perforated peptic ul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Experimental sympathetic activation causes end...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Myocyte cell loss and myocyte cellular hyperpl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>Absorption and motility of the bypassed human ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>Evolution of fundic argyrophil cell hyperplasi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   class                                           document\n",
              "0      0  Laparoscopic treatment of perforated peptic ul...\n",
              "1      0  Experimental sympathetic activation causes end...\n",
              "2      1  Myocyte cell loss and myocyte cellular hyperpl...\n",
              "3      0  Absorption and motility of the bypassed human ...\n",
              "4      1  Evolution of fundic argyrophil cell hyperplasi..."
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlYA9AxDkRzQ"
      },
      "source": [
        "###**Предварительная обработка**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xug4rIFvy7In"
      },
      "source": [
        "K_clusters= 200\n",
        "sim_digree=0.3\n",
        "number_of_words_per_cluster=5\n",
        "min_doc= 0 # الحد الادني لعدد الكلمات في المستند\n",
        "max_idf_r=1\n",
        "min_idf=0\n",
        "\n",
        "\n",
        "token_path=\"/content/drive/MyDrive/PHD/pruned_data/OH_r20_data.txt.npy\"\n",
        "#model_path = \"/content/drive/MyDrive/PHD/Models/fasttext-wiki-news-subwords-300\"\n",
        "\n",
        "#model_path='/content/drive/MyDrive/PHD/Models/word2vec-google-news-300.model'\n",
        "#pre_trained_model = intall_pre_trained_model('/content/drive/MyDrive/PHD Work/w2v/word2vec-google-news-300.model' ,  'word2vec-google-news-300')\n",
        "#'wiki-news-300d-1M-subword.vec', limit=999999\n",
        "# naming resulted embedding file\n",
        "Th=5\n",
        "alpha=0.2\n",
        "modd='ًw2vself300' \n",
        "dataset='OHSim03'\n",
        "typeOfcontent=\"Self_w2v15_\"\n",
        "path='/content/drive/MyDrive/PHD/Data/Embedd/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbAU5Wr91sBO",
        "outputId": "4201df66-7d69-4808-95ec-70987b6c5436"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0G3JORmUFCnm",
        "outputId": "e52d3a0b-9444-4ce0-ec47-42af03503b1b"
      },
      "source": [
        "pip install glove-python-binary"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting glove-python-binary\n",
            "  Downloading glove_python_binary-0.2.0-cp37-cp37m-manylinux1_x86_64.whl (948 kB)\n",
            "\u001b[K     |████████████████████████████████| 948 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from glove-python-binary) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from glove-python-binary) (1.4.1)\n",
            "Installing collected packages: glove-python-binary\n",
            "Successfully installed glove-python-binary-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZTA-08eOZTj"
      },
      "source": [
        "from glove import Corpus, Glove\n",
        "#Creating a corpus object\n",
        "corpus = Corpus() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AReuz5suOkCs"
      },
      "source": [
        "#list of sentences to be vectorized \n",
        "№lines = make_documents_as_list(data_DF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NDYLmwRoNzvN"
      },
      "source": [
        "#Training the corpus to generate the co occurence matrix which is used in GloVe\n",
        "'''\n",
        "#list of sentences to be vectorized \n",
        "lines = make_documents_as_list(data_DF)\n",
        "corpus.fit(lines, window=10)\n",
        "\n",
        "glove = Glove(no_components=300, learning_rate=0.005)\n",
        "glove.fit(corpus.matrix, epochs=100, no_threads=4, verbose=False)\n",
        "glove.add_dictionary(corpus.dictionary)\n",
        "glove.save('/content/drive/MyDrive/PHD/Data/Self_Glove/BBCCCC_glove.model')\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVjCtBxEFF3e"
      },
      "source": [
        "#model = Glove.load('/content/drive/MyDrive/PHD/Data/Self_Glove/BBCCCC_glove.model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HC3-15V_F_IM"
      },
      "source": [
        "####################################################33333\n",
        "def embedding_all_documents_by_Glove(df , model,N):\n",
        "  documents_embeddings = []\n",
        "  for i in range(len(df)):\n",
        "    documents_embeddings.append(convert_doc_to_Glove_embedding(df['document'][i] , model,N))\n",
        "  return documents_embeddings\n",
        "\n",
        "########################################################################################################\n",
        "def convert_doc_to_Glove_embedding(text , model,N):\n",
        "  tokens = pre_processing(text)\n",
        "  if N!=0:\n",
        "    tokens=removeElements(tokens,N)\n",
        "  embeddings_matrix  , irrugular_words = Create_Glove_embedding_matrix(model , tokens , 300)\n",
        "  return embeddings_matrix\n",
        "  ##################################33333\n",
        "def Create_Glove_embedding_matrix(model , tokens , embedding_dim):\n",
        "    embedding_matrix = []\n",
        "    irregular_words  = []\n",
        "    for words in tokens:\n",
        "          try :\n",
        "            temp = model.word_vectors[model.dictionary[words]] \n",
        "            embedding_matrix.append(temp)\n",
        "          except KeyError:\n",
        "            # for the words that are not in model vocabulary\n",
        "            # we initilize with random vector\n",
        "            irregular_words.append(words)\n",
        "            #embedding_matrix.append(np.random.normal(0,np.sqrt(0.300),embedding_dim))\n",
        "    return embedding_matrix , irregular_words\n",
        "####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMUdMyA0U_wp",
        "outputId": "c8ee774f-457c-4a15-b1e9-3072f3253d7d"
      },
      "source": [
        "#all_tokens = new_pre_processing_dataset(data_DF)\n",
        "\n",
        "#all_tokens = pre_processing_dataset(data_DF)\n",
        "#all_tokens=removeElements(all_tokens,20)\n",
        "#fileName=\"/content/drive/MyDrive/PHD Work/PreProData/R52_All_data.txt\"\n",
        "#saveList(all_tokens,fileName)\n",
        "\n",
        "all_tokens=loadList(token_path)\n",
        "\n",
        "documents_as_list = make_documents_as_list(data_DF)\n",
        "model = Word2Vec(sentences=documents_as_list, size=300, window=15, min_count=5, workers=4)\n",
        "\n",
        "#model = Load_pre_trained_model_embeddings(model_path)\n",
        "\n",
        "#with open('/content/drive/MyDrive/PHD Work/DataVLAC/web_embeddings.pickle', 'rb') as handle:\n",
        "#  model = pickle.load(handle)\n",
        "len(all_tokens)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "387534"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9y9uRZfH5fY",
        "outputId": "bd92cdd8-f957-49c6-9b60-bf872b68ceec"
      },
      "source": [
        "unique_words = remove_duplication(all_tokens)\n",
        "print(\"unique words: \" ,len(unique_words),\"\\n\")\n",
        "\n",
        "\n",
        "dic = Save_words_with_Embeddings (unique_words , model)\n",
        "print(\"length dic: \",len(dic),\"\\n\")\n",
        "#model_word_list = list(model.wv.vocab)\n",
        "dim = 300\n",
        "embeddings_matrix  , irrugular_words = Create_embedding_matrix(model , unique_words , dim)\n",
        "\n",
        "\n",
        "######### Clustering\n",
        "labels , centers = spherical_kmeans(K_clusters , embeddings_matrix, sim_digree)\n",
        "print(\"The clusters details\", Counter(labels), \"\\n\")\n",
        "\n",
        "# dictionary cluster index: values are its words embedding\n",
        "concept_dictionary = get_cluster_elements(labels, K_clusters , embeddings_matrix)\n",
        "\n",
        "start_time = time.time()\n",
        "sorted_cons = get_soreted_dictionary(centers , concept_dictionary , number_of_words_per_cluster)\n",
        "print(\"Clustering excution time--- %s seconds ---\" % (time.time() - start_time), \"\\n\")\n",
        "\n",
        "#Save Text Clusters\n",
        "#textClusters = dictAsText(30,10,sorted_cons,dic)\n",
        "#fileName=\"/content/drive/MyDrive/PHD Work/TextClusters/Test_OH50_10Sim03.txt\"\n",
        "#saveList(textClusters,fileName)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "unique words:  3960 \n",
            "\n",
            "length dic:  0 \n",
            "\n",
            "The clusters details Counter({143: 47, 167: 46, 185: 41, 15: 39, 114: 37, 182: 33, 103: 33, 95: 32, 8: 31, 67: 30, 110: 30, 189: 29, 89: 29, 120: 28, 98: 28, 79: 28, 88: 28, 151: 27, 48: 27, 33: 27, 94: 26, 166: 26, 58: 26, 195: 25, 156: 25, 117: 25, 22: 25, 122: 25, 51: 25, 3: 25, 163: 24, 82: 24, 159: 24, 196: 24, 157: 24, 6: 24, 54: 23, 193: 23, 36: 23, 52: 23, 102: 23, 62: 23, 136: 23, 77: 23, 173: 23, 177: 23, 108: 23, 165: 23, 17: 22, 142: 22, 4: 22, 11: 22, 71: 22, 116: 22, 190: 22, 92: 22, 13: 22, 106: 21, 174: 21, 147: 21, 123: 21, 0: 20, 107: 20, 76: 20, 91: 19, 65: 19, 50: 19, 45: 19, 16: 19, 134: 19, 60: 19, 104: 19, 56: 18, 153: 18, 66: 18, 42: 18, 74: 18, 194: 18, 40: 17, 161: 17, 144: 17, 85: 17, 27: 17, 68: 17, 170: 17, 160: 17, 43: 16, 126: 16, 70: 16, 141: 16, 75: 16, 115: 16, 112: 16, 197: 16, 14: 15, 130: 15, 47: 15, 158: 14, 131: 14, 178: 14, 10: 14, 72: 13, 188: 13, 31: 13, 32: 13, 63: 13, 138: 13, 25: 13, 28: 13, 140: 13, 155: 13, 124: 13, 169: 13, 148: 12, 93: 12, 101: 12, 19: 12, 61: 12, 53: 11, 18: 11, 183: 11, 30: 11, 139: 11, 20: 11, 90: 11, 181: 11, 164: 11, 133: 10, 198: 10, 152: 10, 87: 10, 7: 10, 146: 10, 1: 10, 135: 10, 179: 10, 150: 10, 49: 9, 105: 9, 118: 9, 186: 9, 84: 9, 187: 9, 171: 9, 127: 9, 55: 9, 100: 9, 149: 9, 145: 9, 199: 8, 35: 8, 86: 7, 113: 7, 26: 7, 59: 7, 5: 7, 132: 7, 125: 7, 83: 7, 24: 6, 180: 6, 57: 6, 80: 6, 119: 6, 176: 6, 78: 6, 191: 5, 162: 5, 184: 5, 37: 5, 121: 5, 9: 5, 154: 5, 12: 5, 69: 4, 39: 4, 29: 4, 38: 4, 97: 3, 129: 3, 99: 3, 73: 3, 44: 3, 109: 3, 96: 3, 34: 3, 81: 3, 2: 3, 172: 3, 168: 3, 64: 2, 192: 2, 21: 2, 23: 2, 111: 1, 128: 1, 41: 1, 46: 1, 175: 1, 137: 1}) \n",
            "\n",
            "Clustering excution time--- 0.7126245498657227 seconds --- \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksFb5NBa1AdU",
        "outputId": "b9f983cb-ad10-4b12-bb67-4c635f5f5801"
      },
      "source": [
        "N=0\n",
        "documents_embeddings = embedding_all_documents(data_DF , model,N)\n",
        "labels =data_DF['class'].tolist()\n",
        "\n",
        "print(\"the  len of labels and docs \", len(labels),len(documents_embeddings),\"\\n\")\n",
        "##data detils\n",
        "#data_details(documents_embeddings, data_DF, 50)\n",
        "\n",
        "print(\"deleting docs that has not embedded words or WE count less\", min_doc , \"\\n\")\n",
        "\n",
        "#deleting empty\n",
        "documents_embeddings ,labels = delete_empty_docs(documents_embeddings,labels,min_doc)\n",
        "print(\"the  New len of labels and docs \", len(labels),len(documents_embeddings),\"\\n\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the  len of labels and docs  5380 5380 \n",
            "\n",
            "deleting docs that has not embedded words or WE count less 0 \n",
            "\n",
            "[3522]\n",
            "the  New len of labels and docs  5379 5379 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4JU6AUtWOD-i",
        "outputId": "69b92fa9-d26a-444b-815e-f5bd909485ba"
      },
      "source": [
        "data_details(documents_embeddings, data_DF, 50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "№ of all doc= 2225 \n",
            "\n",
            "Befor embeddings: \n",
            " the mean length of docs 2262.936179775281 \n",
            " The max length of docs 25483 \n",
            " The min length of docs 501 \n",
            " № classes   = 5 \n",
            " № docs per class  [401.0, 510.0, 511.0, 386.0, 417.0]\n",
            "After embeddings: \n",
            " the mean length of embeded docs 198.6076404494382 \n",
            " The max length of embeded docs 2040 \n",
            " The min length of embeded docs 44 \n",
            " № Documents with length >  50 = 2224.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NA1crRuZT5E"
      },
      "source": [
        "# Bag of weighted Concepts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOb8CGAlxIrU"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "\n",
        "def data_preperation(dataset , labels, sample):\n",
        "  x_train , x_test , y_train , y_test = train_test_split(dataset , labels , test_size = sample , random_state = 0)\n",
        "  return array(x_train) , array(x_test) , array(y_train) , array(y_test)\n",
        "\n",
        "def train_ML(x_train , y_train):\n",
        "  svm =  SVC(kernel=kernell, random_state=44).fit(x_train , y_train) \n",
        "  #svm =  LinearSVC(penalty=\"l2\", dual=False,tol=1e-3).fit(x_train , y_train) \n",
        "  #MLPClassifier(random_state=1, max_iter=10000).fit(x_train , y_train)  \n",
        "  return svm\n",
        "\n",
        "#linear', 'poly', 'rbf', 'sigmoid',\n",
        "\n",
        "\n",
        "def train_RF(x_train , y_train):\n",
        "  rfc = RandomForestClassifier( n_estimators=500, max_depth=150,n_jobs=1).fit(x_train , y_train)\n",
        "  return rfc\n",
        "\n",
        "\n",
        "def train_DM(x_train, y_train):\n",
        "  batch_size = 32\n",
        "  epochs = 40\n",
        "  drop_ratio = 0.4\n",
        "   \n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Dense(1024))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.Dense(512))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.Dense(125))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.Dense(32))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.Dense(num_classes))\n",
        "  model.add(layers.Activation('softmax'))\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy'])\n",
        "  history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_split=0.1)\n",
        "  return model\n",
        "\n",
        "def evaluation_ML(x_test , y_test , svm_model):\n",
        "  ev=svm_model.score(x_test , y_test)\n",
        "  return ev\n",
        "def evaluation_DL(x_test , y_test , model):\n",
        "  ev=model.evaluate(x_test , y_test)\n",
        "  return ev\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "869bhrJcim0p"
      },
      "source": [
        "### ** Alpha Test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ci2bgMM3NiDu",
        "outputId": "44292bc9-877d-44f4-d673-b1e85b981584"
      },
      "source": [
        "lalpha = [0.2,0.3,0.4,0.5,0.6,0.7,0.8]\n",
        "min_idf = 0\n",
        "\n",
        "for alpha in lalpha:\n",
        "\n",
        "  max_idf_r=1\n",
        "  kernell='poly' \n",
        "  max_idf=max_idf_r* len(documents_embeddings)\n",
        "\n",
        "  print(\"======================================================start represnation:\\n\")\n",
        "  start_time = time.time()\n",
        "  tf_similarity = RepresentationNew(documents_embeddings, Th , alpha , centers , sorted_cons)\n",
        "\n",
        "  print(\"Counting IDF to select concepts with IDF less than \",max_idf, \":\\n\")\n",
        "\n",
        "  idfcount,idfexp=calculate_IDF(tf_similarity)\n",
        "  todelete=concept_to_delete(min_idf, max_idf,idfcount)\n",
        "  print(len(todelete),\" clusters will be deleted:  \", todelete ,\" \\n\")\n",
        "\n",
        "  new_tf_similarity,idfexp,idfcount= clac_new_Tf_idf(todelete,tf_similarity,idfcount,idfexp)\n",
        "  K_clusters=len(idfexp)\n",
        "  print(\"The new number of clusters: \",K_clusters, \":\\n\")\n",
        "\n",
        "  print(\"Calculating BoEC...\\n\")\n",
        "\n",
        "  final_embeddings=CF_IDF(new_tf_similarity,idfexp)\n",
        "\n",
        "  represnationtime=time.time() - start_time\n",
        "\n",
        "\n",
        "  # save the resulted embeddings into file\n",
        "  fName=typeOfcontent+\"_new_maxIDF\"+str(max_idf_r)+\"minidf\"+str(min_idf)+ '_'+str(min_doc)+\"_\" +dataset+'_'+modd+ '_'+str(K_clusters)+'cls_'+'_0'+str(alpha)[2:]+'.txt'\n",
        "  fileName=path+fName\n",
        "  fileName\n",
        "\n",
        "  saveList(final_embeddings,fileName)\n",
        "  print(\"time for representation: \", represnationtime,\" seconds, file saved to: \\n\",fileName)\n",
        "\n",
        "  print(\"build new data frame: \\n\")\n",
        "\n",
        "  embedding_DF = pd.DataFrame({'class': labels, 'document': final_embeddings})\n",
        "\n",
        "\n",
        "  print(\"Start training ............................................ \",alpha, \"\\n\")\n",
        "\n",
        "\n",
        "  final_embeddings = [array(i).reshape(-1) for i in final_embeddings]\n",
        "  labels =embedding_DF['class'].tolist() # ConvertTextLabelToNeumerical(data_DF)\n",
        "\n",
        "  kernell='poly'\n",
        "\n",
        "  x_train , x_test , y_train , y_test = data_preperation(final_embeddings , labels, 0.30)\n",
        "  print(\"resluts without calss info.............\")\n",
        "  svm = train_ML(x_train , y_train)\n",
        "  y_pred=svm.predict(x_test)\n",
        "  f1_without = f1_score(y_test,y_pred, average=\"weighted\")\n",
        "\n",
        "  print(\"f1 without C info\", f1_without)\n",
        "\n",
        "\n",
        " \n",
        "  fileNameD=\"Pre_FAST_\" + dataset + \".csv\"\n",
        "  print(\"saving Results to Excel:\\n\" , fileNameD)\n",
        "  \n",
        "  import csv\n",
        "  with open(r'/content/drive/MyDrive/PHD/CSV_Results/'+fileNameD, 'a', newline='') as csvfile:\n",
        "      fieldnames = ['data','Model','kernell','clusters','file','alpha','accuracy','f1_without','time','F_with']\n",
        "      writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "      writer.writerow({'file':fileName ,'data':dataset, \n",
        "                      'Model':modd,'kernell':kernell,\n",
        "                      'clusters':K_clusters,'alpha':alpha,\n",
        "                      'accuracy':0,'f1_without':f1_without,'time':represnationtime,\n",
        "                      'F_with':0})\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================start represnation:\n",
            "\n",
            "Counting IDF to select concepts with IDF less than  5379 :\n",
            "\n",
            "0  clusters will be deleted:   []  \n",
            "\n",
            "The new number of clusters:  200 :\n",
            "\n",
            "Calculating BoEC...\n",
            "\n",
            "Saved successfully!\n",
            "time for representation:  977.4153361320496  seconds, file saved to: \n",
            " /content/drive/MyDrive/PHD/Data/Embedd/Self_w2v15__new_maxIDF1minidf0_0_OHSim03_ًw2vself300_200cls__02.txt\n",
            "build new data frame: \n",
            "\n",
            "Start training ............................................  0.2 \n",
            "\n",
            "resluts without calss info.............\n",
            "f1 without C info 0.6744644199097463\n",
            "saving Results to Excel:\n",
            " Pre_FAST_OHSim03.csv\n",
            "======================================================start represnation:\n",
            "\n",
            "Counting IDF to select concepts with IDF less than  5379 :\n",
            "\n",
            "0  clusters will be deleted:   []  \n",
            "\n",
            "The new number of clusters:  200 :\n",
            "\n",
            "Calculating BoEC...\n",
            "\n",
            "Saved successfully!\n",
            "time for representation:  968.5291204452515  seconds, file saved to: \n",
            " /content/drive/MyDrive/PHD/Data/Embedd/Self_w2v15__new_maxIDF1minidf0_0_OHSim03_ًw2vself300_200cls__03.txt\n",
            "build new data frame: \n",
            "\n",
            "Start training ............................................  0.3 \n",
            "\n",
            "resluts without calss info.............\n",
            "f1 without C info 0.6587708729006937\n",
            "saving Results to Excel:\n",
            " Pre_FAST_OHSim03.csv\n",
            "======================================================start represnation:\n",
            "\n",
            "Counting IDF to select concepts with IDF less than  5379 :\n",
            "\n",
            "0  clusters will be deleted:   []  \n",
            "\n",
            "The new number of clusters:  200 :\n",
            "\n",
            "Calculating BoEC...\n",
            "\n",
            "Saved successfully!\n",
            "time for representation:  948.8305788040161  seconds, file saved to: \n",
            " /content/drive/MyDrive/PHD/Data/Embedd/Self_w2v15__new_maxIDF1minidf0_0_OHSim03_ًw2vself300_200cls__04.txt\n",
            "build new data frame: \n",
            "\n",
            "Start training ............................................  0.4 \n",
            "\n",
            "resluts without calss info.............\n",
            "f1 without C info 0.6635633716035392\n",
            "saving Results to Excel:\n",
            " Pre_FAST_OHSim03.csv\n",
            "======================================================start represnation:\n",
            "\n",
            "Counting IDF to select concepts with IDF less than  5379 :\n",
            "\n",
            "0  clusters will be deleted:   []  \n",
            "\n",
            "The new number of clusters:  200 :\n",
            "\n",
            "Calculating BoEC...\n",
            "\n",
            "Saved successfully!\n",
            "time for representation:  934.8683123588562  seconds, file saved to: \n",
            " /content/drive/MyDrive/PHD/Data/Embedd/Self_w2v15__new_maxIDF1minidf0_0_OHSim03_ًw2vself300_200cls__05.txt\n",
            "build new data frame: \n",
            "\n",
            "Start training ............................................  0.5 \n",
            "\n",
            "resluts without calss info.............\n",
            "f1 without C info 0.6559267538170229\n",
            "saving Results to Excel:\n",
            " Pre_FAST_OHSim03.csv\n",
            "======================================================start represnation:\n",
            "\n",
            "Counting IDF to select concepts with IDF less than  5379 :\n",
            "\n",
            "0  clusters will be deleted:   []  \n",
            "\n",
            "The new number of clusters:  200 :\n",
            "\n",
            "Calculating BoEC...\n",
            "\n",
            "Saved successfully!\n",
            "time for representation:  928.1964910030365  seconds, file saved to: \n",
            " /content/drive/MyDrive/PHD/Data/Embedd/Self_w2v15__new_maxIDF1minidf0_0_OHSim03_ًw2vself300_200cls__06.txt\n",
            "build new data frame: \n",
            "\n",
            "Start training ............................................  0.6 \n",
            "\n",
            "resluts without calss info.............\n",
            "f1 without C info 0.6454964854428836\n",
            "saving Results to Excel:\n",
            " Pre_FAST_OHSim03.csv\n",
            "======================================================start represnation:\n",
            "\n",
            "Counting IDF to select concepts with IDF less than  5379 :\n",
            "\n",
            "0  clusters will be deleted:   []  \n",
            "\n",
            "The new number of clusters:  200 :\n",
            "\n",
            "Calculating BoEC...\n",
            "\n",
            "Saved successfully!\n",
            "time for representation:  863.0447981357574  seconds, file saved to: \n",
            " /content/drive/MyDrive/PHD/Data/Embedd/Self_w2v15__new_maxIDF1minidf0_0_OHSim03_ًw2vself300_200cls__07.txt\n",
            "build new data frame: \n",
            "\n",
            "Start training ............................................  0.7 \n",
            "\n",
            "resluts without calss info.............\n",
            "f1 without C info 0.6341807713028051\n",
            "saving Results to Excel:\n",
            " Pre_FAST_OHSim03.csv\n",
            "======================================================start represnation:\n",
            "\n",
            "Counting IDF to select concepts with IDF less than  5379 :\n",
            "\n",
            "0  clusters will be deleted:   []  \n",
            "\n",
            "The new number of clusters:  200 :\n",
            "\n",
            "Calculating BoEC...\n",
            "\n",
            "Saved successfully!\n",
            "time for representation:  785.7623085975647  seconds, file saved to: \n",
            " /content/drive/MyDrive/PHD/Data/Embedd/Self_w2v15__new_maxIDF1minidf0_0_OHSim03_ًw2vself300_200cls__08.txt\n",
            "build new data frame: \n",
            "\n",
            "Start training ............................................  0.8 \n",
            "\n",
            "resluts without calss info.............\n",
            "f1 without C info 0.5740899210973655\n",
            "saving Results to Excel:\n",
            " Pre_FAST_OHSim03.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L76auf9WiCS"
      },
      "source": [
        "## اختبار مع ملف محفوظ مسبقا"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhpI8TdxWhsZ"
      },
      "source": [
        "#alpha = fileName.split(\"txt\")[0][-3:-1]\n",
        "fileName1= \"/content/drive/MyDrive/PHD Work/Emed/NewAlphaTest/TestThresholdmaxIDF1minidf1_0_BBCSim03_ًGloVe300_100cls__**.txt.npy\"\n",
        "Lalpha = ['02','03','04','05','06','07','08']\n",
        "\n",
        "for alpha in Lalpha:\n",
        "  fileName = fileName1.replace('**', alpha)\n",
        "  print(fileName)\n",
        "  max_idf_r=1\n",
        "  kernell='poly' \n",
        "  max_idf=max_idf_r* len(documents_embeddings)\n",
        "\n",
        "  print(\"======================================================Using saved embeddings represnation:\\n\")\n",
        "  start_time = time.time()\n",
        "\n",
        "  min_doc_length=0\n",
        "  ### get saved embeddings\n",
        "  final_embeddings=loadList(fileName)\n",
        "  #labels =data_DF['class'].tolist()\n",
        "  final_embeddings ,labels = delete_empty_docs(final_embeddings, labels,min_doc_length )\n",
        "\n",
        "  K_clusters = len(final_embeddings[0])\n",
        "\n",
        "\n",
        "  embedding_DF = pd.DataFrame({'class': labels, 'document': final_embeddings})\n",
        "\n",
        "\n",
        "  print(\"Start training ............................................ \",alpha, \"\\n\")\n",
        "\n",
        "\n",
        "  final_embeddings = [array(i).reshape(-1) for i in final_embeddings]\n",
        "  labels =embedding_DF['class'].tolist() # ConvertTextLabelToNeumerical(data_DF)\n",
        "\n",
        "  kernell='poly'\n",
        "\n",
        "  x_train , x_test , y_train , y_test = data_preperation(final_embeddings , labels, 0.30)\n",
        "  print(\"resluts without calss info.............\")\n",
        "  #svm = train_ML(x_train , y_train)\n",
        "  RF = train_RF(x_train , y_train)\n",
        "  #y_pred=svm.predict(x_test)\n",
        "  y_pred=RF.predict(x_test)\n",
        "  f1_without = f1_score(y_test,y_pred, average=\"weighted\")\n",
        "\n",
        "  print(\"f1 without C info\", f1_without)\n",
        "\n",
        "\n",
        "  print(\"resluts with calss info.............\")\n",
        "  embedding_train_DF = pd.DataFrame({'class': list(y_train), 'document': list(x_train)})\n",
        "  embedding_test_DF = pd.DataFrame({'class': list(y_test), 'document': list(x_test)})\n",
        "\n",
        "  # class weghts from training data\n",
        "  freqarray=document_class_of_concept(data_DF, embedding_train_DF,  list(x_train))\n",
        "\n",
        "  xtrain_weighted= class_weighting(list(x_train),embedding_train_DF,freqarray)\n",
        "  print(\"weghting test data by frqarray built based on training data.................\")\n",
        "  xtest_weighted = class_weighting(list(x_test),embedding_test_DF,freqarray)\n",
        "\n",
        "  #svm = train_ML(xtrain_weighted , y_train)\n",
        "  RF = train_RF(xtrain_weighted , y_train)\n",
        "  #accuracy = evaluation_ML(xtest_weighted , y_test , svm)\n",
        "  #y_pred=svm.predict(xtest_weighted)\n",
        "  y_pred=RF.predict(xtest_weighted)\n",
        "  f1_with = f1_score(y_test,y_pred, average=\"weighted\")\n",
        "\n",
        "\n",
        "  fileNameD=\"Check_Test_alpha_\" + dataset + \".csv\"\n",
        "  print(\"saving Results to Excel:\\n\" , fileNameD)\n",
        "\n",
        "  represnationtime = time.time() - start_time\n",
        "  '''\n",
        "  import csv\n",
        "  with open(r'/content/drive/MyDrive/PHD Work/CSV_Results/'+fileNameD, 'a', newline='') as csvfile:\n",
        "      fieldnames = ['data','Model','kernell','clusters','file','alpha','accuracy','f1_without','time','F_with']\n",
        "      writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "      writer.writerow({'file':fileName ,'data':dataset, \n",
        "                      'Model':modd,'kernell':kernell,\n",
        "                      'clusters':K_clusters,'alpha':alpha,\n",
        "                      'accuracy':accuracy,'f1_without':f1_without,'time':represnationtime,\n",
        "                      'F_with':f1_with})\n",
        "'''\n",
        "  print(\"f1 with C info\", f1_with)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgkQanOSzjfQ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06Tc5jnmkW9T",
        "outputId": "3336ea9a-3e87-4bf7-f5c2-083d4db6f910"
      },
      "source": [
        "#حساب الحجم وتحويلها الى مصفوفة سبارس\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "f= csr_matrix(final_embeddings)\n",
        "\n",
        "import numpy as np\n",
        "from sys import getsizeof\n",
        "#a = final_embeddings\n",
        "#b = np.array(a)\n",
        "getsizeof(f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "64"
            ]
          },
          "execution_count": 28,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMafckFqxOiU"
      },
      "source": [
        "# **BOC**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVKpZm0fxaEJ"
      },
      "source": [
        "لتشغيل هذا الجزء يجب تنفيذ الكود السابق كله بدون الكود الخاص **بي**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFa9hdcAxYmI",
        "outputId": "645e4596-675b-49b4-fc14-3dbe4ef63515"
      },
      "source": [
        "print(\"start represnation:\\n\")\n",
        "start_time = time.time()\n",
        "cfidf = CF_IDF_BOC(documents_embeddings, Th , alpha , centers , sorted_cons)\n",
        "\n",
        "represnationtime=time.time() - start_time\n",
        "\n",
        "final_embeddings=cfidf\n",
        "# save the resulted embeddings into file\n",
        "fName=\"BOC\"+\"_\" +dataset+'_'+modd+ '_'+str(K_clusters)+'cls_'+'_0'+str(alpha)[2:]+'.txt'\n",
        "fileName=path+fName\n",
        "fileName\n",
        "saveList(final_embeddings,fileName)\n",
        "print(\"time for representation: \", represnationtime,\" seconds, file saved to: \\n\",fileName)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "start represnation:\n",
            "\n",
            "4167\n",
            "Saved successfully!\n",
            "time for representation:  966.4624905586243  seconds, file saved to: \n",
            " /content/drive/MyDrive/PHD Work/Emed/Test/BOC_WESim0303_Glove300_200cls__04.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7XKU5PhpRyQ"
      },
      "source": [
        "print(\"From saved embeddings and labels , build new data frame after deleting empty docs.... \\n\")\n",
        "embedding_DF = pd.DataFrame({'class': labels, 'document': final_embeddings})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnwuQQaEztgE",
        "outputId": "9b6409eb-9cda-44f6-ff8a-4f5badce0b59"
      },
      "source": [
        "kernell='linear'\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from tensorflow import keras\n",
        "layers = keras.layers\n",
        "models = keras.models\n",
        "\n",
        "def data_preperation(dataset , labels):\n",
        "  x_train , x_test , y_train , y_test = train_test_split(dataset , labels , test_size = 0.20 , random_state = 0)\n",
        "  return array(x_train) , array(x_test) , array(y_train) , array(y_test)\n",
        "\n",
        "def train_ML(x_train , y_train):\n",
        "  svm =  SVC(kernel=kernell).fit(x_train , y_train) \n",
        "  #svm =  LinearSVC().fit(x_train , y_train) \n",
        "  #MLPClassifier(random_state=1, max_iter=10000).fit(x_train , y_train)  \n",
        "  return svm\n",
        "\n",
        "#linear', 'poly', 'rbf', 'sigmoid',\n",
        "\n",
        "\n",
        "def train_RF(x_train , y_train):\n",
        "  rfc = RandomForestClassifier( n_estimators=500, max_depth=150,n_jobs=1).fit(x_train , y_train)\n",
        "  return rfc\n",
        "\n",
        "\n",
        "def train_DM(x_train, y_train):\n",
        "  batch_size = 32\n",
        "  epochs = 40\n",
        "  drop_ratio = 0.4\n",
        "   \n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Dense(1024))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.Dense(512))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.Dense(125))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.Dense(32))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.Dense(num_classes))\n",
        "  model.add(layers.Activation('softmax'))\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy'])\n",
        "  history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_split=0.1)\n",
        "  return model\n",
        "\n",
        "def evaluation_ML(x_test , y_test , svm_model):\n",
        "  ev=svm_model.score(x_test , y_test)\n",
        "  return ev\n",
        "def evaluation_DL(x_test , y_test , model):\n",
        "  ev=model.evaluate(x_test , y_test)\n",
        "  return ev\n",
        "\n",
        "\n",
        "final_embeddings = [array(i).reshape(-1) for i in final_embeddings]\n",
        "#labels =embedding_DF['class'].tolist() # ConvertTextLabelToNeumerical(data_DF)\n",
        "print(len(final_embeddings))\n",
        "print(len(labels))\n",
        "x_train , x_test , y_train , y_test = data_preperation(final_embeddings , labels)\n",
        "svm = train_ML(x_train , y_train)\n",
        "ev1=evaluation_ML(x_test , y_test , svm)\n",
        "rfc = train_RF(x_train , y_train)\n",
        "ev2=evaluation_ML(x_test , y_test , rfc)\n",
        "print(\"Accuaracy by SVM= \", ev1,\"Accuaracy by Random forest= \",ev2, \"\\n\")\n",
        "\n",
        "y_pred=svm.predict(x_test)\n",
        "bal=balanced_accuracy_score(y_test,y_pred)\n",
        "print(\"Balanced Accuaracy by SVM = \",bal, \"\\n\")\n",
        "\n",
        "print(\"saving Results to Excel:\\n\" , \"REUResults.csv\")\n",
        "\n",
        "import csv\n",
        "with open(r'/content/drive/MyDrive/PHD Work/Ali.csv', 'a', newline='') as csvfile:\n",
        "    fieldnames = ['data','Model','function','clusters','file','a','ML','RF','time','bal']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    writer.writerow({'file':fileName ,'data':dataset, \n",
        "                     'Model':'BOC','function':kernell,\n",
        "                     'clusters':K_clusters,'a':alpha,\n",
        "                     'ML':ev1,'RF':ev2,'time':represnationtime,\n",
        "                     'bal':bal})\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4167\n",
            "4167\n",
            "Accuaracy by SVM=  0.7745803357314148 Accuaracy by Random forest=  0.7601918465227818 \n",
            "\n",
            "Balanced Accuaracy by SVM =  0.7342952436732441 \n",
            "\n",
            "saving Results to Excel:\n",
            " REUResults.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73nsbA7jpMWD"
      },
      "source": [
        "# Averaged **W2V**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztNz2AGxvAxV"
      },
      "source": [
        "البيانات من فوق يتم استدعاء الداتا **فريم** **Текст, выделенный полужирным шрифтом**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTKLzLC0oVlm"
      },
      "source": [
        "#model = Load_pre_trained_model_embeddings(model_path)\n",
        "dataset=\"WE\"\n",
        "\n",
        "#print(\"Covert doc to embedded docs where labels= \", len(labels),\"\\n\")\n",
        "\n",
        "N=0\n",
        "documents_embeddings = embedding_all_documents(data_DF , model,N)\n",
        "labels =data_DF['class'].tolist()\n",
        "print(\"the  len of labels and docs \", len(labels),len(documents_embeddings),\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKC-FBfypL77",
        "outputId": "83db321d-4137-4ca5-9c04-ea3928855622"
      },
      "source": [
        "\n",
        "avgdoc2v=[]\n",
        "dzero=[0] * 300\n",
        "i=0\n",
        "for d in documents_embeddings:\n",
        "  l=len(d)\n",
        "  s=sum(d)\n",
        "  if l!=0:\n",
        "    av=s/l\n",
        "    avgdoc2v.append(av)\n",
        "  else:\n",
        "    avgdoc2v.append(dzero)\n",
        "  i=i+1\n",
        "\n",
        "\n",
        "final_embeddings=avgdoc2v\n",
        "print(\"Doc vector length\", len(avgdoc2v[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Doc vector length 300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xg9rZJ-oCgjm",
        "outputId": "22d515b0-5a34-42bb-bcfe-facc77b2a856"
      },
      "source": [
        "f=np.array(final_embeddings)\n",
        "f.size * f.itemsize /1024 /1024\n",
        "\n",
        "#saveList(final_embeddings,'/content/drive/MyDrive/PHD Work/avg.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2.5463104248046875"
            ]
          },
          "execution_count": 13,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvvkM6yYpXlV",
        "outputId": "5d111e2b-6167-491d-b12f-070a21825a75"
      },
      "source": [
        "kernell='poly'\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from tensorflow import keras\n",
        "layers = keras.layers\n",
        "models = keras.models\n",
        "\n",
        "\n",
        "def train_ML(x_train , y_train):\n",
        "  svm =  SVC(kernel=kernell).fit(x_train , y_train) \n",
        "  #svm =  LinearSVC().fit(x_train , y_train) \n",
        "  #MLPClassifier(random_state=1, max_iter=10000).fit(x_train , y_train)  \n",
        "  return svm\n",
        "\n",
        "#linear', 'poly', 'rbf', 'sigmoid',\n",
        "\n",
        "\n",
        "def train_RF(x_train , y_train):\n",
        "  rfc = RandomForestClassifier( n_estimators=500, max_depth=150,n_jobs=1).fit(x_train , y_train)\n",
        "  return rfc\n",
        "\n",
        "\n",
        "def train_DM(x_train, y_train):\n",
        "  batch_size = 32\n",
        "  epochs = 40\n",
        "  drop_ratio = 0.4\n",
        "   \n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Dense(1024))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.Dense(512))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.Dense(125))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.Dense(32))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.Dense(num_classes))\n",
        "  model.add(layers.Activation('softmax'))\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy'])\n",
        "  history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_split=0.1)\n",
        "  return model\n",
        "\n",
        "def evaluation_ML(x_test , y_test , svm_model):\n",
        "  ev=svm_model.score(x_test , y_test)\n",
        "  return ev\n",
        "def evaluation_DL(x_test , y_test , model):\n",
        "  ev=model.evaluate(x_test , y_test)\n",
        "  return ev\n",
        "\n",
        "\n",
        "final_embeddings = [array(i).reshape(-1) for i in final_embeddings]\n",
        "#labels =data_DF['class'].tolist() # ConvertTextLabelToNeumerical(data_DF)\n",
        "print(len(final_embeddings))\n",
        "print(len(labels))\n",
        "\n",
        "\n",
        "#y_pred=svm.predict(x_test)\n",
        "#bal=balanced_accuracy_score(y_test,y_pred)\n",
        "#print(\"Balanced Accuaracy by SVM = \",bal, \"\\n\")\n",
        "\n",
        "\n",
        "'''\n",
        "import csv\n",
        "with open(r'/content/drive/MyDrive/PHD Work/Self_AVG.csv', 'a', newline='') as csvfile:\n",
        "    fieldnames = ['data','Model','function','clusters','file','a','ML','RF','time','bal']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    writer.writerow({'file':\"averaged glove\" ,'data':dataset, \n",
        "                     'Model':modd,'function':kernell,\n",
        "                     'clusters':300,'a':0,\n",
        "                     'ML':ev1,'RF':'remove words <=2','time':0,\n",
        "                     'bal':bal})\n",
        "'''\n",
        "\n",
        "x_train , x_test , y_train , y_test = data_preperation(final_embeddings , labels, 0.30)\n",
        "print(\"resluts without calss info.............\")\n",
        "svm = train_ML(x_train , y_train)\n",
        "y_pred=svm.predict(x_test)\n",
        "f1_without = f1_score(y_test,y_pred, average=\"weighted\")\n",
        "\n",
        "print(\"f1 without C info\", f1_without)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2225\n",
            "2225\n",
            "resluts without calss info.............\n",
            "f1 without C info 0.9414935169853976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZG4LFHp3HN_"
      },
      "source": [
        "# **TfIDF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "eXe1awhZ-s3z",
        "outputId": "ccd513fa-6f13-4db7-d57a-14ba94a49acd"
      },
      "source": [
        "GN_file_path=\"/content/drive/MyDrive/PHD Work/Data/GroupNews20.csv\"\n",
        "Reuters_file_path=\"/content/drive/MyDrive/PHD Work/Data/Reuters88.csv\"\n",
        "BBC_file_path=\"/content/drive/MyDrive/PHD Work/Data/bbc-text.csv\"\n",
        "#AG_file_path=\"/content/drive/MyDrive/PHD Work/Data/AG_all_data.csv\"\n",
        "ohsumed_file_path=\"/content/drive/MyDrive/PHD Work/Data/ohsumed-allcats.csv\"\n",
        "\n",
        "\n",
        "data_DF = pd.read_csv(ohsumed_file_path)\n",
        "data_DF.columns.values[0] = \"class\"\n",
        "data_DF.columns.values[1] = \"document\"\n",
        "\n",
        "data_DF['class'] = pd.factorize(data_DF['class'])[0]\n",
        "\n",
        "data_DF.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>document</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Laparoscopic treatment of perforated peptic ul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Experimental sympathetic activation causes end...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Myocyte cell loss and myocyte cellular hyperpl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>Absorption and motility of the bypassed human ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>Evolution of fundic argyrophil cell hyperplasi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   class                                           document\n",
              "0      0  Laparoscopic treatment of perforated peptic ul...\n",
              "1      0  Experimental sympathetic activation causes end...\n",
              "2      1  Myocyte cell loss and myocyte cellular hyperpl...\n",
              "3      0  Absorption and motility of the bypassed human ...\n",
              "4      1  Evolution of fundic argyrophil cell hyperplasi..."
            ]
          },
          "execution_count": 42,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AECSL7-7B-sU",
        "outputId": "e5fff374-1d7d-4a2c-ff8d-be6e8ca69c3c"
      },
      "source": [
        "#تتضمن المعالجة المسبقة لكل مستند دون تحويله الى توكن\n",
        "# Spliting into X & y\n",
        "X=documents_as_list_of_sentences(data_DF)\n",
        "y = data_DF.iloc[:, 0].values\n",
        "\n",
        "\n",
        "# Splitting into training & test subsets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3,  random_state = 0)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 0  0  0 ... 19 19 19]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12fc6F_8UMor"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import cross_val_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ydr_9hx20cRb"
      },
      "source": [
        "## حساب حجم المتجه"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1CgvJKpi9Do_"
      },
      "source": [
        "dataset=\"RE\"\n",
        "\n",
        "stime=time.time()\n",
        "# Building a TF IDF matrix out of the corpus of reviews\n",
        "vectorizer = TfidfVectorizer(sublinear_tf=False, max_df=0.5,  stop_words='english')\n",
        "\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "\n",
        "stime=time.time()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JGzczgK3H74"
      },
      "source": [
        "X_test = vectorizer.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amAlP0bP3lcu",
        "outputId": "7112caa4-322c-492e-e6d0-e50091bafd2c"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "kernell='poly'\n",
        "\n",
        "print(\"etime\",etime,\"\\n\")\n",
        "\n",
        "svm = train_ML(X_train , y_train)\n",
        "y_pred=svm.predict(X_test)\n",
        "f1_without = f1_score(y_test,y_pred, average=\"weighted\")\n",
        "\n",
        "print(\"f1 without C info\", f1_without)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "etime 5.593511581420898 \n",
            "\n",
            "f1 without C info 0.8075966391320009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSPcjaCk3WpO",
        "outputId": "5211d510-d706-450e-ae6c-509e2a6d7655"
      },
      "source": [
        "#####old\n",
        "ev1=evaluation_ML(X_test , y_test , svm)\n",
        "\n",
        "\n",
        "y_pred=svm.predict(X_test)\n",
        "bal=balanced_accuracy_score(y_test,y_pred)\n",
        "print(\"Balanced Accuaracy by SVM = \",bal, \"\\n\")\n",
        "\n",
        "\n",
        "import csv\n",
        "with open(r'/content/drive/MyDrive/PHD Work/Ali.csv', 'a', newline='') as csvfile:\n",
        "    fieldnames = ['data','Model','function','clusters','file','a','ML','RF','time','bal']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    writer.writerow({'file':\"Tf-idf\" ,'data':dataset, \n",
        "                     'Model':'modd','function':'no sublinear',\n",
        "                     'clusters':X_test.shape,'a':'18820',\n",
        "                     'ML':ev1,'RF':'maxdf 0.8 no sub','time':etime,\n",
        "                     'bal':bal})\n",
        "\n",
        "\n",
        "\n",
        "print(\"Accuaracy by SVM= \", ev1, \"\\n\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Balanced Accuaracy by SVM =  0.8323086257607694 \n",
            "\n",
            "Accuaracy by SVM=  0.8755952380952381 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ne3Psosj-QPq",
        "outputId": "4ba0a3db-ca79-4852-d64e-3d683292a554"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(11292, 75339)"
            ]
          },
          "execution_count": 91,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gm6OtcCF_8Ra"
      },
      "source": [
        "# **BoW**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "U6YjKpKR9Xlp",
        "outputId": "05bd4618-fef7-4ffe-80c3-c50bc1c43240"
      },
      "source": [
        "GN_file_path=\"/content/drive/MyDrive/PHD Work/Data/GroupNews20.csv\"\n",
        "Reuters_file_path=\"/content/drive/MyDrive/PHD Work/Data/Reuters88.csv\"\n",
        "BBC_file_path=\"/content/drive/MyDrive/PHD Work/Data/bbc-text.csv\"\n",
        "#AG_file_path=\"/content/drive/MyDrive/PHD Work/Data/AG_all_data.csv\"\n",
        "ohsumed_file_path=\"/content/drive/MyDrive/PHD Work/Data/ohsumed-allcats.csv\"\n",
        "\n",
        "\n",
        "data_DF = pd.read_csv(ohsumed_file_path)\n",
        "data_DF.columns.values[0] = \"class\"\n",
        "data_DF.columns.values[1] = \"document\"\n",
        "\n",
        "data_DF['class'] = pd.factorize(data_DF['class'])[0]\n",
        "\n",
        "data_DF.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>document</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Laparoscopic treatment of perforated peptic ul...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>Experimental sympathetic activation causes end...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Myocyte cell loss and myocyte cellular hyperpl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>Absorption and motility of the bypassed human ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>Evolution of fundic argyrophil cell hyperplasi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   class                                           document\n",
              "0      0  Laparoscopic treatment of perforated peptic ul...\n",
              "1      0  Experimental sympathetic activation causes end...\n",
              "2      1  Myocyte cell loss and myocyte cellular hyperpl...\n",
              "3      0  Absorption and motility of the bypassed human ...\n",
              "4      1  Evolution of fundic argyrophil cell hyperplasi..."
            ]
          },
          "execution_count": 102,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjwPdLp9nCnH",
        "outputId": "0414c007-e760-4e82-9230-059c6d6360ed"
      },
      "source": [
        "#تتضمن المعالجة المسبقة لكل مستند دون تحويله الى توكن\n",
        "# Spliting into X & y\n",
        "X=documents_as_list_of_sentences(data_DF)\n",
        "y = data_DF.iloc[:, 0].values\n",
        "\n",
        "\n",
        "# Splitting into training & test subsets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3,  random_state = 0)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 0  0  0 ... 19 19 19]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oYb_NhAUD2V"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import cross_val_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDbGlpF0oW1i"
      },
      "source": [
        "dataset=\"BBC\"\n",
        "\n",
        "stime=time.time()\n",
        "# Building a TF IDF matrix out of the corpus of reviews\n",
        "vec = CountVectorizer(max_df=0.5, lowercase=True)\n",
        "\n",
        "X_train = vec.fit_transform(X_train)\n",
        "X_test = vec.transform(X_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieHmW56oSSC_",
        "outputId": "17d871fe-7fc7-40e2-e50d-acffd8e23fbe"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "kernell='poly'\n",
        "\n",
        "print(\"etime\",etime,\"\\n\")\n",
        "\n",
        "svm = train_ML(X_train , y_train)\n",
        "y_pred=svm.predict(X_test)\n",
        "f1_without = f1_score(y_test,y_pred, average=\"weighted\")\n",
        "\n",
        "print(\"f1 without C info\", f1_without)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "etime 5.593511581420898 \n",
            "\n",
            "f1 without C info 0.10181181496532432\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feoErVy9_y-i",
        "outputId": "051ab813-c430-465e-bd54-97f8eb0b5c37"
      },
      "source": [
        "ev1=evaluation_ML(X_test , y_test , svm)\n",
        "\n",
        "\n",
        "y_pred=svm.predict(X_test)\n",
        "bal=balanced_accuracy_score(y_test,y_pred)\n",
        "print(\"Balanced Accuaracy by SVM = \",bal, \"\\n\")\n",
        "\n",
        "\n",
        "import csv\n",
        "with open(r'/content/drive/MyDrive/PHD Work/Ali.csv', 'a', newline='') as csvfile:\n",
        "    fieldnames = ['data','Model','function','clusters','file','a','ML','RF','time','bal']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    writer.writerow({'file':\"BOW\" ,'data':dataset, \n",
        "                     'Model':'modd','function':'rbf',\n",
        "                     'clusters':X_test.shape,'a':'maxidf05 lower',\n",
        "                     'ML':ev1,'RF':'no idf','time':etime,\n",
        "                     'bal':bal})\n",
        "\n",
        "\n",
        "\n",
        "print(\"Accuaracy by SVM= \", ev1, \"\\n\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Balanced Accuaracy by SVM =  0.3246159672324143 \n",
            "\n",
            "Accuaracy by SVM=  0.6920693928128873 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnAiOtlsAM7N",
        "outputId": "22a1f94a-3bda-42ae-e6f3-e1501d8c8e80"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(11292, 75423)"
            ]
          },
          "execution_count": 81,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcziZjSb_ywd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ma358FrRL5g"
      },
      "source": [
        "# **VLAC**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-HiROsAQyn1",
        "outputId": "e11d941f-c336-418d-b5cb-569c96049fe4"
      },
      "source": [
        "pip install vlac\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting vlac\n",
            "  Downloading vlac-0.1.2.5-py3-none-any.whl (7.2 kB)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from vlac) (0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from vlac) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->vlac) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->vlac) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->vlac) (1.4.1)\n",
            "Installing collected packages: vlac\n",
            "Successfully installed vlac-0.1.2.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vV7AwjjLRPIc"
      },
      "source": [
        "#######################################################################خاص بالموديل\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "############################################################خاص بفلاس\n",
        "from vlac import VLAC\n",
        "import pickle\n",
        "\n",
        "# Training the classifier & predicting on test data\n",
        "def train_ML(x_train , y_train):\n",
        "  svm =  SVC(kernel=krenell).fit(x_train , y_train) \n",
        "  #svm =  LinearSVC().fit(x_train , y_train) \n",
        "  return svm\n",
        "def evaluation_ML(x_test , y_test , svm_model):\n",
        "  ev=svm_model.score(x_test , y_test)\n",
        "  return ev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SObFaxERUsS"
      },
      "source": [
        "#تحميل موديل مدرب على البيانات سابقا\n",
        "# او مدرب مسبقا على كوربوس\n",
        "# Contains embeddings for Reuters R8\n",
        "with open('/content/drive/MyDrive/PHD Work/DataVLAC/r8_glove_1f.pickle', 'rb') as handle:\n",
        "    model = pickle.load(handle)\n",
        "\n",
        "#model = Load_pre_trained_model_embeddings(model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzyykfHxRO9g"
      },
      "source": [
        "# Load data\n",
        "with open('/content/drive/MyDrive/PHD Work/DataVLAC/r8_docs.txt', \"r\") as f:\n",
        "    docs = f.readlines()\n",
        "with open('/content/drive/MyDrive/PHD Work/DataVLAC/r8_labels.txt') as f:\n",
        "    labels=[line for line in f]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FErduLK-RF6_"
      },
      "source": [
        "#self embeding\n",
        "#documents_as_list = documents_as_list(data_DF)\n",
        "model = Word2Vec(sentences=docs, size=300, window=5, min_count=5, workers=4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yKf_BvBROog"
      },
      "source": [
        "# Train model and transform collection of documents\n",
        "vlac_model = VLAC(documents=docs, model=model, oov=False)\n",
        "vlac_featuress, kmeans = vlac_model.fit_transform(num_concepts=30)\n",
        "\n",
        "# Create features for new documents\n",
        "vlac_model = VLAC(documents=docs, model=model, oov=False)\n",
        "vlac_features = vlac_model.transform(kmeans=kmeans)\n",
        "\n",
        "\n",
        "#حفظ الميزات الناتجة\n",
        "fileName=\"/content/drive/MyDrive/PHD Work/Emed/VLAC/VLAC_self_RE8.txt\"\n",
        "saveList(vlac_features,fileName)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NExTosZ1Rx9Z"
      },
      "source": [
        "documents_embeddings=vlac_features\n",
        "krenell=\"poly\"\n",
        "dataset=\"BBC\"\n",
        "# Splitting into training & test subsets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(documents_embeddings, labels, test_size = 0.2,\n",
        "                                                    random_state = 0)\n",
        "\n",
        "\n",
        "\n",
        "stime=time.time()\n",
        "ev1=evaluation_ML(X_test , y_test , svm)\n",
        "etime=time.time()-stime\n",
        "print(\"etime\",etime,\"\\n\")\n",
        "print(\"Accuaracy by SVM= \", ev1, \"\\n\")\n",
        "\n",
        "y_pred=svm.predict(X_test)\n",
        "bal=balanced_accuracy_score(y_test,y_pred)\n",
        "print(\"Balanced Accuaracy by SVM = \",bal, \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_ILFbQn4_dT"
      },
      "source": [
        "# **Deleted**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpUmSoZL40fT"
      },
      "source": [
        "epochs=40\n",
        "import csv\n",
        "\n",
        "with open(r'/content/drive/MyDrive/PHD Work/TestFinalResults.csv', 'a', newline='') as csvfile:\n",
        "    fieldnames = ['data','Model','function','clusters','file','a','ML','RF','time','bal']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    writer.writerow({'file':fileName ,'data':dataset, 'Model':modd,'function':kernell,'clusters':K_clusters,'a':alpha,'ML':ev1,'RF':ev2,'time':represnationtime,'bal':bal})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmXsg5byAX3z"
      },
      "source": [
        "## **Deleted code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaEw4p_ZSfVm"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "X=documents_as_list_of_sentences(data_DF)\n",
        "y = data_DF.iloc[:, 0].values\n",
        "\n",
        "# Building a TF IDF matrix out of the corpus of reviews\n",
        "vectorizer = CountVectorizer( max_df=0.5,  stop_words='english')\n",
        "\n",
        "# Splitting into training & test subsets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3,  random_state = 0)\n",
        "print(y)\n",
        "\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test= vectorizer.transform(X_test)\n",
        "\n",
        "train_tfidf = X_train.todense()\n",
        "\n",
        "test_tfidf = X_test.todense()\n",
        "a= train_tfidf.size * train_tfidf.itemsize \n",
        "b= test_tfidf.size * test_tfidf.itemsize\n",
        "(a+b) / 1024 /1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxU7vUgh_toX"
      },
      "source": [
        "import numpy as np\n",
        "from sys import getsizeof\n",
        "a = final_embeddings\n",
        "b = np.array(a)\n",
        "round(getsizeof(b) / 1024 / 1024,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2O7CoXvAi3Lj"
      },
      "source": [
        "\n",
        "'''\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "# Load some categories from the training set\n",
        "\n",
        "# خمسة فئات\n",
        "categories = [\n",
        "    'rec.sport.hockey',\n",
        "    'talk.religion.misc',\n",
        "    'comp.graphics',\n",
        "    'sci.space',\n",
        "    'talk.politics.guns',\n",
        "\n",
        "]\n",
        "#categories=None\n",
        "news_group = fetch_20newsgroups(subset='all', categories=categories, shuffle=True, random_state=42)\n",
        "news_group_data = news_group.data\n",
        "news_group_target_names = news_group.target_names\n",
        "news_group_target = news_group.target\n",
        "# Creating a dataframe from the loaded data\n",
        "data_DF = pd.DataFrame({'class': news_group_target, \n",
        "                        'document': news_group_data})\n",
        "                        \n",
        "##############################استخراج غروب نيوز\n",
        "# loading dataset\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "# Load some categories from the training set\n",
        "# 8 فئات\n",
        "categories1 = [\n",
        "    'rec.sport.hockey',\n",
        "    'talk.religion.misc',\n",
        "    'comp.graphics',\n",
        "    'sci.space',\n",
        "    'talk.politics.guns',\n",
        "    'sci.electronics',\n",
        "    'sci.med',\n",
        "    'rec.sport.baseball'\n",
        "]\n",
        "# خمسة فئات\n",
        "categories = [\n",
        "    'rec.sport.hockey',\n",
        "    'talk.religion.misc',\n",
        "    'comp.graphics',\n",
        "    'sci.space',\n",
        "    'talk.politics.guns',\n",
        "\n",
        "]\n",
        "categories=None\n",
        "\n",
        "news_group = fetch_20newsgroups(subset='all', categories=categories, shuffle=True, random_state=42)\n",
        "news_group_data = news_group.data\n",
        "news_group_target_names = news_group.target_names\n",
        "news_group_target = news_group.target\n",
        "# Creating a dataframe from the loaded data\n",
        "data_DF = pd.DataFrame({'class': news_group_target, \n",
        "                        'document': news_group_data})\n",
        "\n",
        "#####################################\n",
        "# معالجة بيانات رويترز\n",
        "b= [ 13,24,0,1,2,3,8,10]\n",
        "a= [x for x in range(90)]\n",
        "x=[]\n",
        "for i in a:\n",
        "  if i not in b:\n",
        "    x.append(i)\n",
        "len(x)\n",
        "data= data_DF[~data_DF['class'].isin(x)]\n",
        "#data.to_csv(r'/content/drive/MyDrive/PHD Work/Data/Reuters8.csv', index = False)\n",
        "data_DF = pd.read_csv(\"/content/drive/MyDrive/PHD Work/Data/Reuters8.csv\")\n",
        "data_DF.head()\n",
        "\n",
        "\n",
        "######################################3\n",
        "# convert class to numerical values\n",
        "def label_to_num(labeles):\n",
        "  if (labeles=='tech'):\n",
        "    labeles=0\n",
        "  if (labeles=='business'):\n",
        "    labeles=1\n",
        "  if labeles=='entertainment':\n",
        "    labeles=2\n",
        "  if labeles=='sport':\n",
        "    labeles=3\n",
        "  if labeles=='politics':\n",
        "    labeles=4\n",
        "  return labeles\n",
        "\n",
        "BBC_data['category'] = BBC_data['category'].apply(label_to_num)\n",
        "BBC_data.head()\n",
        "##################################################################\n",
        "def Ali_Representation(documents_embeddings , M ,  sim_val , alpha , centers , dictionary):\n",
        "  embeddings = []\n",
        "  counter = 0\n",
        "  IDf = np.zeros(len(centers)).tolist()\n",
        "  for i in documents_embeddings: \n",
        "    v = Doc2Vec(i, M ,sim_val , alpha , centers , dictionary)\n",
        "    ######## حساب عدد مرات ظهور المفهوم في كل نص\n",
        "    for i in v:\n",
        "      if i[M-1] > 0:\n",
        "        IDf[counter] = IDf[counter] + 1\n",
        "      counter = counter+1\n",
        "    counter = 0\n",
        "    embeddings.append(v)\n",
        "  #########  calculate IDF\n",
        "  for i in IDf:\n",
        "    i = math.log(len(embeddings) / (i+1))\n",
        "  ######## Calculate Cf-IDF\n",
        "  for i in range(len(embeddings)): \n",
        "    for j in range(len(embeddings[i])): \n",
        "       embeddings[i][j][M-1] = embeddings[i][j][M-1] * IDf[j]\n",
        "  return embeddings\n",
        "  \n",
        "#############################################\n",
        "#change last weight\n",
        "M=10 # 0 1 2 3 4\n",
        "#saved_emb=changLastCFIDF(saved_embaddings)\n",
        "#saved_emb=deleteLastCFIDF(saved_embaddings)\n",
        "#saved_emb=resize_embedding(5,2,saved_embaddings)\n",
        "saved_emb=deletFeatures(0,0,saved_embaddings)\n",
        "###################################################################\n",
        "\n",
        "if len(groups[0])>1:\n",
        "  del groups[0][0:len(groups[0])-1]\n",
        "else:\n",
        "  l0=groups[0]\n",
        "\n",
        "if len(groups[1])>1:\n",
        "  del groups[1][0:len(groups[1])-1]\n",
        "else:\n",
        "  l1=groups[1]\n",
        "\n",
        "if len(groups[2])>1:\n",
        "  del groups[2][0:len(groups[2])-1]\n",
        "else:\n",
        "  l2=groups[2]\n",
        "\n",
        "if len(groups[3])>1:\n",
        "  del groups[3][0:len(groups[3])-1]\n",
        "else:\n",
        "  del groups[3]\n",
        "\n",
        "if len(groups[4])>1:\n",
        "  del groups[4][0:len(groups[4])-1]\n",
        "else:\n",
        "  l4=groups[4]\n",
        "#print( 'l0=', l0,'l1=',l1  ,' l2 =',l2,' l3 =',l3, 'l4 =',l4 )\n",
        "\n",
        "newlabels= Convert_list_of_list_to_list(groups)\n",
        "newlabels\n",
        "\n",
        "\n",
        "#############################################\n",
        "def changLastCFIDF(saved_embaddings):\n",
        "  numOfDocs=len(saved_embaddings)\n",
        "  for d in saved_embaddings:\n",
        "    for r in d:\n",
        "      l=len(r)\n",
        "      if r[l-1]!=0:\n",
        "        r[l-1]=math.log10( (numOfDocs+1) /r[l-1])\n",
        "  return saved_embaddings\n",
        "###########################################\n",
        "#delet last weght\n",
        "def deleteLastCFIDF(saved_embaddings):\n",
        "  for d in saved_embaddings:\n",
        "    for r in d:\n",
        "      l=len(r)\n",
        "      del r[l-1]\n",
        "  return saved_embaddings\n",
        "##########################################33\n",
        "#delet last weght\n",
        "def deletFeatures(fromF,toF,saved_embaddings):\n",
        "  for d in saved_embaddings:\n",
        "    for r in d:\n",
        "      l=len(r)\n",
        "      del r[fromF:toF]\n",
        "  return saved_embaddings\n",
        "\n",
        "########################################################################################################\n",
        "# function for max similarity selection from each column\n",
        "def max_sim(similarities , M):\n",
        "    #### Removing All Zeros Rows #####\n",
        "    #start_time = time.time()\n",
        "    zero_row = np.zeros((1,M)).tolist()\n",
        "    similarities= [i for i in similarities if i != zero_row[0]]\n",
        "    #t=time.time() - start_time;\n",
        "\n",
        "    ### If no words has similarity degree with the concepts\n",
        "    if len(similarities)==0:\n",
        "      return np.zeros((1,M)).tolist()\n",
        "    else :\n",
        "      similarities = np.array(similarities)\n",
        "      N=similarities.shape[0]   \n",
        "      sim_list=[]\n",
        "      if N==1:\n",
        "          sim_list.append(similarities[0,0:M]) #####################################################\n",
        "          #return sim_list\n",
        "      else:\n",
        "          for x in range(N):\n",
        "              #sort row x in descending order\n",
        "              similarities[x,:].sort()\n",
        "              #reverse row x to get max values first\n",
        "              similarities[x,:]=np.flipud(similarities[x,:])\n",
        "          \n",
        "              #select M values from columns, column by column\n",
        "          for j in range(M):\n",
        "              for i in range(N):\n",
        "                  if len(sim_list)<M:\n",
        "                      sim_list.append(similarities[i,j]) \n",
        "                  else:\n",
        "                      #break if we got M values\n",
        "                      break;\n",
        "    return sim_list\n",
        "#########################################\n",
        "#متجه المستند بوزنين هما تردد المفهوم ومتوسط درجة تشابه كلماته مع المستند قبل تحويله الى ميزة واحدة\n",
        "def Doc2Vec(embeddeings_document,threshold_sim,alpha,centers,dictionary):\n",
        "  M=2\n",
        "  concept_freq = 0\n",
        "  ## intilize doc array\n",
        "  doc2vec = np.zeros((len(centers)).tolist()\n",
        "  for cnt in range(len(centers)):\n",
        "    counter = 0;\n",
        "    sim =[]\n",
        "    sim_array =[]\n",
        "    for emb_word in embeddeings_document:\n",
        "      Smean =Get_similarity_matrix(emb_word , dictionary[cnt]);\n",
        "      if Smean  > alpha:\n",
        "        concept_freq = concept_freq+1 #for tf\n",
        "        sim.append(Smean)\n",
        "        doc2vec[cnt][M-1] = doc2vec[cnt][M-1] + 1;\n",
        "      else:\n",
        "        continue;\n",
        "      counter = counter +1;\n",
        "    # end for doc words with one concepts\n",
        "    meanofSim=0#  المتوسط لدرجات تشابه كلمات المستند مع المفهوم الحالي\n",
        "    if(len(sim)!=0):\n",
        "      meanofSim=sum(sim)/len(sim)\n",
        "    doc2vec[cnt][M-2]=meanofSim\n",
        "    doc2vec[cnt][0:M-2] = sim_array[0:M-2]\n",
        "    del sim\n",
        "\n",
        "  if concept_freq !=0: # the number of occurence for all concept in this document\n",
        "    for i in doc2vec:\n",
        "      i[M-1] = i[M-1]*i[M-2] / concept_freq\n",
        "      #print('concept_freq',concept_freq,' tf',i[M-1])\n",
        "  #get_concept_freq(doc2vec)\n",
        "  return doc2vec\n",
        "\n",
        "###########\n",
        "#كود للاحتياط\n",
        "  #data['vector']=BBC_data['text'].apply(BoET,args=(model,centers , sorted_cons,0.44,2,10))\n",
        "\n",
        "\n",
        "def get_cluster_elements(k_means , embedding_matrix):\n",
        "  # Nice Pythonic way to get the indices of the points for each corresponding cluster\n",
        "  mydict = {i: np.where(k_means.labels_ == i)[0] for i in range(k_means.n_clusters)}\n",
        "  embeddings_samples = []\n",
        "  for i in range(len(mydict)):\n",
        "    temp= []\n",
        "    for j in mydict[i]:\n",
        "      temp.append(embedding_matrix[j])\n",
        "    embeddings_samples.append(temp)\n",
        "    del temp\n",
        "  return embeddings_samples\n",
        "\n",
        "def pre_processing_dataset(data):\n",
        "  tech = []; entertainment=[]; business=[]; politics=[]; sport=[];\n",
        "  All_tokens = [];\n",
        "  for i in range(len(data)):\n",
        "    if data.iloc[i][0] == 'tech':\n",
        "      tech.append(pre_processing(data.iloc[i][1]))\n",
        "    if data.iloc[i][0] == 'entertainment':\n",
        "     entertainment.append(pre_processing(data.iloc[i][1]))\n",
        "    if data.iloc[i][0] == 'business':\n",
        "     business.append(pre_processing(data.iloc[i][1]))\n",
        "    if data.iloc[i][0] == 'politics':\n",
        "     politics.append(pre_processing(data.iloc[i][1]))\n",
        "    if data.iloc[i][0] == 'sport':\n",
        "     sport.append(pre_processing(data.iloc[i][1]))\n",
        "\n",
        "  All_tokens = tech + entertainment +  business + politics + sport\n",
        "  All_tokens = Convert_list_of_list_to_list(All_tokens)\n",
        "\n",
        "  print('number of technolgy samples' , len(tech))\n",
        "  print('number of entertainment samples' , len(entertainment) )\n",
        "  print('number of business samples' , len(business))\n",
        "  print('number of politics samples' , len(politics))\n",
        "  print('number of sport samples' , len(sport))\n",
        "  print('the number of words in Crorpus' , len(All_tokens))\n",
        "   \n",
        "  return tech , entertainment ,  business , politics , sport , All_tokens\n",
        "\n",
        "\n",
        "def ConvertTextLabelToNeumerical(df):\n",
        "  labels = []\n",
        "  for i in df['class']:\n",
        "    if i == 'tech':\n",
        "      labels.append(0)\n",
        "    elif i =='business':\n",
        "      labels.append(1)\n",
        "    elif i == 'sport':\n",
        "      labels.append(2)\n",
        "    elif i =='entertainment':\n",
        "      labels.append(3)\n",
        "    elif i =='politics':\n",
        "      labels.append(4)\n",
        "  return labels\n",
        "  \n",
        "\n",
        "  def ConvertTextLabelToNeumerical(df):\n",
        "  labels = []\n",
        "  for i in df['class']:\n",
        "    labels.append(i)\n",
        "  return labels\n",
        "  '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv20cox_cYam"
      },
      "source": [
        ""
      ]
    }
  ]
}