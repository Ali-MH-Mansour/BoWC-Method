{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Self_UnSelf_TFIDF_Prun_BoWC17082021_alpha.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "4NA1crRuZT5E"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ali-MH-Mansour/BoWC-Method/blob/main/codes/oldversions/Self_UnSelf_TFIDF_Prun_BoWC17082021_alpha.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34aOryuJLoRN"
      },
      "source": [
        "*Курсив*# 10 07 2021 **طريقتي مع طريقة tfidf**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gge2NxpEmGsT"
      },
      "source": [
        "# **Requirements  || Определение библиотек**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_I0ZTb-sqxH7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#drive.flush_and_unmount(timeout_ms=24)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5btWsnJyjkJh"
      },
      "source": [
        "## Importing all Library here "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yPskImTLE1Q",
        "outputId": "e428767e-0aac-41cf-b580-d64cb5fa826e"
      },
      "source": [
        "################################ new\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk import word_tokenize, sent_tokenize, pos_tag\n",
        "\n",
        "def final_preprocess_text(text):\n",
        "    # 1. Tokenise to alphabetic tokens\n",
        "    text = remove_numbers(text)\n",
        "    text = remove_punctuation(text)\n",
        "    text = convert_to_lower(text)\n",
        "    text = remove_white_space(text)\n",
        "    text = remove_short_words(text)\n",
        "    tokens = toknizing(text)\n",
        "    #tokeniser = RegexpTokenizer(r'[A-Za-z]+')\n",
        "    #tokens = tokeniser.tokenize(text)\n",
        "    \n",
        "    # 2. POS tagging\n",
        "    pos_map = {'J': 'a', 'N': 'n', 'R': 'r', 'V': 'v'}\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    \n",
        "    # 3. Lowercase and lemmatise \n",
        "    lemmatiser = WordNetLemmatizer()\n",
        "    tokens = [lemmatiser.lemmatize(t.lower(), pos=pos_map.get(p[0], 'v')) for t, p in pos_tags]\n",
        "    return tokens\n",
        "#########################################################################\n",
        "def new_pre_processing_dataset(data):\n",
        "  All_tokens = [];\n",
        "  for i in range(len(data)):\n",
        "    All_tokens.append(final_preprocess_text(data.iloc[i][1]))\n",
        "  All_tokens = Convert_list_of_list_to_list(All_tokens)\n",
        "  print('the number of words in Crorpus' , len(All_tokens))\n",
        "\n",
        "  return All_tokens\n",
        "\n",
        "\n",
        "###########################\n",
        "my_stopwords= [\"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\",\"aab\",'aaab', \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"affects\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"ain't\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appear\", \"appreciate\", \"appropriate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"aren't\", \"arise\", \"around\", \"as\", \"a's\", \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"can't\", \"cause\", \"causes\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"changes\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"c'mon\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"could\", \"couldn\", \"couldnt\", \"couldn't\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"c's\", \"ct\", \"cu\", \"currently\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"didn't\", \"different\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"done\", \"don't\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"effect\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"empty\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"h2\", \"h3\", \"had\", \"hadn\", \"hadn't\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"hed\", \"he'd\", \"he'll\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"here's\", \"hereupon\", \"hers\", \"herself\", \"hes\", \"he's\", \"hh\", \"hi\", \"hid\", \"him\", \"himself\", \"his\", \"hither\", \"hj\", \"ho\", \"home\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"how's\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"i'd\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"i'll\", \"im\", \"i'm\", \"immediate\", \"immediately\", \"importance\", \"important\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"invention\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"isn't\", \"it\", \"itd\", \"it'd\", \"it'll\", \"its\", \"it's\", \"itself\", \"iv\", \"i've\", \"ix\", \"iy\", \"iz\", \"j\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"know\", \"known\", \"knows\", \"ko\", \"l\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"let's\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mightn't\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"n\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needn\", \"needn't\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"nothing\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"other\", \"others\", \"otherwise\", \"ou\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"present\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"s2\", \"sa\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shan't\", \"she\", \"shed\", \"she'd\", \"she'll\", \"shes\", \"she's\", \"should\", \"shouldn\", \"shouldn't\", \"should've\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"system\", \"sz\", \"t\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"thats\", \"that's\", \"that've\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"there's\", \"thereto\", \"thereupon\", \"there've\", \"these\", \"they\", \"theyd\", \"they'd\", \"they'll\", \"theyre\", \"they're\", \"they've\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"t's\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"ut\", \"v\", \"va\", \"value\", \"various\", \"vd\", \"ve\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"wa\", \"want\", \"wants\", \"was\", \"wasn\", \"wasnt\", \"wasn't\", \"way\", \"we\", \"wed\", \"we'd\", \"welcome\", \"well\", \"we'll\", \"well-b\", \"went\", \"were\", \"we're\", \"weren\", \"werent\", \"weren't\", \"we've\", \"what\", \"whatever\", \"what'll\", \"whats\", \"what's\", \"when\", \"whence\", \"whenever\", \"when's\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"where's\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whom\", \"whomever\", \"whos\", \"who's\", \"whose\", \"why\", \"why's\", \"wi\", \"widely\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"won't\", \"words\", \"world\", \"would\", \"wouldn\", \"wouldnt\", \"wouldn't\", \"www\", \"x\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"you'd\", \"you'll\", \"your\", \"youre\", \"you're\", \"yours\", \"yourself\", \"yourselves\", \"you've\", \"yr\", \"ys\", \"yt\", \"z\", \"zero\", \"zi\", \"zz\"]\n",
        "\n",
        "!pip install sklearn\n",
        "!pip install spherecluster\n",
        "!pip install soyclustering\n",
        "#################################### Importing all Library here  ################################################\n",
        "from soyclustering import SphericalKMeans\n",
        "import time\n",
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "from numpy import array \n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords \n",
        "from numpy import array\n",
        "import matplotlib.pyplot as plt\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import gensim.downloader\n",
        "from collections import Counter \n",
        "import functools\n",
        "import math\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.sparse import csr_matrix\n",
        "#from sklearn_extra.cluster import KMedoids\n",
        "#from spherecluster import SphericalKMeans\n",
        "############ Our Downloads ##########\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "##################################\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "import numpy as np\n",
        "np.random.seed(2018)\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "############################################\n",
        "#machine learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "##################################  Defining Our Methods Here    ##################################################\n",
        "# Save the pre_trained model embeddings.. \n",
        "def save_embeddings(model, file_path):\n",
        "  word_vectors = model.wv\n",
        "  word_vectors.save(file_path)\n",
        "####################################################################################\n",
        "### loading the embeddings\n",
        "def Load_pre_trained_model_embeddings(file_path):\n",
        "  wv = KeyedVectors.load(file_path, mmap='r')\n",
        "  return wv\n",
        "####################################################################################\n",
        "### install pre_trained_model\n",
        "def intall_pre_trained_model(file_path, model_name):\n",
        "  model = gensim.downloader.load(model_name)\n",
        "  save_embeddings(model , file_path)\n",
        "  return model\n",
        "  print('the models are intalled and saved in the \" ' + file_path + '\"')\n",
        "####################################################################################\n",
        "def convert_to_lower(text):\n",
        "  return text.lower()\n",
        "####################################################################################\n",
        "def remove_numbers(text):\n",
        "  text = re.sub(r'\\d+' , '', text)\n",
        "  return text\n",
        "#################################################\n",
        "def remove_short_words(text):\n",
        "  text = re.sub(r'\\b\\w{1,2}\\b', '', text)\n",
        "  return text\n",
        "####################################################################################\n",
        "def remove_punctuation(text):\n",
        "     punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^+&*_~'''\n",
        "     no_punct = \"\"\n",
        "     for char in text:\n",
        "        if char not in punctuations:\n",
        "            no_punct = no_punct + char\n",
        "     return no_punct\n",
        "####################################################################################\n",
        "def remove_white_space(text):\n",
        "  text = text.strip()\n",
        "  return text\n",
        "####################################################################################\n",
        "def toknizing(text):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = word_tokenize(text)\n",
        "  ## Remove Stopwords from tokens\n",
        "  result = [i for i in tokens if not i in stop_words]\n",
        "  return result\n",
        "###################################################################################\n",
        "def remove_duplication(text):\n",
        "  return list(set(text)) \n",
        "####################################################################################\n",
        "def pre_processing(text):\n",
        "  text = remove_numbers(text)\n",
        "  text = remove_punctuation(text)\n",
        "  text = convert_to_lower(text)\n",
        "  text = remove_white_space(text)\n",
        "  text = remove_short_words(text)\n",
        "  #text = preprocess_toklem(text)\n",
        "  text = toknizing(text)\n",
        "  return text \n",
        "#######################################\n",
        "def pre_processing_no_toknize(text):\n",
        "  text = remove_numbers(text)\n",
        "  text = remove_punctuation(text)\n",
        "  text = convert_to_lower(text)\n",
        "  text = remove_white_space(text)\n",
        "  text = remove_short_words(text)\n",
        "  #text = preprocess_toklem(text)\n",
        "  #text = toknizing(text)\n",
        "  return text\n",
        "####################################################################################\n",
        "def lemmatize_stemming(text):\n",
        "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
        "#######\n",
        "def preprocess_toklem(text):\n",
        "    result = []\n",
        "    for token in gensim.utils.simple_preprocess(text):\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) >= 3:\n",
        "            result.append(lemmatize_stemming(token))\n",
        "    return result\n",
        "\n",
        "def plot_pie_distribution(labels , metric, name):\n",
        "      # Create figure and plot pie:\n",
        "      fig = plt.figure(figsize=(4,4))\n",
        "      colors = ['skyblue', 'peru', 'gray' , 'red' , 'blue']\n",
        "      plt.pie(metric, labels = labels , autopct='%d%%', colors=colors)\n",
        "      plt.axis('equal')\n",
        "      plt.title(name + ' Distribution', fontsize='20')\n",
        "      plt.savefig('plot_eight.png')\n",
        "      plt.show()\n",
        "########################\n",
        "#stem single token\n",
        "def stem_token(alltoken):\n",
        "    result = []\n",
        "    for token in alltoken:\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS:\n",
        "            result.append(lemmatize_stemming(token))\n",
        "    return result\n",
        "####################################################################################\n",
        "def Convert_list_of_list_to_list(all_tokens):\n",
        "  tokens = []\n",
        "  for words in all_tokens:\n",
        "    for i in words:\n",
        "      tokens.append(i)\n",
        "  return tokens\n",
        "#########################################################3\n",
        "def pre_processing_dataset(data):\n",
        "  All_tokens = [];\n",
        "  for i in range(len(data)):\n",
        "    All_tokens.append(pre_processing(data.iloc[i][1]))\n",
        "  All_tokens = Convert_list_of_list_to_list(All_tokens)\n",
        "  print('the number of words in Crorpus' , len(All_tokens))\n",
        "\n",
        "  return All_tokens\n",
        "\n",
        "#########################################################################################\n",
        "def Create_embedding_matrix(model , tokens , embedding_dim):\n",
        "    embedding_matrix = []\n",
        "    irregular_words  = []\n",
        "    for words in tokens:\n",
        "          try :\n",
        "            temp = model.wv[words]\n",
        "            embedding_matrix.append(temp)\n",
        "          except KeyError:\n",
        "            # for the words that are not in model vocabulary\n",
        "            # we initilize with random vector\n",
        "            irregular_words.append(words)\n",
        "            #embedding_matrix.append(np.random.normal(0,np.sqrt(0.300),embedding_dim))\n",
        "    return embedding_matrix , irregular_words\n",
        "\n",
        "#################################################################################\n",
        "\n",
        "###########################################################################################\n",
        "# K-means Clustering the embeddings #\n",
        "def embeddings_k_means_clustering(embeddings_vectors, N_clusters):\n",
        "  #kmedoids = KMedoids(n_clusters=5, random_state=0).fit(embeddings_vectors)\n",
        "  #SphericalKMeans\n",
        "  kmeans = KMeans(n_clusters= N_clusters , random_state=0).fit(embeddings_vectors)\n",
        "  return kmeans\n",
        "\n",
        "\n",
        "########################################################################################\n",
        "def spherical_kmeans(n_clusters_, embeddings_matrix,sim_digree):\n",
        "  embeddings_matrix_csr = csr_matrix(embeddings_matrix)\n",
        "  spherical_kmeans = SphericalKMeans( max_similar=sim_digree, init='similar_cut', \n",
        "      n_clusters = n_clusters_)\n",
        "  labels = spherical_kmeans.fit_predict(embeddings_matrix_csr)\n",
        "  centers = spherical_kmeans.cluster_centers_\n",
        "  return labels , centers\n",
        "\n",
        "############################################################################################\n",
        "def dimentionality_reduction(embeddings_vectors):\n",
        "  pca = PCA(3).fit(embeddings_vectors)\n",
        "  pca_data = pd.DataFrame(pca.transform(embeddings_vectors)) \n",
        "  return pca_data\n",
        "\n",
        "#############################################################################################\n",
        "def clusters_visualization( _3D_data , labels):\n",
        "  fig = plt.figure()\n",
        "  ax = fig.add_subplot(111, projection='3d')\n",
        "  ax.scatter(_3D_data[0], _3D_data[1], _3D_data[2], c = labels,  s=50, cmap='viridis')\n",
        "  centers = k_means.cluster_centers_\n",
        "  ax.scatter(centers[0], centers[1], centers[2] ,  c='red', s=200, alpha=0.8);\n",
        "  ax.set_xlabel('X Label')\n",
        "  ax.set_ylabel('Y Label')\n",
        "  ax.set_zlabel('Z Label')\n",
        "  plt.show()\n",
        "#############################################################################################\n",
        "def clusters_properties(k_means):\n",
        "  print(Counter(k_means.labels_))\n",
        "  centers = k_means.cluster_centers_\n",
        "  '''\n",
        "  Return\n",
        "     - Number of samples for each cluter ,\n",
        "     - The Centers of each cluster\n",
        "  '''\n",
        "  return Counter(k_means.labels_) , centers\n",
        "#############################################################################################\n",
        "def Save_words_with_Embeddings(tokens , pre_model):\n",
        "  dic = {}\n",
        "  for words in tokens: \n",
        "      try :\n",
        "        dic.update({words : pre_model.wv[words]})\n",
        "      except: \n",
        "        continue;\n",
        "  return dic\n",
        "##############################################################################################\n",
        "### this function take the vector and return the word that linked with it.\n",
        "def Vector_to_Word(vector , dic):\n",
        "  word = ''\n",
        "  for i in dic:\n",
        "    if functools.reduce(lambda x, y : x and y, map(lambda p, q: p == q,list(dic[i]),vector), True): \n",
        "      word = i\n",
        "  return word\n",
        "##############################################################################################\n",
        "### get list of list array, where each list contain the groub of element the belong to specific cluster\n",
        "### C * n_sample for each cluster\n",
        "\n",
        "# updated\n",
        "def get_cluster_elements(labels , n_clusters , embedding_matrix):\n",
        "  # Nice Pythonic way to get the indices of the points for each corresponding cluster\n",
        "  mydict = {i: np.where(labels == i)[0] for i in range(n_clusters)}\n",
        "  embeddings_samples = []\n",
        "  for i in range(len(mydict)):\n",
        "    temp= []\n",
        "    for j in mydict[i]:\n",
        "      temp.append(embedding_matrix[j])\n",
        "    embeddings_samples.append(temp)\n",
        "    del temp\n",
        "  return embeddings_samples\n",
        "\n",
        "############################################################################################\n",
        "def get_soreted_dictionary(centers , dictionary , M):\n",
        "  dic_with_similarity =[]\n",
        "  for i in range(len(dictionary)):\n",
        "    temp = []\n",
        "    for j in range(len(dictionary[i])):\n",
        "      temp.append([dictionary[i][j] , cosine_similarity([dictionary[i][j]] , [centers[i]])])\n",
        "    dic_with_similarity.append(temp)\n",
        "    del temp\n",
        "      ####### Sort all vectors clusters ############\n",
        "  sorted_list = []\n",
        "  def take_second(elem):\n",
        "      return elem[1]\n",
        "  for i in range(len(dic_with_similarity)):\n",
        "      sorted_list.append(sorted(dic_with_similarity[i], key=take_second , reverse = True))\n",
        "    ########### Get the hightest M features that related to the its centers among whole concepet dictionary#############\n",
        "  Top_M_Concept = [sorted_list[i][:M] for i in range(len(dic_with_similarity))]\n",
        "  return Top_M_Concept\n",
        "\n",
        "############################################################################################\n",
        "\n",
        "#compare each words in documents with all clusters words and return the mean of similarity.\n",
        "\n",
        "def Get_similarity_matrix(wrd_embedding_vector , concept_words_vectors):\n",
        "  cos_similarity = [] # List, whose elements are similarity values between a document word and the cluster words \n",
        "  mean_of_similarity =0  # mean of similarity values in cos_similarity\n",
        "  for c_word in concept_words_vectors:\n",
        "    cos_similarity.append(cosine_distance(wrd_embedding_vector, c_word[0]))\n",
        "  cos_similarity = array(cos_similarity).astype('float').reshape(-1).tolist()\n",
        "  if len(cos_similarity)!=0:\n",
        "    mean_of_similarity= math.mean(cos_similarity)\n",
        "  return mean_of_similarity\n",
        "\n",
        "### convert document to Vector with size C * M \n",
        "##############################################################################################\n",
        "##############################################################################################\n",
        "def Doc2Vec(embeddeings_document,threshold_sim,alpha,centers,dictionary):\n",
        "  M=2\n",
        "  N=5\n",
        "  concept_freq = 0\n",
        "  ## intilize doc array\n",
        "  doc2vec = np.zeros((len(centers), M)).tolist()\n",
        "  final_vector=np.zeros(len(centers)).tolist()\n",
        "  for cnt in range(len(centers)):\n",
        "    counter = 0;\n",
        "    sim =[]\n",
        "    NN=0\n",
        "    for emb_word in embeddeings_document:\n",
        "      Wsim =cosine_distance(emb_word, centers[cnt])# التشابه مع اقرب كلمة للمركز\n",
        "      if Wsim  > alpha and NN<=N:\n",
        "        NN=NN+1\n",
        "        concept_freq = concept_freq+1 #for tf\n",
        "        Smean =Get_similarity_matrix(emb_word , dictionary[cnt][0:threshold_sim-1]);\n",
        "        sim.append(float((Smean+Wsim)/2))\n",
        "        doc2vec[cnt][M-1] = doc2vec[cnt][M-1] + 1;\n",
        "      elif Wsim  > alpha and NN>N:\n",
        "        concept_freq = concept_freq+1 #for tf\n",
        "        sim.append(Wsim)\n",
        "        doc2vec[cnt][M-1] = doc2vec[cnt][M-1] + 1;\n",
        "      else:\n",
        "        continue;\n",
        "      counter = counter +1;\n",
        "    # end for doc words with one concepts\n",
        "    meanofSim=0#  المتوسط لدرجات تشابه كلمات المستند مع المفهوم الحالي\n",
        "    if(len(sim)!=0):\n",
        "      meanofSim=sum(sim)/len(sim)\n",
        "    doc2vec[cnt][M-2]=meanofSim\n",
        "    del sim\n",
        "#* math.exp(i[M-2])\n",
        "  if concept_freq !=0: # the number of occurence for all concept in this document\n",
        "    for i in doc2vec:\n",
        "      #print(i[M-1], concept_freq, 'sim',i[M-2],'exp',math.exp(i[M-2]))\n",
        "      i[M-1] = i[M-1]* math.exp(i[M-2]) / concept_freq\n",
        "      #print(i[M-1])\n",
        "  doc2vec1=np.array(doc2vec,dtype=object)\n",
        "  final_vector=doc2vec1[:,1].tolist() \n",
        "\n",
        "  return final_vector\n",
        "\n",
        "\n",
        "#############################\n",
        "def Doc2VecTFIDF(embeddeings_tfidf,threshold_sim,alpha,centers,dictionary):\n",
        "  M=2\n",
        "  N=10\n",
        "  concept_freq = 0\n",
        "  ## intilize doc array\n",
        "  doc2vec = np.zeros((len(centers), M)).tolist()\n",
        "  final_vector=np.zeros(len(centers)).tolist()\n",
        "  for cnt in range(len(centers)):\n",
        "    counter = 0;\n",
        "    sim =[]\n",
        "    concept_tf_idf=[]\n",
        "    NN=0\n",
        "    for emb_word in embeddeings_tfidf:    \n",
        "      #emb_word هذه تحتوي تضمين ووزن تفايدف    \n",
        "      #emb_word[0] embedding\n",
        "      #emb_word[1] tfidf       \n",
        "      Wsim =cosine_distance(emb_word[0], dictionary[cnt][0][0])# التشابه مع اقرب كلمة للمركز\n",
        "      if Wsim  > alpha and NN<=N:\n",
        "        NN=NN+1\n",
        "        concept_freq = concept_freq+1 #for tf\n",
        "        #Smean =Get_similarity_matrix(emb_word , dictionary[cnt][1:threshold_sim-1]);\n",
        "        #sim.append(float((Smean+Wsim)/2))\n",
        "        #new\n",
        "        concept_tf_idf.append(emb_word[1])\n",
        "        doc2vec[cnt][M-1] = doc2vec[cnt][M-1] + 1;\n",
        "      elif Wsim  > alpha and NN>N:\n",
        "        concept_freq = concept_freq+1 #for tf\n",
        "        #sim.append(Wsim)\n",
        "        concept_tf_idf.append(emb_word[1])\n",
        "        doc2vec[cnt][M-1] = doc2vec[cnt][M-1] + 1;\n",
        "      else:\n",
        "        continue;\n",
        "      counter = counter +1;\n",
        "    # end for doc words with one concepts\n",
        "    #meanofSim=0#  المتوسط لدرجات تشابه كلمات المستند مع المفهوم الحالي\n",
        "    #if(len(sim)!=0):\n",
        "     # meanofSim=sum(sim)/len(sim)\n",
        "    #mean_of_tf_idf=0\n",
        "    if len(concept_tf_idf)>0:\n",
        "      mean_of_tf_idf=sum(concept_tf_idf)/len(concept_tf_idf)\n",
        "    #doc2vec[cnt][M-2]=meanofSim\n",
        "    doc2vec[cnt][M-2] = mean_of_tf_idf\n",
        "    del concept_tf_idf\n",
        "    #del sim\n",
        "#* math.exp(i[M-2])\n",
        "  if concept_freq !=0: # the number of occurence for all concept in this document\n",
        "    for i in doc2vec:\n",
        "      i[M-1] = i[M-2]  # خزن محل تف متوسط اوزان تفات كلمات المفهوم\n",
        "      #i[M-1] = i[M-1] * math.exp(i[M-2]) / concept_freq   #calculte tf\n",
        "\n",
        "      \n",
        "  doc2vec1=np.array(doc2vec,dtype=object)\n",
        "  final_vector=doc2vec1[:,1].tolist() \n",
        "\n",
        "  return final_vector\n",
        "#################################\n",
        "def Doc2Vec2D(embeddeings_document,threshold_sim,alpha,centers,dictionary):\n",
        "  M=2\n",
        "  concept_freq = 0\n",
        "  ## intilize doc array\n",
        "  doc2vec = np.zeros((len(centers), M)).tolist()\n",
        "  final_vector=np.zeros(len(centers)).tolist()\n",
        "  for cnt in range(len(centers)):\n",
        "    counter = 0;\n",
        "    sim =[]\n",
        "    for emb_word in embeddeings_document:\n",
        "      Smean =Get_similarity_matrix(emb_word , dictionary[cnt][0:threshold_sim-1]);\n",
        "      if Smean  > alpha:\n",
        "        concept_freq = concept_freq+1 #for tf\n",
        "        sim.append(float(Smean))\n",
        "        doc2vec[cnt][M-1] = doc2vec[cnt][M-1] + 1;\n",
        "      else:\n",
        "        continue;\n",
        "      counter = counter +1;\n",
        "    # end for doc words with one concepts\n",
        "    meanofSim=0#  المتوسط لدرجات تشابه كلمات المستند مع المفهوم الحالي\n",
        "    if(len(sim)!=0):\n",
        "      meanofSim=sum(sim)/len(sim)\n",
        "    doc2vec[cnt][M-2]=meanofSim\n",
        "    del sim\n",
        "#* math.exp(i[M-2])\n",
        "  if concept_freq !=0: # the number of occurence for all concept in this document\n",
        "    for i in doc2vec:\n",
        "      #print(i[M-1], concept_freq, 'sim',i[M-2],'exp',math.exp(i[M-2]))\n",
        "      i[M-1] = i[M-1] / concept_freq\n",
        "      #print(i[M-1])\n",
        "  doc2vec1=np.array(doc2vec)\n",
        "  final_vector=doc2vec1[:,1].tolist() \n",
        "\n",
        "  return doc2vec\n",
        "  #######################\n",
        "def DDDoc2Vec(embeddeings_document,threshold_sim,alpha,centers,dictionary):\n",
        "  M=2\n",
        "  concept_freq = 0\n",
        "  ## intilize doc array\n",
        "  doc2vec = np.zeros((len(centers), M)).tolist()\n",
        "  final_vector=np.zeros(len(centers)).tolist()\n",
        "  for cnt in range(len(centers)):\n",
        "    counter = 0;\n",
        "    sim =[]\n",
        "    for emb_word in embeddeings_document:\n",
        "      Wsim1 =cosine_similarity([emb_word], [dictionary[cnt][0][0]])\n",
        "      Wsim2 =cosine_similarity([emb_word], [dictionary[cnt][1][0]])\n",
        "      Wsim3 =cosine_similarity([emb_word], [dictionary[cnt][2][0]])\n",
        "      Smean=float((Wsim1+Wsim2+Wsim3)/3)\n",
        "      #print('Smean', Smean)\n",
        "      if Smean  > alpha:\n",
        "        concept_freq = concept_freq+1 #for tf\n",
        "        sim.append(Smean)\n",
        "        doc2vec[cnt][M-1] = doc2vec[cnt][M-1] + 1;\n",
        "      else:\n",
        "        continue;\n",
        "      counter = counter +1;\n",
        "    # end for doc words with one concepts\n",
        "    meanofSim=0#  المتوسط لدرجات تشابه كلمات المستند مع المفهوم الحالي\n",
        "    lsim=len(sim)\n",
        "    if lsim>0:\n",
        "      meanofSim=sum(sim)/lsim\n",
        "    #print('Mean of sum',meanofSim,'sim',len(sim),doc2vec[cnt][M-1])\n",
        "    doc2vec[cnt][M-2]=meanofSim\n",
        "    del sim\n",
        "\n",
        "  if concept_freq !=0: # the number of occurence for all concept in this document\n",
        "    for i in doc2vec:\n",
        "      #print(i[M-1], concept_freq, 'sim',i[M-2],'exp',math.exp(i[M-2]))\n",
        "      i[M-1] = i[M-1]* math.exp(i[M-2]) / concept_freq\n",
        "  doc2vec1=np.array(doc2vec)\n",
        "  final_vector=doc2vec1[:,1].tolist() \n",
        "\n",
        "  return final_vector\n",
        "#####################################################################################\n",
        "\n",
        "\n",
        "\n",
        "#######################################################################################\n",
        "## convert vectors to words for each cluster (M=10 words)\n",
        "# K number of clsuters\n",
        "def dictAsText(K, M, sordted_concepts, dictionary):\n",
        "  textclusters = np.zeros((K, M)).tolist()\n",
        "  for i in range(K):\n",
        "    num_of_words=len(sorted_cons[i][0:M])\n",
        "    for j in range(num_of_words):\n",
        "      textclusters[i][j]=Vector_to_Word(sordted_concepts[i][j][0],dictionary)\n",
        "  return textclusters\n",
        "\n",
        "#######################################\n",
        "# WMD we need to use whole cluster not only 10 words\n",
        "# dictionarAsstring as input to Doc2WMD\n",
        "\n",
        "def Doc2WMD(model,text_document,embeddeings_document , M , alpha , centers , textclusters,dic):\n",
        "  #doc array\n",
        "  doc2WMD_only = np.zeros(len(centers)).tolist()\n",
        "  doc2WMD_with_Concept = np.zeros(( len(centers), M)).tolist()\n",
        "  counter=0;\n",
        "  #start for conepts\n",
        "  for cnt in range(len(centers)):\n",
        "    for emb_word in embeddeings_document:\n",
        "      s=cosine_similarity([emb_word] , [centers[cnt]])\n",
        "      if ((s  > alpha)):\n",
        "        doc2WMD_with_Concept[cnt][M-1] = doc2WMD_with_Concept[cnt][M-1] + 1 #frequency++\n",
        "      else:\n",
        "        continue;\n",
        "    counter=counter+1;\n",
        "    #end for \n",
        "    # get WMD ((words) dictionary[cnt])\n",
        "    doc1=text_document\n",
        "    doc2 = ' '.join(textclusters[cnt])\n",
        "    #print('counter', counter ,'doc1' ,doc1,' \\n doc2', doc2 ,'\\n')\n",
        "    distance = model.wmdistance(doc1.split(), doc2.split())\n",
        "    doc2WMD_with_Concept[cnt][M-2]= distance\n",
        "    doc2WMD_only[cnt]=distance\n",
        "    #empty strings\n",
        "    doc2=\"\"\n",
        "  #claculate tf-idf\n",
        "  get_concept_freq(doc2WMD_with_Concept)\n",
        "  # normalize vector\n",
        "  WMD_only,bysum=normalize_vector(doc2WMD_only)\n",
        "  doc2WMD_with_Concept=normalize_coloumn_in_2Darray(doc2WMD_with_Concept, 0)\n",
        "  return doc2WMD_with_Concept,WMD_only\n",
        "\n",
        "###################################################################\n",
        "def cosine_distance(u, v):\n",
        "  return np.dot(u, v) / (math.sqrt(np.dot(u, u)) * math.sqrt(np.dot(v, v)))\n",
        "\n",
        "#####################################################################################\n",
        "#TF-idf\n",
        "def get_concept_freq(array2D):\n",
        "  s=0;\n",
        "  for row in array2D:            \n",
        "    s=s+row[len(row)-1]\n",
        "  if s !=0:\n",
        "    for row in array2D:\n",
        "      row[len(row)-1]=row[len(row)-1]/s\n",
        "  return array2D\n",
        "#########################################33\n",
        "def normalize_vector(vect):\n",
        "  bymax=[float(i)/max(vect) for i in vect]\n",
        "  bysum=[float(i)/sum(vect) for i in vect]\n",
        "  return bymax,bysum\n",
        "\n",
        "#####################################################\n",
        "def normalize_coloumn_in_2Darray(array2D, index_column):\n",
        "  arr=np.array(array2D)\n",
        "  list_col=arr[:,index_column]\n",
        "  s=max(list_col) # or sum\n",
        "  #print(' col ', list_col , ' max ' , s)\n",
        "  if s !=0:\n",
        "    for row in array2D:\n",
        "      row[index_column]=float(row[index_column])/s\n",
        "  return array2D\n",
        "\n",
        "\n",
        "\n",
        "####################################################33333\n",
        "def embedding_all_documents(df , model,N):\n",
        "  documents_embeddings = []\n",
        "  for i in range(len(df)):\n",
        "    documents_embeddings.append(convert_doc_to_embedding(df['document'][i] , model,N))\n",
        "  return documents_embeddings\n",
        "\n",
        "########################################################################################################\n",
        "def convert_doc_to_embedding(tokens , model,N):\n",
        "  #tokens = pre_processing(text)\n",
        "  if N!=0:\n",
        "    tokens=removeElements(tokens,N)\n",
        "  embeddings_matrix  , irrugular_words = Create_embedding_matrix(model , tokens , 300)\n",
        "  return embeddings_matrix\n",
        "#########################################################################################################\n",
        "def RepresentationNew(documents_embeddings , sim_val , alpha , centers , dictionary):\n",
        "  embeddings = []\n",
        "  for i in documents_embeddings: \n",
        "    v = Doc2Vec(i ,sim_val , alpha , centers , dictionary)\n",
        "    embeddings.append(v)\n",
        "  return embeddings\n",
        "##########################################\n",
        "\n",
        "def calculate_IDF(doc_vectors):\n",
        "  docL=len(doc_vectors[0])\n",
        "  N=len(doc_vectors) #collection size\n",
        "  IDf = np.zeros(docL).tolist() # بطول العناقيد\n",
        "  for doc in doc_vectors:\n",
        "    counter=0 # counting concepts\n",
        "    for concept_freature in doc:\n",
        "      if concept_freature > 0:\n",
        "        IDf[counter] = IDf[counter] + 1 #  i[M-1]\n",
        "      counter = counter+1\n",
        "  \n",
        "  #ضرب حساب التردد\n",
        "  newIDF=[]\n",
        "  for i in IDf:\n",
        "    if i!=0:\n",
        "      newIDF.append( math.exp(-1*(i/N)))\n",
        "    else:\n",
        "      newIDF.append(0)\n",
        "  return IDf, newIDF\n",
        "\n",
        "########################################### \n",
        "#Calculate Cf-IDF\n",
        "def CF_IDF(doc_vectors,IDF):\n",
        "  docL=len(doc_vectors[0])\n",
        "  for i in range(len(doc_vectors)):\n",
        "    for j in range(docL):\n",
        "       doc_vectors[i][j] = doc_vectors[i][j] * IDF[j]\n",
        "  return doc_vectors\n",
        "\n",
        "#################################################\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "########################333\n",
        "def IDFbySimNoLog(documents_embeddings , sim_val , alpha , centers , dictionary):\n",
        "  embeddings = []\n",
        "  counter = 0\n",
        "  IDf = np.zeros(len(centers)).tolist() # بطول العناقيد\n",
        "  for i in documents_embeddings: \n",
        "    v = Doc2Vec2D(i ,sim_val , alpha , centers , dictionary)\n",
        "    ######## حساب عدد مرات ظهور المفهوم في كل نص\n",
        "    for i in v:   # i is concept line , i need class of doc v\n",
        "      if i > 0:\n",
        "        IDf[counter] = IDf[counter] + 1 #  i[M-1]\n",
        "      counter = counter+1\n",
        "    counter = 0\n",
        "    embeddings.append(v)\n",
        "  print(IDf)\n",
        "  ######## Calculate Cf-IDF\n",
        "  print(len(embeddings))\n",
        "  for i in range(len(embeddings)):\n",
        "    l=len(embeddings[i])\n",
        "    for j in range(l):\n",
        "    #for doc i: muti cell j with idf j \n",
        "       #print('i', i,' j ', j, ' = ', embeddings[i][j] * newIDF[j])\n",
        "       embeddings[i][j] = embeddings[i][j] * IDf[j]\n",
        "  \n",
        "  return embeddings\n",
        "#########################################################################################################\n",
        "\n",
        "def RepresentationNormNoLog(documents_embeddings , sim_val , alpha , centers , dictionary):\n",
        "  embeddings = []\n",
        "  counter = 0\n",
        "  IDf = np.zeros(len(centers)).tolist() # بطول العناقيد\n",
        "  for i in documents_embeddings: \n",
        "    v = Doc2Vec(i ,sim_val , alpha , centers , dictionary)\n",
        "    ######## حساب عدد مرات ظهور المفهوم في كل نص\n",
        "    for i in v:   # i is concept line , i need class of doc v\n",
        "      if i > 0:\n",
        "        IDf[counter] = IDf[counter] + 1 #  i[M-1]\n",
        "      counter = counter+1\n",
        "    counter = 0\n",
        "    embeddings.append(v)\n",
        "  print(IDf)\n",
        "  ######## Calculate Cf-IDF\n",
        "  print(len(embeddings))\n",
        "  for i in range(len(embeddings)):\n",
        "    l=len(embeddings[i])\n",
        "    for j in range(l):\n",
        "    #for doc i: muti cell j with idf j \n",
        "       #print('i', i,' j ', j, ' = ', embeddings[i][j] * newIDF[j])\n",
        "       embeddings[i][j] = embeddings[i][j] * IDf[j]\n",
        "  \n",
        "  return embeddings\n",
        "###############\n",
        "\n",
        "  ##################################3\n",
        "\n",
        "def RepresentationAccNoLog(documents_embeddings , sim_val , alpha , centers , dictionary):\n",
        "  embeddings = []\n",
        "  counter = 0\n",
        "  IDf = np.zeros(len(centers)).tolist() # بطول العناقيد\n",
        "  for i in documents_embeddings: \n",
        "    v = Doc2Vec(i ,sim_val , alpha , centers , dictionary)\n",
        "    ######## حساب عدد مرات ظهور المفهوم في كل نص\n",
        "    for i in v:   # i is concept line , i need class of doc v\n",
        "      if i > 0:\n",
        "        IDf[counter] = IDf[counter] + i #  i[M-1]\n",
        "      counter = counter+1\n",
        "    counter = 0\n",
        "    embeddings.append(v)\n",
        "  \n",
        "  ######## Calculate Cf-IDF\n",
        "  print(len(embeddings))\n",
        "  for i in range(len(embeddings)):\n",
        "    l=len(embeddings[i])\n",
        "    for j in range(l):\n",
        "    #for doc i: muti cell j with idf j \n",
        "       #print('i', i,' j ', j, ' = ', embeddings[i][j] * newIDF[j])\n",
        "       embeddings[i][j] = embeddings[i][j] * IDf[j]\n",
        "  \n",
        "  return embeddings\n",
        "###########################################\n",
        "\n",
        "def RepresentationAccLog(documents_embeddings , sim_val , alpha , centers , dictionary):\n",
        "  embeddings = []\n",
        "  counter = 0\n",
        "  IDf = np.zeros(len(centers)).tolist() # بطول العناقيد\n",
        "  for i in documents_embeddings: \n",
        "    v = Doc2Vec(i ,sim_val , alpha , centers , dictionary)\n",
        "    ######## حساب عدد مرات ظهور المفهوم في كل نص\n",
        "    for i in v:   # i is concept line , i need class of doc v\n",
        "      if i > 0:\n",
        "        IDf[counter] = IDf[counter] + i #  i[M-1]\n",
        "      counter = counter+1\n",
        "    counter = 0\n",
        "    embeddings.append(v)\n",
        "  #print('oldidf',IDf,'\\n')\n",
        "  #########  calculate IDF\n",
        "  #newIDF=[]\n",
        "  for i in IDf:\n",
        "    if i!=0:\n",
        "      N=len(embeddings)\n",
        "      newIDF.append( math.log10( (N+1) /i ) )\n",
        "    else:\n",
        "      newIDF.append(0)\n",
        "  #print('newidf',len(newIDF),'\\n')\n",
        "  \n",
        "  ######## Calculate Cf-IDF\n",
        "  print(len(embeddings))\n",
        "  for i in range(len(embeddings)):\n",
        "    l=len(embeddings[i])\n",
        "    for j in range(l):\n",
        "    #for doc i: muti cell j with idf j \n",
        "       #print('i', i,' j ', j, ' = ', embeddings[i][j] * newIDF[j])\n",
        "       embeddings[i][j] = embeddings[i][j] * newIDF[j]\n",
        "  \n",
        "  return embeddings\n",
        "\n",
        "  #######################################\n",
        "def saveList(myList,filename):\n",
        "    # the filename should mention the extension 'npy'\n",
        "    np.save(filename,myList)\n",
        "    print(\"Saved successfully!\")\n",
        "def loadList(filename):\n",
        "    # the filename should mention the extension 'npy'\n",
        "    tempNumpyArray=np.load(filename)\n",
        "    return tempNumpyArray.tolist()\n",
        "\n",
        "###############################################\n",
        "#######################################################\n",
        "# number of doc in each class \n",
        "def DataBalance(data):\n",
        "  data['class'] = pd.factorize(data['class'])[0]\n",
        "  uniqueValues = data['class'].unique()\n",
        "  l=len(uniqueValues)\n",
        "  labels_freq = np.zeros(l).tolist()\n",
        "  for i in range(len(data)):\n",
        "    label= data.iloc[i][0]\n",
        "    labels_freq[label]=labels_freq[label]+1\n",
        "  return labels_freq \n",
        "\n",
        "#BBC_data['vector']=BBC_data['text'].apply(BoET,args=(model,centers , sorted_cons,0.44,2,10))\n",
        "\n",
        "\n",
        "###############################\n",
        "def document_class_of_concept(orginalDF,data_DF, doc_vectors): \n",
        "  #number of docs in each class\n",
        "  doc_in_class=DataBalance(orginalDF)\n",
        "  # number of classes\n",
        "  L_labels=len(doc_in_class)\n",
        "  K_clusters=len(doc_vectors[0])\n",
        "  #array for freq  \n",
        "  freq_array = np.zeros((L_labels, K_clusters)).tolist() \n",
        "  i=0 # doc counter\n",
        "  for d in doc_vectors:\n",
        "    curren_class=data_DF['class'][i]\n",
        "    counter=0 #idf counter within doc\n",
        "    for concept_feature in d:\n",
        "      if concept_feature>0:\n",
        "        freq_array[curren_class][counter]=freq_array[curren_class][counter]+1\n",
        "      counter=counter+1\n",
        "    i=i+1\n",
        "  arr=np.array(freq_array)\n",
        "  #نسبة عدد مستندات مفهوم ما من فئة ما على عدد مستندات الفئة\n",
        "  for i in range(L_labels):\n",
        "    arr[i][:]= arr[i][:]/doc_in_class[i]\n",
        "  return arr\n",
        "####################################\n",
        "def class_weighting(saved_embaddings,data_DF,freqarray):\n",
        "  lv=len(saved_embaddings)\n",
        "  K_clusters=len(saved_embaddings[0])\n",
        "  for i in range(lv):\n",
        "    curren_class=data_DF['class'][i]\n",
        "    for j in range(K_clusters):\n",
        "      saved_embaddings[i][j] = float(saved_embaddings[i][j])*freqarray[curren_class][j]\n",
        "  return saved_embaddings\n",
        "\n",
        "############################################\n",
        "from collections import Counter \n",
        "def removeElements(lst, k): \n",
        "    counted = Counter(lst) \n",
        "    temp_lst = [] \n",
        "    for el in counted: \n",
        "        if counted[el] < k: \n",
        "            temp_lst.append(el) \n",
        "    res_lst = [] \n",
        "    for el in lst: \n",
        "        if el not in temp_lst: \n",
        "            res_lst.append(el) \n",
        "              \n",
        "    return(res_lst)\n",
        "\n",
        "######################\n",
        "from collections import Counter\n",
        "\n",
        "def rare_words(text):\n",
        "\t\"\"\"\n",
        "\tReturn :- Most Rare words\n",
        "\tInput :- string\n",
        "\tOutput :- list of rare words\n",
        "\t\"\"\"\n",
        "\t# tokenization\n",
        "\ttokens = word_tokenize(text)\n",
        "\tfor word in tokens:\n",
        "\t\tcounter[word]= +1\n",
        "\n",
        "\tRareWords = []\n",
        "\tnumber_rare_words = 10\n",
        "\t# take top 10 frequent words\n",
        "\tfrequentWords = counter.most_common()\n",
        "\tfor (word, word_count) in frequentWords[:-number_rare_words:-1]:\n",
        "\t\tRareWords.append(word)\n",
        "\n",
        "\treturn RareWords\n",
        "\n",
        "###****************\n",
        "def remove_rw(text, RareWords):\n",
        "\t\"\"\"\n",
        "\tReturn :- String after removing frequent words\n",
        "\tInput :- String\n",
        "\tOutput :- String\n",
        "\t\"\"\"\n",
        "\n",
        "\ttokens = word_tokenize(text)\n",
        "\twithout_rw = []\n",
        "\tfor word in tokens:\n",
        "\t\tif word not in RareWords:\n",
        "\t\t\twithout_rw.append(word)\n",
        "\n",
        "\twithout_rw = ' '.join(without_rw)\n",
        "\treturn without_rw\n",
        "### expample\n",
        "###\n",
        "##########################################33\n",
        "#delet last weght\n",
        "def deletFeatures(Feature_index,saved_embaddings):\n",
        "  for d in saved_embaddings:\n",
        "    del d[Feature_index-1]\n",
        "  return saved_embaddings\n",
        "\n",
        "############################################################\n",
        "#تابع لحساب تردد المستند العكسي بناء على بيانات التدريب والاختبار المقسومة\n",
        "def RepresentationGNAG(train_docs,test_docs , sim_val , alpha , centers , dictionary):\n",
        "  train_embeddings = []\n",
        "  test_embeddings = []\n",
        "\n",
        "  counter = 0\n",
        "  IDf = np.zeros(len(centers)).tolist() # بطول العناقيد\n",
        "#idf from train docs  \n",
        "  for i in train_docs: \n",
        "    v = Doc2Vec(i ,sim_val , alpha , centers , dictionary)\n",
        "    ######## حساب عدد مرات ظهور المفهوم في كل نص\n",
        "    for i in v:   # i is concept line , i need class of doc v\n",
        "      if i > 0:\n",
        "        IDf[counter] = IDf[counter] + 1 #  i[M-1]\n",
        "      counter = counter+1\n",
        "    counter = 0\n",
        "    train_embeddings.append(v)\n",
        "#idf from test docs \n",
        "  for i in test_docs: \n",
        "    v = Doc2Vec(i ,sim_val , alpha , centers , dictionary)\n",
        "    ######## حساب عدد مرات ظهور المفهوم في كل نص\n",
        "    for i in v:   # i is concept line , i need class of doc v\n",
        "      if i > 0:\n",
        "        IDf[counter] = IDf[counter] + 1 #  i[M-1]\n",
        "      counter = counter+1\n",
        "    counter = 0\n",
        "    test_embeddings.append(v)\n",
        "\n",
        "  print(IDf)\n",
        "  ######## Calculate Cf-IDF\n",
        "  for i in range(len(train_embeddings)):\n",
        "    l=len(train_embeddings[i])\n",
        "    for j in range(l):\n",
        "    #for doc i: muti cell j with idf j \n",
        "       #print('i', i,' j ', j, ' = ', embeddings[i][j] * newIDF[j])\n",
        "       train_embeddings[i][j] = train_embeddings[i][j] * IDf[j]\n",
        "\n",
        "  for i in range(len(test_embeddings)):\n",
        "    l=len(test_embeddings[i])\n",
        "    for j in range(l):\n",
        "       test_embeddings[i][j] = test_embeddings[i][j] * IDf[j]\n",
        "  return train_embeddings,test_embeddings\n",
        "###############\n",
        "################################################################################\n",
        "#thershold for words in document\n",
        "def delete_empty_docs(documents_embeddings,labels,thershold):\n",
        "  doc_indexes=[]\n",
        "  new_emb=[]\n",
        "  new_label=[]\n",
        "  j=0\n",
        "  #find empty docs\n",
        "  for d in documents_embeddings:\n",
        "    l=len(d)\n",
        "    if l>thershold:\n",
        "      new_label.append(labels[j])\n",
        "      new_emb.append(documents_embeddings[j])\n",
        "    else:\n",
        "      doc_indexes.append(j)\n",
        "    j=j+1\n",
        "  print(doc_indexes)\n",
        "  return new_emb,new_label\n",
        "#################################\n",
        "#################################\n",
        "#used for self embeddings \n",
        "def make_documents_as_list(df):\n",
        "  documents_embeddings = []\n",
        "  for i in range(len(df)):\n",
        "    documents_embeddings.append(final_preprocess_text(df['document'][i]))\n",
        "  return documents_embeddings\n",
        "\n",
        "def documents_as_list_of_sentences(df):\n",
        "  documents_embeddings = []\n",
        "  for i in range(len(df)):\n",
        "    documents_embeddings.append(pre_processing_no_toknize(df['document'][i]))\n",
        "  return documents_embeddings\n",
        "\n",
        "\n",
        "#######################################\n",
        "def word_count(documents_embeddings):\n",
        "  word_count=[]\n",
        "  #find empty docs\n",
        "  for d in documents_embeddings:\n",
        "    l=len(d)\n",
        "    word_count.append(l)\n",
        "    \n",
        "  return word_count\n",
        "##########################################\n",
        "def document_biggerThan(data_DF, doc_vectors,th): \n",
        "  #number of docs in each class\n",
        "  doc_in_class=DataBalance(data_DF)\n",
        "  # number of classes\n",
        "  L_labels=len(doc_in_class)\n",
        "  K_clusters=len(doc_vectors[0])\n",
        "  #array for freq  \n",
        "  freq_array = np.zeros(L_labels).tolist()\n",
        "  mean_array = np.zeros(L_labels).tolist()  \n",
        "  i=0 # doc counter\n",
        "  for d in doc_vectors:\n",
        "    curren_class=data_DF['class'][i]\n",
        "    docL= len(d)\n",
        "    if docL>th:\n",
        "      freq_array[curren_class]=freq_array[curren_class]+1\n",
        "    i=i+1\n",
        "  #متوسط اطوال مستندات كل فئة=عدد مستنداتها الاكبر من حد معين على عدد مستندات الفئة\n",
        "  mean_array = [i / j for i, j in zip(freq_array, doc_in_class)] \n",
        "\n",
        "  return freq_array,mean_array\n",
        "\n",
        "#####################################################\n",
        "def clac_new_Tf_idf(to_delete,tf_similarity,idfcount,idfExp):\n",
        "  i=0\n",
        "  j=0\n",
        "  newIDFexp=[]\n",
        "  newIDFcount=[]\n",
        "  newTf_similarity=[]\n",
        "  num_of_concepts=len(idfexp)\n",
        "\n",
        "  \n",
        "  for d in tf_similarity:\n",
        "    tempD=[]\n",
        "    for j in range(num_of_concepts):\n",
        "      if j not in to_delete:\n",
        "        tempD.append(d[j])\n",
        "    newTf_similarity.append(tempD)\n",
        "    del tempD\n",
        "  ###\n",
        "  for i in range(num_of_concepts):\n",
        "    if i not in todelete:\n",
        "      newIDFexp.append(idfexp[i])\n",
        "      newIDFcount.append(idfcount[i])\n",
        "  return newTf_similarity,newIDFexp,newIDFcount\n",
        "\n",
        "\n",
        "##############################################################\n",
        "def concept_to_delete(min_idf,max_idf,idfcount):\n",
        "  to_delete_List=[]\n",
        "  j=0\n",
        "  for i in idfcount: #30\n",
        "    if (i > max_idf) or (i < min_idf):\n",
        "      to_delete_List.append(j)\n",
        "    j+=1\n",
        "  return to_delete_List\n",
        "\n",
        "######################################################\n",
        "def delete_NaN_docs(documents_embeddings,labels):\n",
        "  doc_indexes=[]\n",
        "  new_emb=[]\n",
        "  new_label=[]\n",
        "  j=0\n",
        "  #find empty docs\n",
        "  for d in documents_embeddings:\n",
        "    if math.isnan(d[0])==False:\n",
        "      new_label.append(labels[j])\n",
        "      new_emb.append(documents_embeddings[j])\n",
        "    else:\n",
        "      doc_indexes.append(j)\n",
        "    j=j+1\n",
        "  print(doc_indexes)\n",
        "  return new_emb,new_label\n",
        "\n",
        "###################################3\n",
        "def docs_word_count(data_DF):\n",
        "  word_count=[]\n",
        "  #find empty docs\n",
        "  for d in data_DF['document']:\n",
        "    l=len(d)\n",
        "    word_count.append(l)\n",
        "    \n",
        "  return word_count\n",
        "\n",
        "#############################################\n",
        "def data_details(documents_embeddings,data_DF,th):\n",
        "  fr,men=document_biggerThan(data_DF,documents_embeddings,th)\n",
        "  no_docs = len(data_DF)\n",
        "  doc_per_class = DataBalance(data_DF)\n",
        "  wc=word_count(documents_embeddings) #embedded word counts\n",
        "  o_wc= docs_word_count(data_DF) # orginal word count\n",
        "\n",
        "  print('№ of all doc=',len(data_DF),\"\\n\")\n",
        "  print(\"Befor embeddings: \\n\",\n",
        "        'the mean length of docs', sum(o_wc)/len(o_wc), \"\\n\",\n",
        "        \"The max length of docs\", max(o_wc),\"\\n\",\n",
        "        \"The min length of docs\", min(o_wc),\"\\n\", \n",
        "        '№ classes ', ' =', len(doc_per_class),\"\\n\",\n",
        "        \"№ docs per class \", doc_per_class )\n",
        "  print(\"After embeddings: \\n\",\n",
        "        \"the mean length of embeded docs\", sum(wc)/len(wc), \"\\n\",\n",
        "        \"The max length of embeded docs\", max(wc),\"\\n\",\n",
        "        \"The min length of embeded docs\", min(wc),\"\\n\", \n",
        "        \"№ Documents with length > \",th, \"=\" ,sum(fr))\n",
        "  \n",
        "\n",
        "##################################################################333\n",
        "#خاص بحقيبة الكلمات الاصلية\n",
        "def BOC(embeddeings_document,threshold_sim,alpha,centers,dictionary):\n",
        "  M=2\n",
        "  concept_freq = 0\n",
        "  ## intilize doc array\n",
        "  doc2vec = np.zeros(len(centers)).tolist()\n",
        "\n",
        "  for cnt in range(len(centers)):\n",
        "    counter = 0;\n",
        "    for emb_word in embeddeings_document:\n",
        "      Wsim= cosine_distance(emb_word , centers[cnt])\n",
        "      if Wsim  > alpha:\n",
        "        concept_freq = concept_freq+1 #for tf\n",
        "        doc2vec[cnt] = doc2vec[cnt] + 1;\n",
        "      else:\n",
        "        continue;\n",
        "      counter = counter +1;\n",
        "    # end for doc words with one concepts\n",
        "  if concept_freq !=0: # the number of occurence for all concept in this document\n",
        "    for i in doc2vec:\n",
        "      i = i / concept_freq  \n",
        "  return doc2vec\n",
        "#####################################\n",
        "def CF_IDF_BOC(documents_embeddings , sim_val , alpha , centers , dictionary):\n",
        "  embeddings = []\n",
        "  counter = 0\n",
        "  IDf = np.zeros(len(centers)).tolist() # بطول العناقيد\n",
        "  for i in documents_embeddings: \n",
        "    bag_of_concepts = BOC(i ,sim_val , alpha , centers , dictionary)\n",
        "    ######## حساب عدد مرات ظهور المفهوم في كل نص\n",
        "    for i in bag_of_concepts:   # i is concept line , i need class of doc v\n",
        "      if i > 0:\n",
        "        IDf[counter] = IDf[counter] + 1 #  i[M-1]\n",
        "      counter = counter+1\n",
        "    counter = 0\n",
        "    embeddings.append(bag_of_concepts)\n",
        "  #########  calculate IDF\n",
        "  newIDF=[]\n",
        "  for i in IDf:\n",
        "    if i!=0:\n",
        "      N=len(embeddings)\n",
        "      newIDF.append( math.log10( (N+1) /i ) )\n",
        "    else:\n",
        "      newIDF.append(0)  \n",
        "  ######## Calculate Cf-IDF\n",
        "  print(len(embeddings))\n",
        "  for i in range(len(embeddings)):\n",
        "    l=len(embeddings[i])\n",
        "    for j in range(l):\n",
        "    #for doc i: muti cell j with idf j \n",
        "       #print('i', i,' j ', j, ' = ', embeddings[i][j] * newIDF[j])\n",
        "       embeddings[i][j] = embeddings[i][j] * newIDF[j]\n",
        "  \n",
        "  return embeddings\n",
        "\n",
        "\n",
        "  #############################################33\n",
        "def text_file_to_list(train_path,test_path):\n",
        "  labels=[]\n",
        "  docs=[]\n",
        "  ###fill train\n",
        "  with open(train_path) as f:\n",
        "    reader = csv.reader(f, delimiter=\"\\t\")\n",
        "    d = list(reader)\n",
        "  for line in d:\n",
        "    docs.append(line[1])\n",
        "    labels.append(line[0])\n",
        "  ####test\n",
        "  with open(test_path) as f:\n",
        "    reader = csv.reader(f, delimiter=\"\\t\")\n",
        "    d = list(reader)\n",
        "  for line in d:\n",
        "    docs.append(line[1])\n",
        "    labels.append(line[0])\n",
        "  return docs,labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.0.1)\n",
            "Collecting spherecluster\n",
            "  Downloading spherecluster-0.1.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from spherecluster) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from spherecluster) (0.22.2.post1)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from spherecluster) (3.6.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from spherecluster) (1.19.5)\n",
            "Collecting nose\n",
            "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[K     |████████████████████████████████| 154 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->spherecluster) (1.0.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spherecluster) (1.15.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spherecluster) (1.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pytest->spherecluster) (57.4.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spherecluster) (21.2.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spherecluster) (8.10.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->spherecluster) (0.7.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->spherecluster) (1.4.0)\n",
            "Installing collected packages: nose, spherecluster\n",
            "Successfully installed nose-1.3.7 spherecluster-0.1.7\n",
            "Collecting soyclustering\n",
            "  Downloading soyclustering-0.2.0-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: numpy>=1.1 in /usr/local/lib/python3.7/dist-packages (from soyclustering) (1.19.5)\n",
            "Installing collected packages: soyclustering\n",
            "Successfully installed soyclustering-0.2.0\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1YTMPZMVZ_N"
      },
      "source": [
        "# **TFIDF functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sH27_YTbVWSF"
      },
      "source": [
        "###################################\n",
        "## for tfidf vectors\n",
        "def Self_get_tokened_docs(data_DF):\n",
        "  doclist= []\n",
        "  for d in  data_DF['document']:\n",
        "    doclist.append(d)\n",
        "\n",
        "  tokened_docs=[]\n",
        "  for d in doclist:\n",
        "    doc_as_token = final_preprocess_text(d)\n",
        "    doc_as_text = ' '.join(doc_as_token)\n",
        "    tokened_docs.append(doc_as_text)\n",
        "  return tokened_docs\n",
        "\n",
        "############################\n",
        "###################################\n",
        "## for tfidf vectors\n",
        "def get_tokened_docs(data_DF):\n",
        "  doclist= []\n",
        "  for d in  data_DF['document']:\n",
        "    doclist.append(d)\n",
        "\n",
        "  tokened_docs=[]\n",
        "  for d in doclist:\n",
        "    doc_as_token = pre_processing(d)\n",
        "    doc_as_text = ' '.join(doc_as_token)\n",
        "    tokened_docs.append(doc_as_text)\n",
        "  return tokened_docs\n",
        "\n",
        "####################################################33333\n",
        "def embedding_tfidf_all_documents(tokened_docs, model, doc_tfidf):\n",
        "  documents_emb_tfidfs = []\n",
        "  for i in range(len(tokened_docs)):\n",
        "    doc_tokens= pre_processing(tokened_docs[i])\n",
        "    emb_tfidf_tuple = Create_embeddingTFIDF_matrix(  i, doc_tokens, model  , 300, doc_tfidf)\n",
        "    documents_emb_tfidfs.append(emb_tfidf_tuple)\n",
        "  return documents_emb_tfidfs\n",
        "##################################################\n",
        "\n",
        "###############\n",
        "def Create_embeddingTFIDF_matrix(doc_id, doc_tokens  , model ,  embedding_dim, doc_tfidf):\n",
        "    emb_tfidf =[]\n",
        "    tfidf=0\n",
        "    for i, word in enumerate(doc_tokens):\n",
        "      try:\n",
        "        temp = model.wv[word]\n",
        "        try :  \n",
        "          tfidf= doc_tfidf[doc_id][word]\n",
        "          tup = (tfidf,temp)\n",
        "          emb_tfidf.append(tup)\n",
        "        except:\n",
        "          # for the words that are not in model vocabulary\n",
        "          tfidf = 0\n",
        "          tup = (tfidf,temp)\n",
        "          emb_tfidf.append(tup)\n",
        "      except:\n",
        "        tfidf = 0\n",
        "    return emb_tfidf\n",
        "\n",
        "############################\n",
        "def RepresentationEMTFIDF(docs_emb_tfidfs ,sim_val , alpha , centers , dictionary):\n",
        "  embeddings = []\n",
        "  for doc in docs_emb_tfidfs: \n",
        "    v = Doc2VecTFIDF(doc ,sim_val , alpha , centers , dictionary)\n",
        "    embeddings.append(v)\n",
        "  return embeddings\n",
        "\n",
        "############################################\n",
        "def Doc2VecTFIDF(doc_embed_tfidf,threshold_sim,alpha,centers,dictionary):\n",
        "  M=2\n",
        "  N=5\n",
        "  concept_freq = 0\n",
        "  ## intilize doc array\n",
        "  doc2vec = np.zeros((len(centers), M)).tolist()\n",
        "  final_vector=np.zeros(len(centers)).tolist()\n",
        "  for cnt in range(len(centers)):\n",
        "    counter = 0;\n",
        "    sim =[]\n",
        "    concept_tf_idf=[]\n",
        "    NN=0\n",
        "    for emb_word in doc_embed_tfidf:    \n",
        "      #emb_word هذه تحتوي تضمين ووزن تفايدف    \n",
        "      #emb_word[0]  tfidf\n",
        "      #emb_word[1] embedding       \n",
        "      Wsim =cosine_distance(emb_word[1], centers[cnt])# التشابه مع اقرب كلمة للمركز\n",
        "      if Wsim  > alpha and NN<=N:\n",
        "        NN=NN+1\n",
        "        concept_freq = concept_freq+1 #for tf\n",
        "        #Smean =Get_similarity_matrix(emb_word , dictionary[cnt][1:threshold_sim-1]);\n",
        "        #sim.append(float((Smean+Wsim)/2))\n",
        "        #new\n",
        "        #print(\"emb\", len(emb_word[1]), \"tf\", emb_word[0])\n",
        "        concept_tf_idf.append(emb_word[0])\n",
        "        doc2vec[cnt][M-1] = doc2vec[cnt][M-1] + 1;\n",
        "      elif Wsim  > alpha and NN>N:\n",
        "        concept_freq = concept_freq+1 #for tf\n",
        "        #sim.append(Wsim)\n",
        "        concept_tf_idf.append(emb_word[0])\n",
        "        doc2vec[cnt][M-1] = doc2vec[cnt][M-1] + 1;\n",
        "      else:\n",
        "        continue;\n",
        "      counter = counter +1;\n",
        "    # end for doc words with one concepts\n",
        "    #meanofSim=0#  المتوسط لدرجات تشابه كلمات المستند مع المفهوم الحالي\n",
        "    #if(len(sim)!=0):\n",
        "     # meanofSim=sum(sim)/len(sim)\n",
        "    mean_of_tf_idf=0\n",
        "    maxtfIDF= 0\n",
        "    if len(concept_tf_idf)>0:\n",
        "      maxtfIDF = max(concept_tf_idf)\n",
        "      mean_of_tf_idf=sum(concept_tf_idf)/len(concept_tf_idf)\n",
        "    #doc2vec[cnt][M-2]=meanofSim\n",
        "    doc2vec[cnt][M-2] = mean_of_tf_idf\n",
        "    del concept_tf_idf\n",
        "    #del sim\n",
        "\n",
        "# the number of occurence for all concept in this document\n",
        "  if concept_freq !=0: \n",
        "    for i in doc2vec:\n",
        "      i[M-1] =  i[M-2]  # خزن محل تف متوسط اوزان تفات كلمات المفهوم\n",
        "\n",
        "      \n",
        "  doc2vec1=np.array(doc2vec,dtype=object)\n",
        "  final_vector=doc2vec1[:,1].tolist() \n",
        "\n",
        "  return final_vector\n",
        "\n",
        "############################################\n",
        "def Doc2VecTFIDFSim(doc_embed_tfidf,threshold_sim,alpha,centers,dictionary):\n",
        "  M=2\n",
        "  N=10\n",
        "  concept_freq = 0\n",
        "  ## intilize doc array\n",
        "  doc2vec = np.zeros((len(centers), M)).tolist()\n",
        "  final_vector=np.zeros(len(centers)).tolist()\n",
        "  for cnt in range(len(centers)):\n",
        "    counter = 0;\n",
        "    #sim =[]\n",
        "    concept_tf_idf=[]\n",
        "    NN=0\n",
        "    for emb_word in doc_embed_tfidf:    \n",
        "      #emb_word هذه تحتوي تضمين ووزن تفايدف    \n",
        "      #emb_word[0]  tfidf\n",
        "      #emb_word[1] embedding       \n",
        "      Wsim =cosine_distance(emb_word[1], dictionary[cnt][0][0])# التشابه مع اقرب كلمة للمركز\n",
        "      if Wsim  > alpha and NN<=N:\n",
        "        NN=NN+1\n",
        "        concept_freq = concept_freq+1 #for tf\n",
        "        #Smean =Get_similarity_matrix(emb_word[1] , dictionary[cnt][1:threshold_sim-1]);\n",
        "        #sim.append(float((Smean+Wsim)/2))\n",
        "        #new\n",
        "        concept_tf_idf.append(emb_word[0])\n",
        "        doc2vec[cnt][M-1] = doc2vec[cnt][M-1] + 1;\n",
        "      elif Wsim  > alpha and NN>N:\n",
        "        concept_freq = concept_freq+1 #for tf\n",
        "        #sim.append(Wsim)\n",
        "        concept_tf_idf.append(emb_word[0])\n",
        "        doc2vec[cnt][M-1] = doc2vec[cnt][M-1] + 1;  # cell 2\n",
        "      else:\n",
        "        continue;\n",
        "      counter = counter +1;\n",
        "    # end for doc words with one concepts\n",
        "    meanofSim=0          #  المتوسط لدرجات تشابه كلمات المستند مع المفهوم الحالي\n",
        "    if(len(sim)!=0):\n",
        "      meanofSim=sum(sim)/len(sim)\n",
        "    \n",
        "    mean_of_tf_idf=0\n",
        "    if len(concept_tf_idf)>0:\n",
        "      mean_of_tf_idf=sum(concept_tf_idf)/len(concept_tf_idf)\n",
        "    \n",
        "    doc2vec[cnt][M-2]=meanofSim   # cell 1\n",
        "    doc2vec[cnt][M-3] = mean_of_tf_idf # cell 0\n",
        "    del concept_tf_idf\n",
        "    #del sim\n",
        "\n",
        "# the number of occurence for all concept in this document\n",
        "  if concept_freq !=0: \n",
        "    for i in doc2vec:\n",
        "        i[M-1] = i[M-1] *i[M-2] / concept_freq   #calculte tf\n",
        "\n",
        "\n",
        "      \n",
        "  doc2vec1=np.array(doc2vec,dtype=object)\n",
        "  final_vector=doc2vec1[:,M-1].tolist() \n",
        "\n",
        "  return final_vector \n",
        "################################################\n",
        "def Doc2VecTFDist(doc_embed_tfidf,threshold_sim,alpha,centers,dictionary):\n",
        "  M=2\n",
        "  N=10\n",
        "  concept_freq = 0\n",
        "  ## intilize doc array\n",
        "  doc2vec = np.zeros((len(centers), M)).tolist()\n",
        "  final_vector=np.zeros(len(centers)).tolist()\n",
        "  for cnt in range(len(centers)):\n",
        "    for emb_word in doc_embed_tfidf:         \n",
        "      Wsim =cosine_distance(emb_word[1], dictionary[cnt][0][0])# التشابه مع اقرب كلمة للمركز\n",
        "      if Wsim  > alpha:\n",
        "        concept_freq = concept_freq+1 #for tf\n",
        "        doc2vec[cnt][M-1] = doc2vec[cnt][M-1] + 1;\n",
        "      else:\n",
        "        continue;\n",
        "\n",
        "\n",
        "# the number of occurence for all concept in this document\n",
        "  #if concept_freq !=0: \n",
        "   # for i in doc2vec:\n",
        "    #    i[M-1] = (i[M-1] *i[M-1]) / concept_freq   #calculte tf* t distr = (t count/ count all terms) * t count / count term per all docs\n",
        "        #هنا نحسب مربع التكرار على تكرار كل المفاهيم ولاحقا نحسب مجموع تكرار كل مفهوم عبر كل المجموعة ونقسم عليه\n",
        "\n",
        "  doc2vec1=np.array(doc2vec,dtype=object)\n",
        "  final_vector=doc2vec1[:,1].tolist() \n",
        "\n",
        "  return final_vector\n",
        "\n",
        "##################################\n",
        "\n",
        "def concept_distribution(doc_vectors):\n",
        "  num_of_concepts = len(doc_vectors[0])\n",
        "  concept_per_collection = np.zeros(num_of_concepts).tolist()\n",
        "  for doc in doc_vectors:\n",
        "    counter=0 # counting concepts\n",
        "    for concept_count in doc:\n",
        "      concept_per_collection[counter] = concept_per_collection[counter] + concept_count\n",
        "      counter = counter+1\n",
        "  return concept_per_collection\n",
        "\n",
        "#####################################\n",
        "def get_concept_distrib(doc_vectors, concept_per_collection):\n",
        "  new_doc_vect = []\n",
        "  for doc in doc_vectors:\n",
        "    counter=0\n",
        "    doc_vec =[]\n",
        "    for f in doc:\n",
        "      newF = f /concept_per_collection[counter]\n",
        "      doc_vec.append(newF)\n",
        "      counter = counter +1\n",
        "    new_doc_vect.append(doc_vec)\n",
        "    del doc_vec\n",
        "  return new_doc_vect\n",
        "\n",
        "#############################\n",
        "#استخراج الكلمات ذات وزن اكبر من متوسط اوزان كلمات المستند\n",
        "def get_important_words_by_tfidf(doc_vectors):\n",
        "  from statistics import mean\n",
        "  new_doc_list =[]\n",
        "  for doc_vect in doc_vectors:\n",
        "    #get mean value of dict elements tfidf values\n",
        "    #thr_mean= mean(doc_vect[k] for k in doc_vect)\n",
        "    d = dict((k, v) for k, v in doc_vect.items() if v > 0)\n",
        "    new_doc_list.append(d)\n",
        "  return new_doc_list\n",
        "##########################\n",
        "#تحويل القاموس لكل مستند الى قائمة من اسماء الكلمات فقط دون الوزن\n",
        "def get_doc_features_list(new_doc_list):\n",
        "  doc_list_important = []\n",
        "  for doc in new_doc_list:\n",
        "    ##we need only the value\n",
        "    doc_features = list(doc.keys())\n",
        "    doc_list_important.append(doc_features)\n",
        "  return doc_list_important\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "####################################################33333\n",
        "def self_embedding_all_documents(df , model,N):\n",
        "  documents_embeddings = []\n",
        "  for i in range(len(df)):\n",
        "    documents_embeddings.append(Self_convert_doc_to_embedding(df['document'][i] , model,N))\n",
        "  return documents_embeddings\n",
        "\n",
        "########################################################################################################\n",
        "def Self_convert_doc_to_embedding(text , model,N):\n",
        "  tokens = final_preprocess_text(text)\n",
        "  if N!=0:\n",
        "    tokens=removeElements(tokens,N)\n",
        "  embeddings_matrix  , irrugular_words = Create_embedding_matrix(model , tokens , 300)\n",
        "  return embeddings_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj8ww4vWj5BX"
      },
      "source": [
        "# Загрузка Датасетов\n",
        "ВВС & GN & Reuters & OHSUMED & WebKB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "PkuKDatzK8lQ",
        "outputId": "a8a65fad-43dd-49b8-840c-e7863b4070ff"
      },
      "source": [
        "# text doc dataset\n",
        "#استخراج الاسطر من الملف وفصل الفة عن نص المستند\n",
        "import csv\n",
        "R52_train_no_short=\"/content/drive/MyDrive/PHD Work/Data/R52nostop/r52-train-no-stop.txt\"\n",
        "R52_test_no_short=\"/content/drive/MyDrive/PHD Work/Data/R52nostop/r52-test-no-stop.txt\"\n",
        "\n",
        "GN_train_no_stop=\"/content/drive/MyDrive/PHD Work/Data/GNnostop/20ng-train-no-stop.txt\"\n",
        "GN_test_no_stop=\"/content/drive/MyDrive/PHD Work/Data/GNnostop/20ng-test-no-stop.txt\"\n",
        "\n",
        "docs, labels = text_file_to_list(GN_train_no_stop, GN_test_no_stop)\n",
        "data_DF = pd.DataFrame({'class': labels, \n",
        "                        'document': docs})\n",
        "#data_DF['class'] = pd.factorize(data_DF['class'])[0]\n",
        "\n",
        "data_DF.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>document</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>alt.atheism</td>\n",
              "      <td>alt atheism faq atheist resources archive name...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>alt.atheism</td>\n",
              "      <td>alt atheism faq introduction atheism archive n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>alt.atheism</td>\n",
              "      <td>gospel dating article mimsy umd edu mangoe umd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>alt.atheism</td>\n",
              "      <td>university violating separation church state d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>alt.atheism</td>\n",
              "      <td>soc motss princeton axes matching funds for bo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         class                                           document\n",
              "0  alt.atheism  alt atheism faq atheist resources archive name...\n",
              "1  alt.atheism  alt atheism faq introduction atheism archive n...\n",
              "2  alt.atheism  gospel dating article mimsy umd edu mangoe umd...\n",
              "3  alt.atheism  university violating separation church state d...\n",
              "4  alt.atheism  soc motss princeton axes matching funds for bo..."
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGG166KkAODC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "051a3d59-46fd-4914-a23e-5f2d08eb74b2"
      },
      "source": [
        "########### WebKB\n",
        "with open('/content/drive/MyDrive/PHD Work/DataVLAC/web_docs.txt', \"r\") as f:\n",
        "    docs = f.readlines()\n",
        "with open('/content/drive/MyDrive/PHD Work/DataVLAC/web_labels.txt') as f:\n",
        "    labels=[line for line in f]\n",
        "data_DF = pd.DataFrame({'class': labels, \n",
        "                        'document': docs})\n",
        "data_DF.columns.values[0] = \"class\"\n",
        "data_DF.columns.values[1] = \"document\"\n",
        "\n",
        "data_DF['class'] = pd.factorize(data_DF['class'])[0]\n",
        "data_DF.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>document</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>brian comput scienc depart univers wisconsin d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>denni swanson web page mail pop uki offic hour...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>russel impagliazzo depart comput scienc engin ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>dave phd student depart comput scienc univers ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>center lifelong learn design univers colorado ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   class                                           document\n",
              "0      0  brian comput scienc depart univers wisconsin d...\n",
              "1      0  denni swanson web page mail pop uki offic hour...\n",
              "2      1  russel impagliazzo depart comput scienc engin ...\n",
              "3      0  dave phd student depart comput scienc univers ...\n",
              "4      2  center lifelong learn design univers colorado ..."
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "rnb1TFbGkhct",
        "outputId": "a8f3f0e2-a47c-4e2f-9ed1-5e95ec5e9070"
      },
      "source": [
        "Reuters_file_path=\"/content/drive/MyDrive/PHD Work/Data/Reuters88.csv\"\n",
        "BBC_file_path=\"/content/drive/MyDrive/PHD Work/Data/bbc-text.csv\"\n",
        "ohsumed_file_path=\"/content/drive/MyDrive/PHD Work/Data/ohsumed-allcats.csv\"\n",
        "arabic =\"/content/drive/MyDrive/PHD Work/Data/arabic_dataset_classifiction.csv\"\n",
        "\n",
        "data_DF = pd.read_csv(Reuters_file_path)\n",
        "\n",
        "\n",
        "data_DF.columns.values[0] = \"class\"\n",
        "data_DF.columns.values[1] = \"document\"\n",
        "\n",
        "data_DF['class'] = pd.factorize(data_DF['class'])[0]\n",
        "data_DF.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-c5d3f95a77b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0marabic\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/PHD Work/Data/arabic_dataset_classifiction.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdata_DF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mReuters_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/PHD Work/Data/Reuters88.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTV8zj8SmxvY"
      },
      "source": [
        "def Doc2Vec(embeddeings_document,threshold_sim,alpha,centers,dictionary):\n",
        "  M=2\n",
        "  N=5\n",
        "  concept_freq = 0\n",
        "  ## intilize doc array\n",
        "  doc2vec = np.zeros((len(centers), M)).tolist()\n",
        "  final_vector=np.zeros(len(centers)).tolist()\n",
        "  for cnt in range(len(centers)):\n",
        "    counter = 0;\n",
        "    sim =[]\n",
        "    NN=0\n",
        "    for emb_word in embeddeings_document:\n",
        "      Wsim =cosine_distance(emb_word, centers[cnt])# التشابه مع اقرب كلمة للمركز\n",
        "      if Wsim  > alpha :\n",
        "        concept_freq = concept_freq+1 #for tf\n",
        "        sim.append(Wsim)\n",
        "        doc2vec[cnt][M-1] = doc2vec[cnt][M-1] + 1;\n",
        "      else:\n",
        "        continue;\n",
        "      counter = counter +1;\n",
        "    # end for doc words with one concepts\n",
        "    meanofSim=0#  المتوسط لدرجات تشابه كلمات المستند مع المفهوم الحالي\n",
        "    if(len(sim)!=0):\n",
        "      meanofSim=sum(sim)/len(sim)\n",
        "    doc2vec[cnt][M-2]=meanofSim\n",
        "    del sim\n",
        "#* math.exp(i[M-2])\n",
        "  if concept_freq !=0: # the number of occurence for all concept in this document\n",
        "    for i in doc2vec:\n",
        "      #print(i[M-1], concept_freq, 'sim',i[M-2],'exp',math.exp(i[M-2]))\n",
        "      i[M-1] = i[M-1]* math.exp(i[M-2]) / concept_freq\n",
        "      #print(i[M-1])\n",
        "  doc2vec1=np.array(doc2vec,dtype=object)\n",
        "  final_vector=doc2vec1[:,1].tolist() \n",
        "\n",
        "  return final_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlYA9AxDkRzQ"
      },
      "source": [
        "###**Предварительная обработка**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xug4rIFvy7In"
      },
      "source": [
        "K_clusters= 100\n",
        "sim_digree=0.3\n",
        "number_of_words_per_cluster = 5\n",
        "min_doc=0 # الحد الادني لعدد الكلمات في المستند\n",
        "max_idf_r=1\n",
        "min_idf=1\n",
        "\n",
        "\n",
        "token_path=\"/content/drive/MyDrive/PHD Work/PreProData/BBC_r20_data.txt.npy\"\n",
        "\n",
        "#token_path = \"/content/drive/MyDrive/PHD Work/PreProData/GNlast_All_20_data.txt.npy\"\n",
        "\n",
        "model_path='/content/drive/MyDrive/PHD Work/GloVe300/glove-wiki-gigaword-300'\n",
        "#model_path='/content/drive/MyDrive/PHD Work/w2v/word2vec-google-news-300.model'\n",
        "\n",
        "#pre_trained_model = intall_pre_trained_model('/content/drive/MyDrive/PHD Work/w2v/word2vec-google-news-300.model' ,  'word2vec-google-news-300')\n",
        "\n",
        "# naming resulted embedding file\n",
        "Th=5\n",
        "alpha=0.2\n",
        "modd='SelfW2V300' \n",
        "dataset='BBCsim3'\n",
        "typeOfcontent=\"test_\"\n",
        "path='/content/drive/MyDrive/PHD Work/PRUN/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzLJHAkcB7p1",
        "outputId": "074c5b37-cd0d-4657-8a9f-a7ce23e2197d"
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yunQuj79QLYi"
      },
      "source": [
        "#self embedding need rerun the functions\n",
        "documents_as_list = make_documents_as_list(data_DF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whtm6ukM_5QW"
      },
      "source": [
        "\n",
        "from gensim.models import FastText\n",
        "\n",
        "\n",
        "model = FastText(documents_as_list, size=300, window=15, min_count=5, workers=4,sg=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5nb9pEICZ-s"
      },
      "source": [
        "all_tokens=loadList(token_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "vMUdMyA0U_wp",
        "outputId": "b5e9aab5-2bbe-49a7-f031-a2d30ed61577"
      },
      "source": [
        "#all_tokens = new_pre_processing_dataset(data_DF)\n",
        "\n",
        "#all_tokens = pre_processing_dataset(data_DF)\n",
        "#all_tokens=removeElements(all_tokens,20)\n",
        "#fileName=\"/content/drive/MyDrive/PHD Work/PreProData/R52_All_data.txt\"\n",
        "#saveList(all_tokens,fileName)\n",
        "all_tokens=loadList(token_path)\n",
        "\n",
        "\n",
        "#model = Word2Vec(sentences=documents_as_list, size=300, window=15, min_count=5, workers=4)\n",
        "\n",
        "model = Load_pre_trained_model_embeddings(model_path)\n",
        "\n",
        "#with open('/content/drive/MyDrive/PHD Work/DataVLAC/web_embeddings.pickle', 'rb') as handle:\n",
        "#  model = pickle.load(handle)\n",
        "len(all_tokens)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-11e21f68574f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#model = Word2Vec(sentences=documents_as_list, size=300, window=15, min_count=5, workers=4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLoad_pre_trained_model_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#with open('/content/drive/MyDrive/PHD Work/DataVLAC/web_embeddings.pickle', 'rb') as handle:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-cd1598ebf48a>\u001b[0m in \u001b[0;36mLoad_pre_trained_model_embeddings\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;31m### loading the embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mLoad_pre_trained_model_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m   \u001b[0mwv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mwv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m####################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname_or_handle, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseKeyedVectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, fname, mmap)\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSaveLoad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_adapt_by_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m         \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_specials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loaded %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/utils.py\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \"\"\"\n\u001b[0;32m-> 1358\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1359\u001b[0m         \u001b[0;31m# Because of loading from S3 load can't be used (missing readline in smart_open)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    496\u001b[0m     \u001b[0mignore_ext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, compression, transport_params)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m     )\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0mopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'errors'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_builtin_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/PHD Work/GloVe300/glove-wiki-gigaword-300'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnOheQ2GwAQd"
      },
      "source": [
        "## unique_words are the most impoortants by tfidf\n",
        "\n",
        "*   Новый пункт\n",
        "*   Новый пункт\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjbUgUFCv_lN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e04b13f0-9aa0-4779-9fe8-12359a279b8a"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "\n",
        "#### select self or pretrained tokens\n",
        "#الذاتي يكون لها تجذير وتختلف عن المسبق \n",
        "tokened_docs = Self_get_tokened_docs(data_DF)\n",
        "\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words=my_stopwords, use_idf=True, max_df=0.9)\n",
        "vectors = vectorizer.fit_transform(tokened_docs)\n",
        "#print(vectors)\n",
        "fnames= vectorizer.get_feature_names()\n",
        "###\n",
        "index_value={i[1]:i[0] for i in vectorizer.vocabulary_.items()}\n",
        "\n",
        "###\n",
        "fully_indexed = []  # all deoc vectors by tfidf\n",
        "for row in vectors:\n",
        "  fully_indexed.append({index_value[column]:value for (column,value) in zip(row.indices,row.data)})\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['articl', 'mon'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkQilS4KLoJg"
      },
      "source": [
        "تقليم الداتا"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUX_47vcLrWv"
      },
      "source": [
        "بالغاء التعليق عن الداتا تصبح هي نفسها الداتا تبعنا"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w20ZN_AHLnq3"
      },
      "source": [
        "######################################  \n",
        "new_doc_list= get_important_words_by_tfidf(fully_indexed)\n",
        "\n",
        "new_doc_list_sorted =[]\n",
        "#sort each dict of a document\n",
        "for dn in new_doc_list:\n",
        "  newD = sorted(dn.items(), key=lambda x: x[1], reverse=True)\n",
        "  newD = dict(newD)\n",
        "  new_doc_list_sorted.append(newD)\n",
        "\n",
        "doc_list_important= get_doc_features_list(new_doc_list_sorted)\n",
        "labels = data_DF['class'].tolist()\n",
        "data_DF = pd.DataFrame({'class': labels, 'document': doc_list_important})\n",
        "\n",
        "\n",
        "tfidf_all_tokens =Convert_list_of_list_to_list(doc_list_important)\n",
        "important_token = remove_duplication(tfidf_all_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zM2cA2lTDLW4"
      },
      "source": [
        "np.array(doc_list_important[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcdF3HxWwHNN"
      },
      "source": [
        "## unique_words old"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wln2Kfrj3JMb",
        "outputId": "19c33907-c6dd-4e26-c280-0425e72096e5"
      },
      "source": [
        "unique_words = remove_duplication(all_tokens)\n",
        "print(\"all unique words: \" ,len(unique_words),\"\\n\")\n",
        "\n",
        "all_tokens_unique = []\n",
        "for uw in unique_words:\n",
        "  if uw in important_token:\n",
        "    all_tokens_unique.append(uw)\n",
        "\n",
        "unique_words = all_tokens_unique\n",
        "print(\"unique words after removing words that doesnt have tfidf weight: \" ,len(unique_words),\"\\n\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all unique words:  3174 \n",
            "\n",
            "unique words after removing words that doesnt have tfidf weight:  2897 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9y9uRZfH5fY",
        "outputId": "722c303e-0d72-4ddc-d836-75909e8376d4"
      },
      "source": [
        "dic = Save_words_with_Embeddings (unique_words , model)\n",
        "print(\"length dic: \",len(dic),\"\\n\")\n",
        "#model_word_list = list(model.wv.vocab)\n",
        "dim = model.vector_size\n",
        "embeddings_matrix  , irrugular_words = Create_embedding_matrix(model , unique_words , dim)\n",
        "\n",
        "\n",
        "######### Clustering\n",
        "labels , centers = spherical_kmeans(K_clusters , embeddings_matrix, sim_digree)\n",
        "print(\"The clusters details\", Counter(labels), \"\\n\")\n",
        "\n",
        "# dictionary cluster index: values are its words embedding\n",
        "concept_dictionary = get_cluster_elements(labels, K_clusters , embeddings_matrix)\n",
        "\n",
        "start_time = time.time()\n",
        "sorted_cons = get_soreted_dictionary(centers , concept_dictionary , number_of_words_per_cluster)\n",
        "print(\"Clustering excution time--- %s seconds ---\" % (time.time() - start_time), \"\\n\")\n",
        "\n",
        "#Save Text Clusters\n",
        "#textClusters = dictAsText(30,10,sorted_cons,dic)\n",
        "#fileName=\"/content/drive/MyDrive/PHD Work/TextClusters/Test_OH50_10Sim03.txt\"\n",
        "#saveList(textClusters,fileName)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length dic:  2897 \n",
            "\n",
            "The clusters details Counter({13: 55, 29: 49, 90: 48, 130: 45, 177: 41, 133: 40, 104: 39, 192: 39, 95: 39, 75: 38, 22: 36, 179: 33, 196: 33, 93: 32, 156: 32, 38: 32, 41: 32, 96: 32, 53: 32, 147: 30, 58: 30, 25: 29, 46: 27, 82: 27, 19: 27, 134: 27, 165: 27, 99: 27, 154: 26, 72: 25, 21: 25, 109: 24, 170: 24, 101: 23, 39: 23, 18: 23, 67: 23, 33: 23, 181: 22, 56: 22, 74: 22, 146: 21, 139: 21, 183: 21, 135: 20, 112: 20, 17: 20, 169: 19, 189: 19, 126: 19, 26: 19, 199: 19, 71: 18, 83: 18, 176: 18, 14: 18, 63: 18, 5: 17, 160: 17, 128: 16, 4: 16, 113: 16, 185: 16, 61: 16, 125: 16, 88: 16, 27: 16, 16: 16, 116: 16, 76: 15, 54: 15, 188: 15, 86: 15, 31: 15, 115: 15, 118: 15, 81: 14, 45: 14, 3: 14, 173: 14, 103: 14, 92: 14, 106: 14, 57: 13, 174: 13, 108: 13, 40: 13, 195: 13, 37: 13, 161: 13, 127: 13, 34: 12, 191: 12, 6: 12, 78: 12, 59: 12, 132: 12, 151: 12, 142: 12, 23: 12, 172: 12, 171: 12, 43: 11, 150: 11, 73: 11, 131: 11, 91: 11, 9: 11, 85: 11, 162: 10, 178: 10, 111: 10, 84: 10, 79: 10, 49: 10, 107: 10, 138: 10, 153: 10, 159: 10, 52: 9, 155: 9, 30: 9, 166: 9, 163: 9, 12: 9, 7: 9, 141: 9, 105: 9, 80: 9, 64: 9, 97: 9, 184: 9, 122: 8, 158: 8, 114: 8, 70: 8, 47: 8, 87: 8, 186: 8, 20: 8, 129: 8, 157: 8, 1: 8, 50: 8, 44: 8, 62: 8, 51: 8, 60: 8, 148: 7, 66: 7, 152: 7, 175: 7, 190: 7, 121: 7, 35: 7, 0: 7, 65: 7, 193: 7, 180: 7, 167: 6, 24: 6, 15: 6, 197: 6, 2: 6, 98: 6, 194: 6, 117: 6, 140: 6, 149: 6, 36: 6, 168: 5, 55: 5, 124: 5, 8: 5, 102: 5, 137: 5, 123: 5, 48: 5, 145: 5, 164: 4, 69: 4, 144: 4, 100: 4, 143: 4, 32: 4, 120: 4, 110: 4, 182: 3, 42: 3, 187: 3, 94: 3, 11: 3, 198: 3, 10: 2, 89: 2, 136: 2, 119: 2, 77: 2, 28: 2, 68: 1}) \n",
            "\n",
            "Clustering excution time--- 0.7253050804138184 seconds --- \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NA1crRuZT5E"
      },
      "source": [
        "# Bag of weighted Concepts with tfidf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3in7xUWp6-q"
      },
      "source": [
        "لمحة:\n",
        "هذا القسم يتم فيه تضمين كل مستند كمجموعة كلمات لكل منها قيمة وزن تفايدف وشعاع تضمين\n",
        "وعند تطبيق طريقتي يتم حساب متوسط اوزان كلمات العنقود كوزن له"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Llbm7hSNxmnj"
      },
      "source": [
        "## **embedding by new dictionary**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1pSh2QRYYLn"
      },
      "source": [
        "document_emb = embedding_tfidf_all_documents(tokened_docs, model, fully_indexed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksFb5NBa1AdU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b97adadd-d3bb-498d-e3f8-58524a391bc7"
      },
      "source": [
        "###لم يعد مناسب جدا هنا\n",
        "labels =data_DF['class'].tolist()\n",
        "\n",
        "print(\"the  len of labels and docs \", len(labels),len(document_emb),\"\\n\")\n",
        "##data detils\n",
        "#data_details(document_emb, data_DF, 50)\n",
        "\n",
        "#print(\"deleting docs that has not embedded words or WE count less\", min_doc , \"\\n\")\n",
        "\n",
        "#deleting empty\n",
        "min_doc = 0\n",
        "documents_embeddings ,labels = delete_empty_docs(document_emb,labels,min_doc)\n",
        "print(\"the  New len of labels and docs \", len(labels),len(document_emb),\"\\n\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the  len of labels and docs  2225 2225 \n",
            "\n",
            "[]\n",
            "the  New len of labels and docs  2225 2225 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ftH_Q9_m47C"
      },
      "source": [
        "## tfidf weighting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aG9kXd5UbiOn",
        "outputId": "f44551fc-2f88-433f-f9cd-5fd86bc77eab"
      },
      "source": [
        "print(\"start represnation:\\n\")\n",
        "start_time = time.time()\n",
        "alpha= 0.2\n",
        "doc_vectors = RepresentationEMTFIDF(document_emb, Th , alpha , centers , sorted_cons)\n",
        "\n",
        "\n",
        "represnationtime=time.time() - start_time\n",
        "print(\"time for representation: \", represnationtime)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start represnation:\n",
            "\n",
            "time for representation:  821.4428706169128\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gxOmROEXKVCD",
        "outputId": "465e9f74-e1c2-4287-fcee-98dcaeb06f09"
      },
      "source": [
        "concept_per_collection = concept_distribution(doc_vectors) \n",
        "print(len(concept_per_collection))\n",
        "\n",
        "new_doc_vect = get_concept_distrib(doc_vectors, concept_per_collection)\n",
        "### Tf* CD\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyVkwYZOOcV0",
        "outputId": "12ce8c39-442e-4020-afef-3c0784bccd16"
      },
      "source": [
        "\n",
        "fName= \"Doc2VecDist\"+dataset+'_'+modd+ '_'+str(K_clusters)+'cls_'+'_0'+str(alpha)[2:]+'.txt'\n",
        "fileName=path+fName\n",
        "\n",
        "saveList(new_doc_vect,fileName)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Vgw9YF87OcT6",
        "outputId": "b6cc13aa-cbc2-4e29-faeb-ef7a6f668a85"
      },
      "source": [
        "#دون حذفcfedf \n",
        "tf_similarity = new_doc_vect.copy()\n",
        "idfcount,idfexp = calculate_IDF(tf_similarity)\n",
        "\n",
        "final_embeddings = CF_IDF(tf_similarity,idfexp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/PHD Work/Emed/TFIDF/Doc2VecDistRE_Glove300_100cls__02.txt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVmln-3dOcRT"
      },
      "source": [
        "final_embeddings= new_doc_vect\n",
        "print(\"build new data frame: \\n\")\n",
        "labels = data_DF['class'].tolist()\n",
        "embedding_DF = pd.DataFrame({'class': labels, 'document': final_embeddings})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRvrIaHqOcO4"
      },
      "source": [
        "############بعدها انتقل الى التدريب او تسيت الفا"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZOujYPPVImk"
      },
      "source": [
        "## text alpha"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzJQaNE1Ntvg"
      },
      "source": [
        "lalpha= [0.3, 0.4, 0.5]\n",
        "\n",
        "for alpha in lalpha:\n",
        "  max_idf_r=1\n",
        "  kernell='poly' \n",
        "  max_idf=max_idf_r* len(labels)\n",
        "\n",
        "  print(\"start represnation:\\n\")\n",
        "  start_time = time.time()\n",
        "  final_vectors = RepresentationEMTFIDF(document_emb, Th , alpha , centers , sorted_cons)\n",
        "\n",
        "  concept_per_collection = concept_distribution(final_vectors) \n",
        "  print(len(concept_per_collection))\n",
        "\n",
        "  new_doc_vect = get_concept_distrib(final_vectors, concept_per_collection)\n",
        "  \n",
        "  represnationtime=time.time() - start_time\n",
        "\n",
        "\n",
        "  # save the resulted embeddings into file\n",
        "  fName='TestDistOnly'+ '_'+str(min_doc)+\"_\" +dataset+'_'+modd+ '_'+str(K_clusters)+'cls_'+'_0'+str(alpha)[2:]+'.txt'\n",
        "  fileName=path+fName\n",
        "  fileName\n",
        "  saveList(new_doc_vect,fileName)\n",
        "  print(\"time for representation: \", represnationtime,\" seconds, file saved to: \\n\",fileName)\n",
        "\n",
        "  print(\"build new data frame: \\n\")\n",
        "\n",
        "  embedding_DF = pd.DataFrame({'class': labels, 'document': final_vectors})\n",
        "\n",
        "\n",
        "  print(\"Start training ............................................ \",alpha, \"\\n\")\n",
        "\n",
        "\n",
        "  final_embeddings = [array(i).reshape(-1) for i in final_vectors]\n",
        "  labels =embedding_DF['class'].tolist() # ConvertTextLabelToNeumerical(data_DF)\n",
        "\n",
        "  kernell='poly'\n",
        "\n",
        "  x_train , x_test , y_train , y_test = data_preperation(final_embeddings , labels, 0.30)\n",
        "  print(\"resluts without calss info.............\")\n",
        "  svm = train_ML(x_train , y_train)\n",
        "  y_pred=svm.predict(x_test)\n",
        "  f1_without = f1_score(y_test,y_pred, average=\"weighted\")\n",
        "\n",
        "  print(\"f1 without C info\", f1_without)\n",
        "\n",
        "\n",
        "  print(\"resluts with calss info.............\")\n",
        "  embedding_train_DF = pd.DataFrame({'class': list(y_train), 'document': list(x_train)})\n",
        "  embedding_test_DF = pd.DataFrame({'class': list(y_test), 'document': list(x_test)})\n",
        "\n",
        "  # class weghts from training data\n",
        "  freqarray=document_class_of_concept(data_DF, embedding_train_DF,  list(x_train))\n",
        "\n",
        "  xtrain_weighted= class_weighting(list(x_train),embedding_train_DF,freqarray)\n",
        "  print(\"weghting test data by frqarray built based on training data.................\")\n",
        "  xtest_weighted = class_weighting(list(x_test),embedding_test_DF,freqarray)\n",
        "\n",
        "  svm = train_ML(xtrain_weighted , y_train)\n",
        "  accuracy = evaluation_ML(xtest_weighted , y_test , svm)\n",
        "  y_pred=svm.predict(xtest_weighted)\n",
        "  f1_with = f1_score(y_test,y_pred, average=\"weighted\")\n",
        "\n",
        "  print(\"f1 with C info\", f1_with)\n",
        "\n",
        "  fileNameD=\"Test_alpha_DistOnly\" + dataset + \".csv\"\n",
        "  print(\"saving Results to Excel:\\n\" , fileNameD)\n",
        "  \n",
        "  import csv\n",
        "  with open(r'/content/drive/MyDrive/PHD Work/CSV_Results/'+fileNameD, 'a', newline='') as csvfile:\n",
        "      fieldnames = ['data','Model','kernell','clusters','file','alpha','accuracy','f1_without','time','F_with']\n",
        "      writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "      writer.writerow({'file':fileName ,'data':dataset, \n",
        "                      'Model':modd,'kernell':kernell,\n",
        "                      'clusters':K_clusters,'alpha':alpha,\n",
        "                      'accuracy':accuracy,'f1_without':f1_without,'time':represnationtime,\n",
        "                      'F_with':f1_with})\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tL_cGXIx6q1"
      },
      "source": [
        "# **BOWC embedding without tfidf**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvnnNpaUVFwQ"
      },
      "source": [
        "هنا طريقتي مع الحفاظ على كلمات المستند التي لها وزن تفايدف وتضمين في موديل التضمين"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brpqVIucLWOO"
      },
      "source": [
        "هنا عملية تقليم ولا يتم ضرب المتجهات بوزن تفايدف"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGVciu0VKp9D"
      },
      "source": [
        "pretrained pruning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWl7da6y55qn"
      },
      "source": [
        "#داخل هذه الدالة ل ايتم اجراء معالجة لان المستندات معالجة اصلا وتم تقليمها\n",
        "#data_DF هنا هي معالجة وليست الاصلية\n",
        "#في حال تم الغاء المعاجلة فوق في الخلية الخاصة بترميز تردد المصطلح يبقى فقط القامو مختصر والمستندات كما في الحالة الطبيعية\n",
        "documents_embeddingss = embedding_all_documents(data_DF, model, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7uVxIvvKt51"
      },
      "source": [
        "self embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icSzSvN1VdrG",
        "outputId": "d5bed674-b027-41ca-8b85-fc24725d2729"
      },
      "source": [
        "data_details(documents_embeddingss, data_DF, 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "№ of all doc= 8491 \n",
            "\n",
            "Befor embeddings: \n",
            " the mean length of docs 39.163349428806974 \n",
            " The max length of docs 296 \n",
            " The min length of docs 1 \n",
            " № classes   = 8 \n",
            " № docs per class  [329.0, 295.0, 552.0, 205.0, 2369.0, 453.0, 3926.0, 362.0]\n",
            "After embeddings: \n",
            " the mean length of embeded docs 39.159580732540334 \n",
            " The max length of embeded docs 296 \n",
            " The min length of embeded docs 1 \n",
            " № Documents with length >  10 = 7454.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oriGCMey-LGO"
      },
      "source": [
        "## test alpha"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdPmTNGRc44z"
      },
      "source": [
        "labels = data_DF['class'].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JDLMcbTqH0I"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5H8kgxEzs-N"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from tensorflow import keras\n",
        "layers = keras.layers\n",
        "models = keras.models\n",
        "\n",
        "def data_preperation(dataset , labels, sample):\n",
        "  x_train , x_test , y_train , y_test = train_test_split(dataset , labels , test_size = sample , random_state = 0)\n",
        "  return array(x_train) , array(x_test) , array(y_train) , array(y_test)\n",
        "\n",
        "def train_ML(x_train , y_train):\n",
        "  svm =  SVC(kernel=kernell, random_state=44, probability=True).fit(x_train , y_train) \n",
        "  #svm =  LinearSVC(penalty=\"l2\", dual=False,tol=1e-3).fit(x_train , y_train) \n",
        "  #MLPClassifier(random_state=1, max_iter=10000).fit(x_train , y_train)  \n",
        "  return svm\n",
        "  \n",
        "def data_preperation(dataset , labels, sample):\n",
        "  x_train , x_test , y_train , y_test = train_test_split(dataset , labels , test_size = sample , random_state = 0)\n",
        "  return array(x_train) , array(x_test) , array(y_train) , array(y_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xxMundA-I1M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcef6d8a-fbf3-4a49-f6f3-aaa3d83b45a7"
      },
      "source": [
        "lalpha= [0.2, 0.3, 0.4, 0.5, 0.6, 0.7 , 0.8]\n",
        "\n",
        "for alpha in lalpha:\n",
        "  max_idf_r=1\n",
        "  kernell='poly' \n",
        "  max_idf=max_idf_r* len(documents_embeddingss)\n",
        "\n",
        "  print(\"======================================================start represnation:\\n\")\n",
        "  start_time = time.time()\n",
        "  tf_similarity = RepresentationNew(documents_embeddingss, Th , alpha , centers , sorted_cons)\n",
        "\n",
        "  print(\"Counting IDF to select concepts with IDF less than \",max_idf, \":\\n\")\n",
        "\n",
        "  idfcount,idfexp=calculate_IDF(tf_similarity)\n",
        "  todelete=concept_to_delete(min_idf, max_idf,idfcount)\n",
        "  print(len(todelete),\" clusters will be deleted:  \", todelete ,\" \\n\")\n",
        "\n",
        "  new_tf_similarity,idfexp,idfcount= clac_new_Tf_idf(todelete,tf_similarity,idfcount,idfexp)\n",
        "  K_clusters=len(idfexp)\n",
        "  print(\"The new number of clusters: \",K_clusters, \":\\n\")\n",
        "\n",
        "  print(\"Calculating BoEC...\\n\")\n",
        "\n",
        "  final_embeddings=CF_IDF(new_tf_similarity,idfexp)\n",
        "\n",
        "  represnationtime=time.time() - start_time\n",
        "\n",
        "\n",
        "  # save the resulted embeddings into file\n",
        "  fName=typeOfcontent+\"_w15_min5_maxIDF\"+str(max_idf_r)+\"minidf\"+str(min_idf)+ '_'+str(min_doc)+\"_\" +dataset+'_'+modd+ '_'+str(K_clusters)+'cls_'+'_0'+str(alpha)[2:]+'.txt'\n",
        "  fileName=path+fName\n",
        "  fileName\n",
        "  saveList(final_embeddings,fileName)\n",
        "  print(\"time for representation: \", represnationtime,\" seconds, file saved to: \\n\",fileName)\n",
        "\n",
        "  print(\"build new data frame: \\n\")\n",
        "\n",
        "  embedding_DF = pd.DataFrame({'class': labels, 'document': final_embeddings})\n",
        "\n",
        "\n",
        "  print(\"Start training ............................................ \",alpha, \"\\n\")\n",
        "\n",
        "\n",
        "  final_embeddings = [array(i).reshape(-1) for i in final_embeddings]\n",
        "  labels =embedding_DF['class'].tolist() # ConvertTextLabelToNeumerical(data_DF)\n",
        "\n",
        "  kernell='poly'\n",
        "\n",
        "  x_train , x_test , y_train , y_test = data_preperation(final_embeddings , labels, 0.30)\n",
        "  print(\"resluts without calss info.............\")\n",
        "  svm = train_ML(x_train , y_train)\n",
        "  y_pred=svm.predict(x_test)\n",
        "  f1_without = f1_score(y_test,y_pred, average=\"weighted\")\n",
        "\n",
        "  print(\"f1 without C info\", f1_without)\n",
        "\n",
        "\n",
        "  fileNameD=\"test_time_PRUN_w2v_\" + dataset + \".csv\"\n",
        "  print(\"saving Results to Excel:\\n\" , fileNameD)\n",
        "  \n",
        "  import csv\n",
        "  with open(r'/content/drive/MyDrive/PHD Work/CSV_Results/'+fileNameD, 'a', newline='') as csvfile:\n",
        "      fieldnames = ['data','Model','kernell','clusters','file','alpha','accuracy','f1_without','time','F_with']\n",
        "      writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "      writer.writerow({'file':fileName ,'data':dataset, \n",
        "                      'Model':modd,'kernell':kernell,\n",
        "                      'clusters':K_clusters,'alpha':alpha,\n",
        "                      'accuracy':0,'f1_without':f1_without,'time':represnationtime,\n",
        "                      'F_with':0})\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================start represnation:\n",
            "\n",
            "Counting IDF to select concepts with IDF less than  8491 :\n",
            "\n",
            "0  clusters will be deleted:   []  \n",
            "\n",
            "The new number of clusters:  200 :\n",
            "\n",
            "Calculating BoEC...\n",
            "\n",
            "Saved successfully!\n",
            "time for representation:  476.65484499931335  seconds, file saved to: \n",
            " /content/drive/MyDrive/PHD Work/PRUN/test__w15_min5_maxIDF1minidf1_0_REsim3_SelfW2V300_200cls__02.txt\n",
            "build new data frame: \n",
            "\n",
            "Start training ............................................  0.2 \n",
            "\n",
            "resluts without calss info.............\n",
            "f1 without C info 0.9176135483550543\n",
            "saving Results to Excel:\n",
            " test_time_PRUN_w2v_REsim3.csv\n",
            "======================================================start represnation:\n",
            "\n",
            "Counting IDF to select concepts with IDF less than  8491 :\n",
            "\n",
            "0  clusters will be deleted:   []  \n",
            "\n",
            "The new number of clusters:  200 :\n",
            "\n",
            "Calculating BoEC...\n",
            "\n",
            "Saved successfully!\n",
            "time for representation:  469.9456970691681  seconds, file saved to: \n",
            " /content/drive/MyDrive/PHD Work/PRUN/test__w15_min5_maxIDF1minidf1_0_REsim3_SelfW2V300_200cls__03.txt\n",
            "build new data frame: \n",
            "\n",
            "Start training ............................................  0.3 \n",
            "\n",
            "resluts without calss info.............\n",
            "f1 without C info 0.9256558927398137\n",
            "saving Results to Excel:\n",
            " test_time_PRUN_w2v_REsim3.csv\n",
            "======================================================start represnation:\n",
            "\n",
            "Counting IDF to select concepts with IDF less than  8491 :\n",
            "\n",
            "0  clusters will be deleted:   []  \n",
            "\n",
            "The new number of clusters:  200 :\n",
            "\n",
            "Calculating BoEC...\n",
            "\n",
            "Saved successfully!\n",
            "time for representation:  462.42733669281006  seconds, file saved to: \n",
            " /content/drive/MyDrive/PHD Work/PRUN/test__w15_min5_maxIDF1minidf1_0_REsim3_SelfW2V300_200cls__04.txt\n",
            "build new data frame: \n",
            "\n",
            "Start training ............................................  0.4 \n",
            "\n",
            "resluts without calss info.............\n",
            "f1 without C info 0.9300531990485688\n",
            "saving Results to Excel:\n",
            " test_time_PRUN_w2v_REsim3.csv\n",
            "======================================================start represnation:\n",
            "\n",
            "Counting IDF to select concepts with IDF less than  8491 :\n",
            "\n",
            "0  clusters will be deleted:   []  \n",
            "\n",
            "The new number of clusters:  200 :\n",
            "\n",
            "Calculating BoEC...\n",
            "\n",
            "Saved successfully!\n",
            "time for representation:  451.4470748901367  seconds, file saved to: \n",
            " /content/drive/MyDrive/PHD Work/PRUN/test__w15_min5_maxIDF1minidf1_0_REsim3_SelfW2V300_200cls__05.txt\n",
            "build new data frame: \n",
            "\n",
            "Start training ............................................  0.5 \n",
            "\n",
            "resluts without calss info.............\n",
            "f1 without C info 0.9239932100371215\n",
            "saving Results to Excel:\n",
            " test_time_PRUN_w2v_REsim3.csv\n",
            "======================================================start represnation:\n",
            "\n",
            "Counting IDF to select concepts with IDF less than  8491 :\n",
            "\n",
            "0  clusters will be deleted:   []  \n",
            "\n",
            "The new number of clusters:  200 :\n",
            "\n",
            "Calculating BoEC...\n",
            "\n",
            "Saved successfully!\n",
            "time for representation:  438.6652936935425  seconds, file saved to: \n",
            " /content/drive/MyDrive/PHD Work/PRUN/test__w15_min5_maxIDF1minidf1_0_REsim3_SelfW2V300_200cls__06.txt\n",
            "build new data frame: \n",
            "\n",
            "Start training ............................................  0.6 \n",
            "\n",
            "resluts without calss info.............\n",
            "f1 without C info 0.8857576235386028\n",
            "saving Results to Excel:\n",
            " test_time_PRUN_w2v_REsim3.csv\n",
            "======================================================start represnation:\n",
            "\n",
            "Counting IDF to select concepts with IDF less than  8491 :\n",
            "\n",
            "0  clusters will be deleted:   []  \n",
            "\n",
            "The new number of clusters:  200 :\n",
            "\n",
            "Calculating BoEC...\n",
            "\n",
            "Saved successfully!\n",
            "time for representation:  435.10400772094727  seconds, file saved to: \n",
            " /content/drive/MyDrive/PHD Work/PRUN/test__w15_min5_maxIDF1minidf1_0_REsim3_SelfW2V300_200cls__07.txt\n",
            "build new data frame: \n",
            "\n",
            "Start training ............................................  0.7 \n",
            "\n",
            "resluts without calss info.............\n",
            "f1 without C info 0.8569075072820604\n",
            "saving Results to Excel:\n",
            " test_time_PRUN_w2v_REsim3.csv\n",
            "======================================================start represnation:\n",
            "\n",
            "Counting IDF to select concepts with IDF less than  8491 :\n",
            "\n",
            "0  clusters will be deleted:   []  \n",
            "\n",
            "The new number of clusters:  200 :\n",
            "\n",
            "Calculating BoEC...\n",
            "\n",
            "Saved successfully!\n",
            "time for representation:  432.08227944374084  seconds, file saved to: \n",
            " /content/drive/MyDrive/PHD Work/PRUN/test__w15_min5_maxIDF1minidf1_0_REsim3_SelfW2V300_200cls__08.txt\n",
            "build new data frame: \n",
            "\n",
            "Start training ............................................  0.8 \n",
            "\n",
            "resluts without calss info.............\n",
            "f1 without C info 0.7296367779634814\n",
            "saving Results to Excel:\n",
            " test_time_PRUN_w2v_REsim3.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGVksnHP-Or3"
      },
      "source": [
        "## representation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ci2bgMM3NiDu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82dc373c-769e-4bfb-e39e-d07cfce21509"
      },
      "source": [
        "print(\"start represnatio OLDn:\\n\")\n",
        "start_time = time.time()\n",
        "alpha = 0.4\n",
        "tf_similarity = RepresentationNew(documents_embeddingss, Th , alpha , centers , sorted_cons)\n",
        "represnationtime = time.time() - start_time\n",
        "print(\"time for representation: \", represnationtime)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start represnatio OLDn:\n",
            "\n",
            "time for representation:  818.4240050315857\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6Sf7vjYYG-d"
      },
      "source": [
        "idfcount,idfexp = calculate_IDF(tf_similarity)\n",
        "\n",
        "final_embeddings = CF_IDF(tf_similarity,idfexp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "634bF5xqpDHH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56b86dca-50d8-40e1-d8b4-25ed0c683e67"
      },
      "source": [
        "# save the resulted embeddings into file\n",
        "fName = typeOfcontent+\"_\"+dataset+'_'+modd+ '_'+str(K_clusters)+'cls_'+'_0'+str(alpha)[2:]+'.txt'\n",
        "fileName=path+fName\n",
        "fileName\n",
        "saveList(final_embeddings,fileName)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oFWePdc9snZ5",
        "outputId": "9595c8f3-b2fe-430c-abbb-f4f220ca4bcc"
      },
      "source": [
        "fileName"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/PHD Work/Emed/TFIDF/termPruntfidf__GNsim3_Glove300_200cls__02.txt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QS-5zSlwuUwn"
      },
      "source": [
        "max_idf = 0.95* len(labels)\n",
        "ttf_similarity = final_embeddings.copy()\n",
        "#دون حذف \n",
        "print(\"Counting IDF to select concepts with IDF less than \",max_idf, \":\\n\")\n",
        "\n",
        "idfcount,idfexp=calculate_IDF(ttf_similarity)\n",
        "\n",
        "todelete=concept_to_delete(min_idf, max_idf,idfcount)\n",
        "print(len(todelete),\" clusters will be deleted:  \", todelete ,\" \\n\")\n",
        "\n",
        "\n",
        "new_tf_similarity,idfexp,idfcount= clac_new_Tf_idf(todelete,ttf_similarity,idfcount,idfexp)\n",
        "K_clusters=len(idfexp)\n",
        "print(\"The new number of clusters: \",K_clusters, \":\\n\")\n",
        "\n",
        "print(\"Calculating BoEC...\\n\")\n",
        "\n",
        "final_embeddings=CF_IDF(new_tf_similarity,idfexp)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EL-3alp86F_i",
        "outputId": "d45743a6-747b-40cb-84fa-b889826b40bf"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10xifz7kde8S"
      },
      "source": [
        "OLD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXvAtP5kFlYS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f327df28-da7a-4407-b32d-aba4e3bf14f5"
      },
      "source": [
        "N=0\n",
        "documents_embeddings = embedding_all_documents(data_DF , model,N)\n",
        "labels =data_DF['class'].tolist()\n",
        "print(\"the  len of labels and docs \", len(labels),len(documents_embeddings),\"\\n\")\n",
        "#deleting empty\n",
        "documents_embeddings ,labels = delete_empty_docs(documents_embeddings,labels,min_doc)\n",
        "print(\"the  New len of labels and docs \", len(labels),len(documents_embeddings),\"\\n\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:178: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "the  len of labels and docs  2225 2225 \n",
            "\n",
            "[]\n",
            "the  New len of labels and docs  2225 2225 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKkQSxWgeG2A"
      },
      "source": [
        "labels =data_DF['class'].tolist()\n",
        "\n",
        "embedding_DF = pd.DataFrame({'class': labels, 'document': final_embeddings})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STJTLXW_ZaQO"
      },
      "source": [
        "##Machine Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AA2Uc4MVdhdQ"
      },
      "source": [
        "\n",
        "kernell='poly'\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from tensorflow import keras\n",
        "layers = keras.layers\n",
        "models = keras.models\n",
        "\n",
        "def data_preperation(dataset , labels, sample):\n",
        "  x_train , x_test , y_train , y_test = train_test_split(dataset , labels , test_size = sample , random_state = 0)\n",
        "  return array(x_train) , array(x_test) , array(y_train) , array(y_test)\n",
        "\n",
        "def train_ML(x_train , y_train):\n",
        "  svm =  SVC(kernel=kernell, random_state=44, probability=True).fit(x_train , y_train) \n",
        "  #svm =  LinearSVC(penalty=\"l2\", dual=False,tol=1e-3).fit(x_train , y_train) \n",
        "  #MLPClassifier(random_state=1, max_iter=10000).fit(x_train , y_train)  \n",
        "  return svm\n",
        "\n",
        "#linear', 'poly', 'rbf', 'sigmoid',\n",
        "\n",
        "\n",
        "def train_RF(x_train , y_train):\n",
        "  rfc = RandomForestClassifier( n_estimators=500, max_depth=150,n_jobs=1).fit(x_train , y_train)\n",
        "  return rfc\n",
        "\n",
        "\n",
        "def train_DM(x_train, y_train):\n",
        "  batch_size = 32\n",
        "  epochs = 40\n",
        "  drop_ratio = 0.4\n",
        "   \n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Dense(1024))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.Dense(512))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.Dense(125))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.Dense(32))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.Dense(num_classes))\n",
        "  model.add(layers.Activation('softmax'))\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy'])\n",
        "  history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_split=0.1)\n",
        "  return model\n",
        "\n",
        "def evaluation_ML(x_test , y_test , svm_model):\n",
        "  ev=svm_model.score(x_test , y_test)\n",
        "  return ev\n",
        "def evaluation_DL(x_test , y_test , model):\n",
        "  ev=model.evaluate(x_test , y_test)\n",
        "  return ev\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iheyVCJXCDAk",
        "outputId": "a3aee970-420d-4012-92ee-a34f44d3514f"
      },
      "source": [
        "\n",
        "final_embeddings = [array(i).reshape(-1) for i in final_embeddings]\n",
        "labels =embedding_DF['class'].tolist() # ConvertTextLabelToNeumerical(data_DF)\n",
        "print(len(final_embeddings))\n",
        "print(len(labels))\n",
        "x_train , x_test , y_train , y_test = data_preperation(final_embeddings , labels, 0.20)\n",
        "svm = train_ML(x_train , y_train)\n",
        "\n",
        "\n",
        "ev1 = evaluation_ML(x_test , y_test , svm)\n",
        "\n",
        "X =final_embeddings\n",
        "y= labels\n",
        "#scores = cross_val_score(svm, X, y, cv=5, scoring='f1_weighted')\n",
        "#print(scores)\n",
        "y_pred=svm.predict(x_test)\n",
        "f1_without = f1_score(y_test,y_pred, average=\"weighted\")\n",
        "\n",
        "print(\"f1_without\", f1_without)\n",
        "print(\"saving Results to Excel:\\n\" , \"REUResults.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8491\n",
            "8491\n",
            "f1_without 0.9238581745464485\n",
            "saving Results to Excel:\n",
            " REUResults.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_f9iSvnJ5V9",
        "outputId": "df86582d-cc45-443b-b026-d1d3057f7292"
      },
      "source": [
        "kernell='poly'\n",
        "\n",
        "x_train , x_test , y_train , y_test = data_preperation(final_embeddings , labels, 0.30)\n",
        "\n",
        "embedding_train_DF = pd.DataFrame({'class': list(y_train), 'document': list(x_train)})\n",
        "embedding_test_DF = pd.DataFrame({'class': list(y_test), 'document': list(x_test)})\n",
        "\n",
        "freqarray = document_class_of_concept(data_DF, embedding_train_DF,  list(x_train))\n",
        "\n",
        "#print(\"Weighting each concept by class weight based on train data.... \", \"\\n\", freqarray)\n",
        "\n",
        "xtrain_weighted= class_weighting(list(x_train),embedding_train_DF,freqarray)\n",
        "\n",
        "xtest_weighted = class_weighting(list(x_test),embedding_test_DF,freqarray)\n",
        "\n",
        "\n",
        "svm = train_ML(xtrain_weighted , y_train)\n",
        "\n",
        "\n",
        "ev1 = evaluation_ML(xtest_weighted , y_test , svm)\n",
        "\n",
        "\n",
        "y_pred=svm.predict(xtest_weighted)\n",
        "f1_with = f1_score(y_test,y_pred, average=\"weighted\")\n",
        "\n",
        "print(\"f1_with\", f1_with)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f1_with 0.982008841846525\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "s_bR_o58qA1k",
        "outputId": "99729611-c3cc-4826-9b8b-43489c24e2f2"
      },
      "source": [
        "fileName"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/PHD Work/Emed/TFIDF/Self_termPruntfidf__REsim3_Glove300_200cls__02.txt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GB-BRJFsMa4",
        "outputId": "e03421a0-0a26-4237-8aa2-b5f3d890c877"
      },
      "source": [
        "print(represnationtime)\n",
        "# RE300"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "580.4908838272095\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Bz2f-0AlmEd",
        "outputId": "f7605717-e5d1-4b9c-beb0-dc2554df7e74"
      },
      "source": [
        "fileNameD=\"self_TFIDF\" + dataset + \".csv\"\n",
        "print(\"saving Results to Excel:\\n\" , fileNameD)\n",
        "\n",
        "\n",
        "import csv\n",
        "with open(r'/content/drive/MyDrive/PHD Work/CSV_Results/'+fileNameD, 'a', newline='') as csvfile:\n",
        "    fieldnames = ['data','Model','kernell','clusters','file','alpha','accuracy','f1_without','time','F_with']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    writer.writerow({'file':fileName ,'data':dataset, \n",
        "                    'Model':modd,'kernell':kernell,\n",
        "                    'clusters':K_clusters,'alpha':alpha,\n",
        "                    'accuracy':0,'f1_without':f1_without,'time':represnationtime,\n",
        "                    'F_with':f1_with})\n",
        "print(represnationtime)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "saving Results to Excel:\n",
            " self_TFIDFREsim3.csv\n",
            "818.4240050315857\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujbwBLD9uH7E",
        "outputId": "637d21c2-ece0-4c13-eb70-247795bf4f5f"
      },
      "source": [
        "print(\"From saved embeddings and labels , build new data frame after deleting empty docs.... \\n\")\n",
        "embedding_DF = pd.DataFrame({'class': labels, 'document': saved_embaddings})\n",
        "\n",
        "print(\"Calc freq array... \", \"\\n\")\n",
        "freqarray=document_class_of_concept(embedding_DF,  saved_embaddings)\n",
        "\n",
        "print(\"Weighting each concept by class weight.... \", \"\\n\")\n",
        "\n",
        "last_embedding= class_weighting(saved_embaddings,embedding_DF,freqarray)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.96      0.97        90\n",
            "           1       0.96      0.88      0.91        98\n",
            "           2       0.94      0.97      0.95       175\n",
            "           3       0.98      0.90      0.94        58\n",
            "           4       0.98      0.99      0.99       661\n",
            "           5       0.98      0.95      0.97       136\n",
            "           6       0.99      1.00      0.99      1219\n",
            "           7       0.95      0.91      0.93       111\n",
            "\n",
            "    accuracy                           0.98      2548\n",
            "   macro avg       0.97      0.94      0.96      2548\n",
            "weighted avg       0.98      0.98      0.98      2548\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4UpRMLjdAIT"
      },
      "source": [
        "## اختبار الفا مع ملف محفوظ سابقا"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5i-9QdxdFjX"
      },
      "source": [
        "#alpha = fileName.split(\"txt\")[0][-3:-1]\n",
        "fileName1= \"/content/drive/MyDrive/PHD Work/Emed/Test_alphaTestThresholdmaxIDF1minidf0_0_RESim03_ًGloVe300_80cls__**.txt.npy\"\n",
        "Lalpha = ['02','03','04','05','06','07','08']\n",
        "\n",
        "for alpha in Lalpha:\n",
        "  fileName = fileName1.replace('**', alpha)\n",
        "  print(fileName)\n",
        "  max_idf_r=1\n",
        "  kernell='poly' \n",
        "  max_idf=max_idf_r* len(documents_embeddings)\n",
        "\n",
        "  print(\"======================================================Using saved embeddings represnation:\\n\")\n",
        "  start_time = time.time()\n",
        "\n",
        "  min_doc_length=0\n",
        "  ### get saved embeddings\n",
        "  final_embeddings=loadList(fileName)\n",
        "  #labels =data_DF['class'].tolist()\n",
        "  final_embeddings ,labels = delete_empty_docs(final_embeddings, labels,min_doc_length )\n",
        "\n",
        "  K_clusters = len(final_embeddings[0])\n",
        "\n",
        "\n",
        "  embedding_DF = pd.DataFrame({'class': labels, 'document': final_embeddings})\n",
        "\n",
        "\n",
        "  print(\"Start training ............................................ \",alpha, \"\\n\")\n",
        "\n",
        "\n",
        "  final_embeddings = [array(i).reshape(-1) for i in final_embeddings]\n",
        "  labels =embedding_DF['class'].tolist() # ConvertTextLabelToNeumerical(data_DF)\n",
        "\n",
        "  kernell='poly'\n",
        "\n",
        "  x_train , x_test , y_train , y_test = data_preperation(final_embeddings , labels, 0.30)\n",
        "  print(\"resluts without calss info.............\")\n",
        "  svm = train_ML(x_train , y_train)\n",
        "  y_pred=svm.predict(x_test)\n",
        "  f1_without = f1_score(y_test,y_pred, average=\"weighted\")\n",
        "\n",
        "  print(\"f1 without C info\", f1_without)\n",
        "\n",
        "\n",
        "  print(\"resluts with calss info.............\")\n",
        "  embedding_train_DF = pd.DataFrame({'class': list(y_train), 'document': list(x_train)})\n",
        "  embedding_test_DF = pd.DataFrame({'class': list(y_test), 'document': list(x_test)})\n",
        "\n",
        "  # class weghts from training data\n",
        "  freqarray=document_class_of_concept(data_DF, embedding_train_DF,  list(x_train))\n",
        "\n",
        "  xtrain_weighted= class_weighting(list(x_train),embedding_train_DF,freqarray)\n",
        "  print(\"weghting test data by frqarray built based on training data.................\")\n",
        "  xtest_weighted = class_weighting(list(x_test),embedding_test_DF,freqarray)\n",
        "\n",
        "  svm = train_ML(xtrain_weighted , y_train)\n",
        "  accuracy = evaluation_ML(xtest_weighted , y_test , svm)\n",
        "  y_pred=svm.predict(xtest_weighted)\n",
        "  f1_with = f1_score(y_test,y_pred, average=\"weighted\")\n",
        "\n",
        "\n",
        "  fileNameD=\"Test_alpha_\" + dataset + \".csv\"\n",
        "  print(\"saving Results to Excel:\\n\" , fileNameD)\n",
        "\n",
        "  represnationtime = time.time() - start_time\n",
        "  '''\n",
        "  import csv\n",
        "  with open(r'/content/drive/MyDrive/PHD Work/CSV_Results/'+fileNameD, 'a', newline='') as csvfile:\n",
        "      fieldnames = ['data','Model','kernell','clusters','file','alpha','accuracy','f1_without','time','F_with']\n",
        "      writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "      writer.writerow({'file':fileName ,'data':dataset, \n",
        "                      'Model':modd,'kernell':kernell,\n",
        "                      'clusters':K_clusters,'alpha':alpha,\n",
        "                      'accuracy':accuracy,'f1_without':f1_without,'time':represnationtime,\n",
        "                      'F_with':f1_with})\n",
        "'''\n",
        "  print(\"f1 with C info\", f1_with)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHcn7czs_Jty"
      },
      "source": [
        "## اختبار ملف محفوظ ملف واحد"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_7NDCTM_JG6"
      },
      "source": [
        "#alpha = fileName.split(\"txt\")[0][-3:-1]\n",
        "fileName= \"/content/drive/MyDrive/PHD Work/Emed/TFIDF/expr_GNsim3_Glove300_300cls__02.txt.npy\"\n",
        "\n",
        "alpha = 0.2\n",
        "max_idf_r=1\n",
        "kernell='poly' \n",
        "max_idf=0\n",
        "\n",
        "print(\"======================================================Using saved embeddings represnation:\\n\")\n",
        "start_time = time.time()\n",
        "\n",
        "min_doc_length=0\n",
        "### get saved embeddings\n",
        "final_embeddings=loadList(fileName)\n",
        "#labels =data_DF['class'].tolist()\n",
        "final_embeddings ,labels = delete_empty_docs(final_embeddings, labels,min_doc_length )\n",
        "\n",
        "K_clusters = len(final_embeddings[0])\n",
        "\n",
        "\n",
        "embedding_DF = pd.DataFrame({'class': labels, 'document': final_embeddings})\n",
        "\n",
        "\n",
        "print(\"Start training ............................................ \",alpha, \"\\n\")\n",
        "\n",
        "\n",
        "final_embeddings = [array(i).reshape(-1) for i in final_embeddings]\n",
        "labels =embedding_DF['class'].tolist() # ConvertTextLabelToNeumerical(data_DF)\n",
        "\n",
        "kernell='poly'\n",
        "\n",
        "x_train , x_test , y_train , y_test = data_preperation(final_embeddings , labels, 0.30)\n",
        "print(\"resluts without calss info.............\")\n",
        "svm = train_ML(x_train , y_train)\n",
        "y_pred=svm.predict(x_test)\n",
        "f1_without = f1_score(y_test,y_pred, average=\"weighted\")\n",
        "\n",
        "print(\"f1 without C info\", f1_without)\n",
        "\n",
        "\n",
        "print(\"resluts with calss info.............\")\n",
        "embedding_train_DF = pd.DataFrame({'class': list(y_train), 'document': list(x_train)})\n",
        "embedding_test_DF = pd.DataFrame({'class': list(y_test), 'document': list(x_test)})\n",
        "\n",
        "# class weghts from training data\n",
        "freqarray=document_class_of_concept(data_DF, embedding_train_DF,  list(x_train))\n",
        "\n",
        "xtrain_weighted= class_weighting(list(x_train),embedding_train_DF,freqarray)\n",
        "print(\"weghting test data by frqarray built based on training data.................\")\n",
        "xtest_weighted = class_weighting(list(x_test),embedding_test_DF,freqarray)\n",
        "\n",
        "svm = train_ML(xtrain_weighted , y_train)\n",
        "accuracy = evaluation_ML(xtest_weighted , y_test , svm)\n",
        "y_pred=svm.predict(xtest_weighted)\n",
        "f1_with = f1_score(y_test,y_pred, average=\"weighted\")\n",
        "\n",
        "\n",
        "fileNameD=\"Test_alpha_TFIDF\" + dataset + \".csv\"\n",
        "print(\"saving Results to Excel:\\n\" , fileNameD)\n",
        "\n",
        "represnationtime = time.time() - start_time\n",
        "\n",
        "import csv\n",
        "with open(r'/content/drive/MyDrive/PHD Work/CSV_Results/'+fileNameD, 'a', newline='') as csvfile:\n",
        "    fieldnames = ['data','Model','kernell','clusters','file','alpha','accuracy','f1_without','time','F_with']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    writer.writerow({'file':fileName ,'data':dataset, \n",
        "                    'Model':modd,'kernell':kernell,\n",
        "                    'clusters':K_clusters,'alpha':alpha,\n",
        "                    'accuracy':accuracy,'f1_without':f1_without,'time':represnationtime,\n",
        "                    'F_with':f1_with})\n",
        "\n",
        "print(\"f1 with C info\", f1_with)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMafckFqxOiU"
      },
      "source": [
        "# **BOC**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVKpZm0fxaEJ"
      },
      "source": [
        "لتشغيل هذا الجزء يجب تنفيذ الكود السابق كله بدون الكود الخاص **بي**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sb3ffN7KF-Fw"
      },
      "source": [
        "###########################################\n",
        "def BOC_SIM(embeddeings_document,threshold_sim,alpha,centers,dictionary):\n",
        "  M=2\n",
        "  concept_freq = 0\n",
        "  ## intilize doc array\n",
        "  doc2vec = np.zeros(len(centers)).tolist()\n",
        "\n",
        "  for cnt in range(len(centers)):\n",
        "    counter = 0;\n",
        "    sim =[]\n",
        "    for emb_word in embeddeings_document:\n",
        "      Wsim= cosine_distance(emb_word , centers[cnt])\n",
        "      if Wsim  > alpha:\n",
        "        concept_freq = concept_freq+1 #for tf\n",
        "        doc2vec[cnt] = doc2vec[cnt] + 1;\n",
        "        Smean =Get_similarity_matrix(emb_word , dictionary[cnt][0:threshold_sim-1])\n",
        "        sim.append(Smean)\n",
        "      else:\n",
        "        continue;\n",
        "      counter = counter +1;\n",
        "    # end for doc words with one concepts\n",
        "    meanofSim = 1 #الواحد حيادي للضرب لكي لا يلغي التكرار\n",
        "    if(len(sim)!=0):\n",
        "      meanofSim= sum(sim)/len(sim)\n",
        "    doc2vec[cnt]=doc2vec[cnt] * meanofSim\n",
        "  if concept_freq !=0: # the number of occurence for all concept in this document\n",
        "    for i in doc2vec:\n",
        "      i = i / concept_freq  \n",
        "  return doc2vec\n",
        "\n",
        "####################################\n",
        "def CF_IDF_BOC_SIM(documents_embeddings , sim_val , alpha , centers , dictionary):\n",
        "  embeddings = []\n",
        "  counter = 0\n",
        "  IDf = np.zeros(len(centers)).tolist() # بطول العناقيد\n",
        "  for i in documents_embeddings: \n",
        "    bag_of_concepts = BOC_SIM(i ,sim_val , alpha , centers , dictionary)\n",
        "    ######## حساب عدد مرات ظهور المفهوم في كل نص\n",
        "    for i in bag_of_concepts:   # i is concept line , i need class of doc v\n",
        "      if i > 0:\n",
        "        IDf[counter] = IDf[counter] + 1 #  i[M-1]\n",
        "      counter = counter+1\n",
        "    counter = 0\n",
        "    embeddings.append(bag_of_concepts)\n",
        "  #########  calculate IDF\n",
        "  newIDF=[]\n",
        "  for i in IDf:\n",
        "    if i!=0:\n",
        "      N=len(embeddings)\n",
        "      newIDF.append( math.log10( (N+1) /i ) )\n",
        "    else:\n",
        "      newIDF.append(0)  \n",
        "  ######## Calculate Cf-IDF\n",
        "  print(len(embeddings))\n",
        "  for i in range(len(embeddings)):\n",
        "    l=len(embeddings[i])\n",
        "    for j in range(l):\n",
        "    #for doc i: muti cell j with idf j \n",
        "       #print('i', i,' j ', j, ' = ', embeddings[i][j] * newIDF[j])\n",
        "       embeddings[i][j] = embeddings[i][j] * newIDF[j]\n",
        "  \n",
        "  return embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFa9hdcAxYmI",
        "outputId": "645e4596-675b-49b4-fc14-3dbe4ef63515"
      },
      "source": [
        "print(\"start represnation:\\n\")\n",
        "start_time = time.time()\n",
        "cfidf = CF_IDF_BOC_SIM(documents_embeddings, Th , alpha , centers , sorted_cons)\n",
        "\n",
        "represnationtime=time.time() - start_time\n",
        "\n",
        "final_embeddings=cfidf\n",
        "# save the resulted embeddings into file\n",
        "fName=\"BOC_SIM_\"+\"_\" +dataset+'_'+modd+ '_'+str(K_clusters)+'cls_'+'_0'+str(alpha)[2:]+'.txt'\n",
        "fileName=path+fName\n",
        "fileName\n",
        "saveList(final_embeddings,fileName)\n",
        "print(\"time for representation: \", represnationtime,\" seconds, file saved to: \\n\",fileName)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "start represnation:\n",
            "\n",
            "4167\n",
            "Saved successfully!\n",
            "time for representation:  966.4624905586243  seconds, file saved to: \n",
            " /content/drive/MyDrive/PHD Work/Emed/Test/BOC_WESim0303_Glove300_200cls__04.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXnauqInHGIv",
        "outputId": "7aa3e7e9-6fce-4b0b-dfb8-74ca38628b66"
      },
      "source": [
        "fileName=\"/content/drive/MyDrive/PHD Work/BOC_embedd/BOC_RESim0303_Glove300_200cls__04.txt.npy\"\n",
        "final_embeddings=loadList(fileName)\n",
        "\n",
        "len(final_embeddings)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8491"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7XKU5PhpRyQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dba7f2db-3b50-4822-d1b8-aea98327de47"
      },
      "source": [
        "print(\"From saved embeddings and labels , build new data frame after deleting empty docs.... \\n\")\n",
        "embedding_DF = pd.DataFrame({'class': labels, 'document': final_embeddings})\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From saved embeddings and labels , build new data frame after deleting empty docs.... \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnwuQQaEztgE",
        "outputId": "0dff8e49-cff2-4531-d803-152feda4b839"
      },
      "source": [
        "kernell='poly'\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from tensorflow import keras\n",
        "layers = keras.layers\n",
        "models = keras.models\n",
        "\n",
        "def data_preperation(dataset , labels):\n",
        "  x_train , x_test , y_train , y_test = train_test_split(dataset , labels , test_size = 0.20 , random_state = 0)\n",
        "  return array(x_train) , array(x_test) , array(y_train) , array(y_test)\n",
        "\n",
        "def train_ML(x_train , y_train):\n",
        "  svm =  SVC(kernel=kernell).fit(x_train , y_train) \n",
        "  #svm =  LinearSVC().fit(x_train , y_train) \n",
        "  #MLPClassifier(random_state=1, max_iter=10000).fit(x_train , y_train)  \n",
        "  return svm\n",
        "\n",
        "#linear', 'poly', 'rbf', 'sigmoid',\n",
        "\n",
        "\n",
        "def train_RF(x_train , y_train):\n",
        "  rfc = RandomForestClassifier( n_estimators=500, max_depth=150,n_jobs=1).fit(x_train , y_train)\n",
        "  return rfc\n",
        "\n",
        "\n",
        "def train_DM(x_train, y_train):\n",
        "  batch_size = 32\n",
        "  epochs = 40\n",
        "  drop_ratio = 0.4\n",
        "   \n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Dense(1024))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.Dense(512))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.Dense(125))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.Dense(32))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.Dense(num_classes))\n",
        "  model.add(layers.Activation('softmax'))\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy'])\n",
        "  history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_split=0.1)\n",
        "  return model\n",
        "\n",
        "def evaluation_ML(x_test , y_test , svm_model):\n",
        "  ev=svm_model.score(x_test , y_test)\n",
        "  return ev\n",
        "def evaluation_DL(x_test , y_test , model):\n",
        "  ev=model.evaluate(x_test , y_test)\n",
        "  return ev\n",
        "\n",
        "\n",
        "final_embeddings = [array(i).reshape(-1) for i in final_embeddings]\n",
        "#labels =embedding_DF['class'].tolist() # ConvertTextLabelToNeumerical(data_DF)\n",
        "print(len(final_embeddings))\n",
        "print(len(labels))\n",
        "x_train , x_test , y_train , y_test = data_preperation(final_embeddings , labels)\n",
        "svm = train_ML(x_train , y_train)\n",
        "ev1=evaluation_ML(x_test , y_test , svm)\n",
        "#rfc = train_RF(x_train , y_train)\n",
        "#ev2=evaluation_ML(x_test , y_test , rfc)\n",
        "print(\"Accuaracy by SVM= \", ev1)\n",
        "\n",
        "y_pred=svm.predict(x_test)\n",
        "#bal=balanced_accuracy_score(y_test,y_pred)\n",
        "#print(\"Balanced Accuaracy by SVM = \",bal, \"\\n\")\n",
        "\n",
        "print(\"saving Results to Excel:\\n\" , \"REUResults.csv\")\n",
        "'''\n",
        "import csv\n",
        "with open(r'/content/drive/MyDrive/PHD Work/Ali.csv', 'a', newline='') as csvfile:\n",
        "    fieldnames = ['data','Model','function','clusters','file','a','ML','RF','time','bal']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    writer.writerow({'file':fileName ,'data':dataset, \n",
        "                     'Model':'BOC','function':kernell,\n",
        "                     'clusters':K_clusters,'a':alpha,\n",
        "                     'ML':ev1,'RF':ev2,'time':represnationtime,\n",
        "                     'bal':bal})\n",
        "'''\n",
        "\n",
        "X =final_embeddings\n",
        "y= labels\n",
        "scores = cross_val_score(svm, X, y, cv=5, scoring='f1_weighted')\n",
        "print(scores)\n",
        "y_pred=svm.predict(x_test)\n",
        "f1 = f1_score(y_test,y_pred, average=\"weighted\")\n",
        "\n",
        "print(\"f1\", f1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8491\n",
            "8491\n",
            "Accuaracy by SVM=  0.6821659799882284\n",
            "saving Results to Excel:\n",
            " REUResults.csv\n",
            "[0.63115655 0.61744477 0.61119804 0.62833408 0.67460486]\n",
            "f1 0.643101528611045\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73nsbA7jpMWD"
      },
      "source": [
        "# Averaged **W2V**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztNz2AGxvAxV"
      },
      "source": [
        "البيانات من فوق يتم استدعاء الداتا **فريم** **Текст, выделенный полужирным шрифтом**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKC-FBfypL77",
        "outputId": "1a3ff56a-570b-4746-cac2-07369dbe4dc9"
      },
      "source": [
        "#model = Load_pre_trained_model_embeddings(model_path)\n",
        "dataset=\"RE\"\n",
        "\n",
        "#print(\"Covert doc to embedded docs where labels= \", len(labels),\"\\n\")\n",
        "\n",
        "N=0\n",
        "documents_embeddings = embedding_all_documents(data_DF , model,N)\n",
        "labels =data_DF['class'].tolist()\n",
        "print(\"the  len of labels and docs \", len(labels),len(documents_embeddings),\"\\n\")\n",
        "\n",
        "avgdoc2v=[]\n",
        "dzero=[0] * 300\n",
        "i=0\n",
        "for d in documents_embeddings:\n",
        "  l=len(d)\n",
        "  s=sum(d)\n",
        "  if l!=0:\n",
        "    av=s/l\n",
        "    avgdoc2v.append(av)\n",
        "  else:\n",
        "    avgdoc2v.append(dzero)\n",
        "  i=i+1\n",
        "\n",
        "\n",
        "final_embeddings=avgdoc2v\n",
        "print(\"Doc vector length\", len(avgdoc2v[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:176: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "the  len of labels and docs  8491 8491 \n",
            "\n",
            "Doc vector length 300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xg9rZJ-oCgjm",
        "outputId": "b4c504fc-6274-495e-c72d-013c1889a08e"
      },
      "source": [
        "f=np.array(final_embeddings)\n",
        "f.size * f.itemsize /1024 /1024\n",
        "\n",
        "#saveList(final_embeddings,'/content/drive/MyDrive/PHD Work/avg.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9.717178344726562"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvvkM6yYpXlV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca0ce392-eee2-43ba-ad07-2073161a7e53"
      },
      "source": [
        "kernell='rbf'\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from tensorflow import keras\n",
        "layers = keras.layers\n",
        "models = keras.models\n",
        "\n",
        "def data_preperation(dataset , labels, sample):\n",
        "  x_train , x_test , y_train , y_test = train_test_split(dataset , labels , test_size = sample , random_state = 0)\n",
        "  return array(x_train) , array(x_test) , array(y_train) , array(y_test)\n",
        "\n",
        "def train_ML(x_train , y_train):\n",
        "  svm =  SVC(kernel=kernell).fit(x_train , y_train) \n",
        "  #svm =  LinearSVC().fit(x_train , y_train) \n",
        "  #MLPClassifier(random_state=1, max_iter=10000).fit(x_train , y_train)  \n",
        "  return svm\n",
        "\n",
        "#linear', 'poly', 'rbf', 'sigmoid',\n",
        "\n",
        "\n",
        "def train_RF(x_train , y_train):\n",
        "  rfc = RandomForestClassifier( n_estimators=500, max_depth=150,n_jobs=1).fit(x_train , y_train)\n",
        "  return rfc\n",
        "\n",
        "\n",
        "def train_DM(x_train, y_train):\n",
        "  batch_size = 32\n",
        "  epochs = 40\n",
        "  drop_ratio = 0.4\n",
        "   \n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Dense(1024))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.Dense(512))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.Dense(125))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.Dense(32))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.Dense(num_classes))\n",
        "  model.add(layers.Activation('softmax'))\n",
        "\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy'])\n",
        "  history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_split=0.1)\n",
        "  return model\n",
        "\n",
        "def evaluation_ML(x_test , y_test , svm_model):\n",
        "  ev=svm_model.score(x_test , y_test)\n",
        "  return ev\n",
        "def evaluation_DL(x_test , y_test , model):\n",
        "  ev=model.evaluate(x_test , y_test)\n",
        "  return ev\n",
        "\n",
        "\n",
        "final_embeddings = [array(i).reshape(-1) for i in final_embeddings]\n",
        "#labels =embedding_DF['class'].tolist() # ConvertTextLabelToNeumerical(data_DF)\n",
        "print(len(final_embeddings))\n",
        "print(len(labels))\n",
        "x_train , x_test , y_train , y_test = data_preperation(final_embeddings , labels ,0.20)\n",
        "svm = train_ML(x_train , y_train)\n",
        "ev1=evaluation_ML(x_test , y_test , svm)\n",
        "#rfc = train_RF(x_train , y_train)\n",
        "#ev2=evaluation_ML(x_test , y_test , rfc)\n",
        "print(\"Accuaracy by SVM= \", ev1)\n",
        "\n",
        "y_pred=svm.predict(x_test)\n",
        "#bal=balanced_accuracy_score(y_test,y_pred)\n",
        "#print(\"Balanced Accuaracy by SVM = \",bal, \"\\n\")\n",
        "\n",
        "print(\"saving Results to Excel:\\n\" , \"REUResults.csv\")\n",
        "'''\n",
        "import csv\n",
        "with open(r'/content/drive/MyDrive/PHD Work/Ali.csv', 'a', newline='') as csvfile:\n",
        "    fieldnames = ['data','Model','function','clusters','file','a','ML','RF','time','bal']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    writer.writerow({'file':fileName ,'data':dataset, \n",
        "                     'Model':'BOC','function':kernell,\n",
        "                     'clusters':K_clusters,'a':alpha,\n",
        "                     'ML':ev1,'RF':ev2,'time':represnationtime,\n",
        "                     'bal':bal})\n",
        "'''\n",
        "\n",
        "X =final_embeddings\n",
        "y= labels\n",
        "scores = cross_val_score(svm, X, y, cv=5, scoring='f1_weighted')\n",
        "print(scores)\n",
        "y_pred=svm.predict(x_test)\n",
        "f1 = f1_score(y_test,y_pred, average=\"weighted\")\n",
        "\n",
        "print(\"f1\", f1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8491\n",
            "8491\n",
            "Accuaracy by SVM=  0.9440847557386698\n",
            "saving Results to Excel:\n",
            " REUResults.csv\n",
            "[0.95109409 0.94412164 0.93042893 0.93789713 0.94373445]\n",
            "f1 0.9433409044196897\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeLR521ezX_Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZG4LFHp3HN_"
      },
      "source": [
        "# **TfIDF**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "eXe1awhZ-s3z",
        "outputId": "b07986aa-fe29-4d90-e367-1275106f4f8d"
      },
      "source": [
        "GN_file_path=\"/content/drive/MyDrive/PHD Work/Data/GroupNews20.csv\"\n",
        "Reuters_file_path=\"/content/drive/MyDrive/PHD Work/Data/Reuters88.csv\"\n",
        "BBC_file_path=\"/content/drive/MyDrive/PHD Work/Data/bbc-text.csv\"\n",
        "#AG_file_path=\"/content/drive/MyDrive/PHD Work/Data/AG_all_data.csv\"\n",
        "ohsumed_file_path=\"/content/drive/MyDrive/PHD Work/Data/ohsumed-allcats.csv\"\n",
        "\n",
        "\n",
        "data_DF = pd.read_csv(Reuters_file_path)\n",
        "data_DF.columns.values[0] = \"class\"\n",
        "data_DF.columns.values[1] = \"document\"\n",
        "\n",
        "data_DF['class'] = pd.factorize(data_DF['class'])[0]\n",
        "\n",
        "data_DF.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>document</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>THAI TRADE DEFICIT WIDENS IN FIRST QUARTER\\n  ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>SRI LANKA GETS USDA APPROVAL FOR WHEAT PRICE\\n...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   class                                           document\n",
              "0      0  ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RI...\n",
              "1      1  CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STO...\n",
              "2      2  JAPAN TO REVISE LONG-TERM ENERGY DEMAND DOWNWA...\n",
              "3      3  THAI TRADE DEFICIT WIDENS IN FIRST QUARTER\\n  ...\n",
              "4      1  SRI LANKA GETS USDA APPROVAL FOR WHEAT PRICE\\n..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AECSL7-7B-sU",
        "outputId": "42f77ea1-2f6e-442e-9d53-665d61a693cb"
      },
      "source": [
        "#تتضمن المعالجة المسبقة لكل مستند دون تحويله الى توكن\n",
        "# Spliting into X & y\n",
        "X=documents_as_list_of_sentences(data_DF)\n",
        "y = data_DF.iloc[:, 0].values\n",
        "\n",
        "\n",
        "# Splitting into training & test subsets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4,  random_state = 0)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 2 ... 6 6 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12fc6F_8UMor"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "def evaluation_ML(x_test , y_test , svm_model):\n",
        "  ev=svm_model.score(x_test , y_test)\n",
        "  return ev\n",
        "def evaluation_DL(x_test , y_test , model):\n",
        "  ev=model.evaluate(x_test , y_test)\n",
        "  return ev\n",
        "\n",
        "# Training the classifier & predicting on test data\n",
        "def train_ML(x_train , y_train):\n",
        "  svm =  SVC().fit(x_train , y_train) \n",
        "  return svm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ydr_9hx20cRb"
      },
      "source": [
        "## حساب حجم المتجه"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYJoUPDA0Xmm"
      },
      "source": [
        "# Building a TF IDF matrix out of the corpus of reviews\n",
        "vectorizer = TfidfVectorizer(sublinear_tf=False, max_df=0.5,  stop_words='english')\n",
        "\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iBXEMWk1Ilh",
        "outputId": "7e4936bd-6900-4c22-9093-5f281384b67a"
      },
      "source": [
        "train_tfidf = X_train.todense()\n",
        "test_tfidf = X_test.todense()\n",
        "a= train_tfidf.size * train_tfidf.itemsize \n",
        "b= test_tfidf.size * test_tfidf.itemsize\n",
        "(a+b) / 1024 /1024"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1461.9818725585938"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qx0UcUnv4VrS",
        "outputId": "cf4980de-38d9-4ca5-da48-ef4f201dd239"
      },
      "source": [
        "fileName=\"/content/drive/MyDrive/PHD Work/TFIDFRE.txt\"\n",
        "saveList(train_tfidf,fileName)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved successfully!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4ls0aaS08r7",
        "outputId": "84aa6b07-6410-46b6-e261-4c5c47821cf9"
      },
      "source": [
        "import numpy as np\n",
        "from sys import getsizeof\n",
        "a = test_tfidf\n",
        "b = np.array(a)\n",
        "print(getsizeof(a) / 1024 / 1024)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0001373291015625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CgvJKpi9Do_",
        "outputId": "16900b09-3448-43e6-874f-ec50caa5cd35"
      },
      "source": [
        "dataset=\"RE\"\n",
        "\n",
        "stime=time.time()\n",
        "# Building a TF IDF matrix out of the corpus of reviews\n",
        "vectorizer = TfidfVectorizer(sublinear_tf=False, max_df=0.8,  stop_words='english')\n",
        "\n",
        "#X_train = vectorizer.fit_transform(X_train)\n",
        "\n",
        "stime=time.time()\n",
        "\n",
        "svm = train_ML(X_train , y_train)\n",
        "\n",
        "etime=time.time()-stime\n",
        "print(\"etime\",etime,\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "etime 13.408873796463013 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSPcjaCk3WpO",
        "outputId": "17fd932d-c4d9-4519-c3fe-370af120e995"
      },
      "source": [
        "#X_test = vectorizer.transform(X_test)\n",
        "ev1=evaluation_ML(X_test , y_test , svm)\n",
        "print(\" Accuaracy by SVM = \",ev1, \"\\n\")\n",
        "\n",
        "y_pred=svm.predict(X_test)\n",
        "#bal=balanced_accuracy_score(y_test,y_pred)\n",
        "\n",
        "\n",
        "#scores = cross_val_score(svm, X, y, cv=5, scoring='f1_weighted')\n",
        "#print(scores)\n",
        "y_pred=svm.predict(X_test)\n",
        "f1 = f1_score(y_test,y_pred, average=\"weighted\")\n",
        "\n",
        "print(\"f1\", f1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Accuaracy by SVM =  0.9334707094495143 \n",
            "\n",
            "f1 0.9323160804952761\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3hhyz15JyKo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ne3Psosj-QPq",
        "outputId": "4ba0a3db-ca79-4852-d64e-3d683292a554"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11292, 75339)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gm6OtcCF_8Ra"
      },
      "source": [
        "# **BoW**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6YjKpKR9Xlp",
        "outputId": "243403b1-bd33-49f1-fc1c-e3c611341dc0"
      },
      "source": [
        "GN_file_path=\"/content/drive/MyDrive/PHD Work/Data/GroupNews20.csv\"\n",
        "Reuters_file_path=\"/content/drive/MyDrive/PHD Work/Data/Reuters88.csv\"\n",
        "BBC_file_path=\"/content/drive/MyDrive/PHD Work/Data/bbc-text.csv\"\n",
        "#AG_file_path=\"/content/drive/MyDrive/PHD Work/Data/AG_all_data.csv\"\n",
        "ohsumed_file_path=\"/content/drive/MyDrive/PHD Work/Data/ohsumed-allcats.csv\"\n",
        "\n",
        "\n",
        "data_DF = pd.read_csv(BBC_file_path)\n",
        "data_DF.columns.values[0] = \"class\"\n",
        "data_DF.columns.values[1] = \"document\"\n",
        "\n",
        "data_DF['class'] = pd.factorize(data_DF['class'])[0]\n",
        "\n",
        "data_DF.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>document</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>tv future in the hands of viewers with home th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>worldcom boss  left books alone  former worldc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   class                                           document\n",
              "0      0  tv future in the hands of viewers with home th...\n",
              "1      1  worldcom boss  left books alone  former worldc...\n",
              "2      2  tigers wary of farrell  gamble  leicester say ...\n",
              "3      2  yeading face newcastle in fa cup premiership s...\n",
              "4      3  ocean s twelve raids box office ocean s twelve..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guazcNIn9eGR"
      },
      "source": [
        "الحجم **هنا**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTtKnBb_9cY9",
        "outputId": "7803d57a-1e72-459f-9715-b880ffc5aa4f"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "X=documents_as_list_of_sentences(data_DF)\n",
        "y = data_DF.iloc[:, 0].values\n",
        "\n",
        "# Building a TF IDF matrix out of the corpus of reviews\n",
        "vectorizer = CountVectorizer( max_df=0.5,  stop_words='english')\n",
        "\n",
        "# Splitting into training & test subsets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4,  random_state = 0)\n",
        "print(y)\n",
        "\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test= vectorizer.transform(X_test)\n",
        "\n",
        "train_tfidf = X_train.todense()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 1 ... 2 3 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTOMGSzlBLTb",
        "outputId": "8c09cc2b-d057-4356-8b85-fd4441fca73c"
      },
      "source": [
        "test_tfidf = X_test.todense()\n",
        "a= train_tfidf.size * train_tfidf.itemsize \n",
        "b= test_tfidf.size * test_tfidf.itemsize\n",
        "(a+b) / 1024 /1024"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "242.06271362304688"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjwPdLp9nCnH",
        "outputId": "c7b8b55e-c719-4e04-f30c-0533c17d18c7"
      },
      "source": [
        "#تتضمن المعالجة المسبقة لكل مستند دون تحويله الى توكن\n",
        "# Spliting into X & y\n",
        "X=documents_as_list_of_sentences(data_DF)\n",
        "y = data_DF.iloc[:, 0].values\n",
        "\n",
        "\n",
        "# Splitting into training & test subsets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.4,  random_state = 0)\n",
        "print(y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 0 1 ... 2 3 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8oYb_NhAUD2V"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "def evaluation_ML(x_test , y_test , svm_model):\n",
        "  ev=svm_model.score(x_test , y_test)\n",
        "  return ev\n",
        "def evaluation_DL(x_test , y_test , model):\n",
        "  ev=model.evaluate(x_test , y_test)\n",
        "  return ev\n",
        "\n",
        "# Training the classifier & predicting on test data\n",
        "def train_ML(x_train , y_train):\n",
        "  svm =  SVC().fit(x_train , y_train)  \n",
        "  return svm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDbGlpF0oW1i",
        "outputId": "a7a81d75-cc94-4540-c4ba-259b007b12fe"
      },
      "source": [
        "dataset=\"WebKB\"\n",
        "\n",
        "stime=time.time()\n",
        "# Building a TF IDF matrix out of the corpus of reviews\n",
        "vec = CountVectorizer(max_df=0.5, lowercase=True)\n",
        "\n",
        "X_train = vec.fit_transform(X_train)\n",
        "\n",
        "stime=time.time()\n",
        "\n",
        "svm = train_ML(X_train , y_train)\n",
        "\n",
        "etime=time.time()-stime\n",
        "print(\"etime\",etime,\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "etime 3.6646416187286377 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feoErVy9_y-i",
        "outputId": "509d5f76-fc57-46c2-dafa-511b9b68864d"
      },
      "source": [
        "X_test = vec.transform(X_test)\n",
        "ev1=evaluation_ML(X_test , y_test , svm)\n",
        "\n",
        "\n",
        "y_pred=svm.predict(X_test)\n",
        "bal=balanced_accuracy_score(y_test,y_pred)\n",
        "print(\"Balanced Accuaracy by SVM = \",bal, \"\\n\")\n",
        "\n",
        "\n",
        "import csv\n",
        "with open(r'/content/drive/MyDrive/PHD Work/Ali.csv', 'a', newline='') as csvfile:\n",
        "    fieldnames = ['data','Model','function','clusters','file','a','ML','RF','time','bal']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    writer.writerow({'file':\"BOW\" ,'data':dataset, \n",
        "                     'Model':'modd','function':'rbf',\n",
        "                     'clusters':X_test.shape,'a':'maxidf05 lower',\n",
        "                     'ML':ev1,'RF':'no idf','time':etime,\n",
        "                     'bal':bal})\n",
        "\n",
        "\n",
        "\n",
        "print(\"Accuaracy by SVM= \", ev1, \"\\n\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Balanced Accuaracy by SVM =  0.7729964338736572 \n",
            "\n",
            "Accuaracy by SVM=  0.8214285714285714 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnAiOtlsAM7N",
        "outputId": "22a1f94a-3bda-42ae-e6f3-e1501d8c8e80"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11292, 75423)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CcziZjSb_ywd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Doc2Vec"
      ],
      "metadata": {
        "id": "_GpJCS6VgqXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents_as_list = make_documents_as_list(data_DF)\n",
        "\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "\n",
        "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(documents_as_list)]\n",
        "modeld2v = Doc2Vec(documents, vector_size=300, window=10, min_count=1, workers=4)"
      ],
      "metadata": {
        "id": "GrhmuJrfg1Gk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "00c7d9f0-48b1-4e5d-8af4-1bd51cb68b87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-7fbf523a3761>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdocuments_as_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_documents_as_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_DF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2vec\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTaggedDocument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mTaggedDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments_as_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'make_documents_as_list' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "final_embeddings = modeld2v.docvecs\n",
        "embedding_DF = pd.DataFrame({'class': labels, 'document': final_embeddings })\n"
      ],
      "metadata": {
        "id": "bGHGB7e9gs60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ma358FrRL5g"
      },
      "source": [
        "# **VLAC**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vV7AwjjLRPIc"
      },
      "source": [
        "#######################################################################خاص بالموديل\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "############################################################خاص بفلاس\n",
        "pip install vlac\n",
        "from vlac import VLAC\n",
        "import pickle\n",
        "\n",
        "# Training the classifier & predicting on test data\n",
        "def train_ML(x_train , y_train):\n",
        "  svm =  SVC(kernel=krenell).fit(x_train , y_train) \n",
        "  #svm =  LinearSVC().fit(x_train , y_train) \n",
        "  return svm\n",
        "def evaluation_ML(x_test , y_test , svm_model):\n",
        "  ev=svm_model.score(x_test , y_test)\n",
        "  return ev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SObFaxERUsS"
      },
      "source": [
        "#تحميل موديل مدرب على البيانات سابقا\n",
        "# او مدرب مسبقا على كوربوس\n",
        "# Contains embeddings for Reuters R8\n",
        "with open('/content/drive/MyDrive/PHD Work/DataVLAC/r8_glove_1f.pickle', 'rb') as handle:\n",
        "    model = pickle.load(handle)\n",
        "\n",
        "#model = Load_pre_trained_model_embeddings(model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzyykfHxRO9g"
      },
      "source": [
        "# Load data\n",
        "with open('/content/drive/MyDrive/PHD Work/DataVLAC/r8_docs.txt', \"r\") as f:\n",
        "    docs = f.readlines()\n",
        "with open('/content/drive/MyDrive/PHD Work/DataVLAC/r8_labels.txt') as f:\n",
        "    labels=[line for line in f]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yKf_BvBROog"
      },
      "source": [
        "# Train model and transform collection of documents\n",
        "vlac_model = VLAC(documents=docs, model=model, oov=False)\n",
        "vlac_featuress, kmeans = vlac_model.fit_transform(num_concepts=30)\n",
        "\n",
        "# Create features for new documents\n",
        "vlac_model = VLAC(documents=docs, model=model, oov=False)\n",
        "vlac_features = vlac_model.transform(kmeans=kmeans)\n",
        "\n",
        "\n",
        "#حفظ الميزات الناتجة\n",
        "fileName=\"/content/drive/MyDrive/PHD Work/Emed/VLAC/VLAC_RE8.txt\"\n",
        "saveList(vlac_features,fileName)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCQivT53T3l1",
        "outputId": "8948b9bb-ee09-49e3-e545-969fba7a40c1"
      },
      "source": [
        "fileName=\"/content/drive/MyDrive/PHD Work/VLAC vect/vlac_RE.txt.npy\"\n",
        "final_embeddings=loadList(fileName)\n",
        "len(final_embeddings)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8491"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NExTosZ1Rx9Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea4d7aae-000e-4195-ee50-bdbc93fc16f7"
      },
      "source": [
        "krenell=\"poly\"\n",
        "dataset=\"RE\"\n",
        "# Splitting into training & test subsets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "final_embeddings = [array(i).reshape(-1) for i in final_embeddings]\n",
        "print(len(final_embeddings))\n",
        "print(len(labels))\n",
        "x_train , x_test , y_train , y_test = data_preperation(final_embeddings , labels)\n",
        "svm = train_ML(x_train , y_train)\n",
        "\n",
        "stime=time.time()\n",
        "#ev1=evaluation_ML(X_test , y_test , svm)\n",
        "#etime=time.time()-stime\n",
        "#print(\"etime\",etime,\"\\n\")\n",
        "#print(\"Accuaracy by SVM= \", ev1, \"\\n\")\n",
        "\n",
        "y_pred=svm.predict(x_test)\n",
        "#bal=balanced_accuracy_score(y_test,y_pred)\n",
        "#print(\"Balanced Accuaracy by SVM = \",bal, \"\\n\")\n",
        "X =final_embeddings\n",
        "y= labels\n",
        "scores = cross_val_score(svm, X, y, cv=5, scoring='f1_weighted')\n",
        "print(scores)\n",
        "y_pred=svm.predict(x_test)\n",
        "f1 = f1_score(y_test,y_pred, average=\"weighted\")\n",
        "\n",
        "print(\"f1\", f1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8491\n",
            "8491\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhdQe6ZWqWQh"
      },
      "source": [
        "# Deep Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vo4p5pCgqxx5",
        "outputId": "c63085b6-3b53-4eea-9f07-5149a2178fd3"
      },
      "source": [
        "filename = \"/content/drive/MyDrive/PHD Work/Emed/LASTSELF/SelfGlove_maxIDF1minidf0_0_BBCSim03_ًGloVe300_100cls__02.txt.npy\"\n",
        "final_embaddings = loadList(filename)\n",
        "print(len(final_embaddings))\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "\n",
        "def evaluation_ML(x_test , y_test , svm_model):\n",
        "  print(svm_model.score(x_test , y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9CD5FMnq6P8",
        "outputId": "4fd0a9c7-07cf-4212-a9f9-6237cb39d967"
      },
      "source": [
        "final_embaddings = [array(i).reshape(-1) for i in final_embaddings]\n",
        "print(len(final_embaddings))\n",
        "print(len(labels))\n",
        "x_train , x_test , y_train , y_test = data_preperation(final_embaddings , labels, 0.3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2225\n",
            "2225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYCWoHpvrb6O",
        "outputId": "32331e5d-a849-44e1-ca16-515c279161dc"
      },
      "source": [
        "kernell= \"poly\"\n",
        "svm = train_ML(x_train , y_train)\n",
        "#evaluation_ML(x_test , y_test , svm)\n",
        "y_pred=svm.predict(x_test)\n",
        "f1_score_valur = f1_score(y_test,y_pred, average=\"weighted\")\n",
        "print(f1_score_valur)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9415980963587208\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7lrC-ytqVuY"
      },
      "source": [
        "from tensorflow import keras\n",
        "layers = keras.layers\n",
        "models = keras.models\n",
        "\n",
        "epochs= 50\n",
        "\n",
        "def train_DM(x_train, y_train, epochs):\n",
        "  batch_size = 32\n",
        "  drop_ratio = 0.2\n",
        "   \n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Dense(1024))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.Dense(512))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.Dense(125))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.Dense(32))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.Dense(num_classes)) # 5 neurons \n",
        "  model.add(layers.Activation('softmax')) # to return the class number\n",
        "\n",
        "  model.compile(loss= 'categorical_crossentropy',\n",
        "                optimizer='adam',\n",
        "                metrics= ['accuracy'])\n",
        "  history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_split=0.1)\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "def evaluation_DL(x_test , y_test , model):\n",
        "  print(model.evaluate(x_test , y_test))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PW_8QzXFsEEn",
        "outputId": "be6bbb2a-94d5-46aa-f1d4-7d897b270151"
      },
      "source": [
        "num_classes = np.max(y_train) + 1\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "print(y_train.shape)\n",
        "print(x_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1557, 5)\n",
            "(1557, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8r_USY2sGKU",
        "outputId": "74287636-451a-4bf2-9ec5-cbeec936f477"
      },
      "source": [
        "mlp = train_DM(x_train , y_train, epochs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "44/44 [==============================] - 1s 18ms/step - loss: 1.6022 - accuracy: 0.2363 - val_loss: 1.5921 - val_accuracy: 0.2244\n",
            "Epoch 2/50\n",
            "44/44 [==============================] - 1s 14ms/step - loss: 1.5227 - accuracy: 0.3055 - val_loss: 1.4214 - val_accuracy: 0.3462\n",
            "Epoch 3/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 1.2563 - accuracy: 0.4982 - val_loss: 0.9929 - val_accuracy: 0.7051\n",
            "Epoch 4/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.9317 - accuracy: 0.6345 - val_loss: 0.7803 - val_accuracy: 0.6859\n",
            "Epoch 5/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.7046 - accuracy: 0.7466 - val_loss: 0.6117 - val_accuracy: 0.8013\n",
            "Epoch 6/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.5496 - accuracy: 0.8094 - val_loss: 0.4976 - val_accuracy: 0.8077\n",
            "Epoch 7/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.5233 - accuracy: 0.8037 - val_loss: 0.3989 - val_accuracy: 0.8397\n",
            "Epoch 8/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.4109 - accuracy: 0.8522 - val_loss: 0.3717 - val_accuracy: 0.8462\n",
            "Epoch 9/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.3917 - accuracy: 0.8572 - val_loss: 0.6272 - val_accuracy: 0.7821\n",
            "Epoch 10/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.4586 - accuracy: 0.8323 - val_loss: 0.3102 - val_accuracy: 0.8782\n",
            "Epoch 11/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.3403 - accuracy: 0.8758 - val_loss: 0.3094 - val_accuracy: 0.8846\n",
            "Epoch 12/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.3509 - accuracy: 0.8687 - val_loss: 0.2572 - val_accuracy: 0.9103\n",
            "Epoch 13/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.3411 - accuracy: 0.8765 - val_loss: 0.2837 - val_accuracy: 0.9038\n",
            "Epoch 14/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.3582 - accuracy: 0.8687 - val_loss: 0.3516 - val_accuracy: 0.8654\n",
            "Epoch 15/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.3473 - accuracy: 0.8779 - val_loss: 0.3128 - val_accuracy: 0.8718\n",
            "Epoch 16/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.3050 - accuracy: 0.8844 - val_loss: 0.2413 - val_accuracy: 0.8974\n",
            "Epoch 17/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.3093 - accuracy: 0.8872 - val_loss: 0.2302 - val_accuracy: 0.9038\n",
            "Epoch 18/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.3111 - accuracy: 0.8829 - val_loss: 0.2201 - val_accuracy: 0.8974\n",
            "Epoch 19/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.3164 - accuracy: 0.8844 - val_loss: 0.3075 - val_accuracy: 0.8654\n",
            "Epoch 20/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.3160 - accuracy: 0.8822 - val_loss: 0.3827 - val_accuracy: 0.8462\n",
            "Epoch 21/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.3383 - accuracy: 0.8801 - val_loss: 0.2831 - val_accuracy: 0.8782\n",
            "Epoch 22/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.2840 - accuracy: 0.8965 - val_loss: 0.2496 - val_accuracy: 0.9295\n",
            "Epoch 23/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.2958 - accuracy: 0.8908 - val_loss: 0.2422 - val_accuracy: 0.8910\n",
            "Epoch 24/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.3001 - accuracy: 0.8894 - val_loss: 0.2495 - val_accuracy: 0.8910\n",
            "Epoch 25/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.2721 - accuracy: 0.9044 - val_loss: 0.2291 - val_accuracy: 0.9231\n",
            "Epoch 26/50\n",
            "44/44 [==============================] - 1s 14ms/step - loss: 0.2989 - accuracy: 0.8844 - val_loss: 0.2099 - val_accuracy: 0.8974\n",
            "Epoch 27/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.3927 - accuracy: 0.8565 - val_loss: 0.2647 - val_accuracy: 0.8910\n",
            "Epoch 28/50\n",
            "44/44 [==============================] - 1s 14ms/step - loss: 0.2812 - accuracy: 0.8965 - val_loss: 0.1986 - val_accuracy: 0.9038\n",
            "Epoch 29/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.2739 - accuracy: 0.9001 - val_loss: 0.2431 - val_accuracy: 0.9038\n",
            "Epoch 30/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.2724 - accuracy: 0.8965 - val_loss: 0.1963 - val_accuracy: 0.9231\n",
            "Epoch 31/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.2974 - accuracy: 0.8908 - val_loss: 0.2213 - val_accuracy: 0.9231\n",
            "Epoch 32/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.2480 - accuracy: 0.9115 - val_loss: 0.2035 - val_accuracy: 0.9167\n",
            "Epoch 33/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.2660 - accuracy: 0.9101 - val_loss: 0.3023 - val_accuracy: 0.8846\n",
            "Epoch 34/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.2961 - accuracy: 0.8829 - val_loss: 0.2036 - val_accuracy: 0.9359\n",
            "Epoch 35/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.2796 - accuracy: 0.8972 - val_loss: 0.2321 - val_accuracy: 0.9103\n",
            "Epoch 36/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.3140 - accuracy: 0.8822 - val_loss: 0.1917 - val_accuracy: 0.9038\n",
            "Epoch 37/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.2318 - accuracy: 0.9179 - val_loss: 0.1893 - val_accuracy: 0.9103\n",
            "Epoch 38/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.2462 - accuracy: 0.9094 - val_loss: 0.2583 - val_accuracy: 0.8846\n",
            "Epoch 39/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.2599 - accuracy: 0.9086 - val_loss: 0.1896 - val_accuracy: 0.9231\n",
            "Epoch 40/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.2658 - accuracy: 0.9036 - val_loss: 0.1940 - val_accuracy: 0.9487\n",
            "Epoch 41/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.2884 - accuracy: 0.8958 - val_loss: 0.2841 - val_accuracy: 0.8910\n",
            "Epoch 42/50\n",
            "44/44 [==============================] - 1s 14ms/step - loss: 0.3110 - accuracy: 0.8844 - val_loss: 0.1889 - val_accuracy: 0.9231\n",
            "Epoch 43/50\n",
            "44/44 [==============================] - 1s 14ms/step - loss: 0.2631 - accuracy: 0.9108 - val_loss: 0.1831 - val_accuracy: 0.9359\n",
            "Epoch 44/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.2351 - accuracy: 0.9193 - val_loss: 0.1891 - val_accuracy: 0.9359\n",
            "Epoch 45/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.2415 - accuracy: 0.9151 - val_loss: 0.1602 - val_accuracy: 0.9359\n",
            "Epoch 46/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.2321 - accuracy: 0.9172 - val_loss: 0.1998 - val_accuracy: 0.9295\n",
            "Epoch 47/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.2492 - accuracy: 0.9079 - val_loss: 0.1778 - val_accuracy: 0.9231\n",
            "Epoch 48/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.2338 - accuracy: 0.9101 - val_loss: 0.1904 - val_accuracy: 0.9423\n",
            "Epoch 49/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.2424 - accuracy: 0.9193 - val_loss: 0.1573 - val_accuracy: 0.9487\n",
            "Epoch 50/50\n",
            "44/44 [==============================] - 1s 13ms/step - loss: 0.2534 - accuracy: 0.9186 - val_loss: 0.2224 - val_accuracy: 0.8782\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDhsqPWEtCYl",
        "outputId": "6af22c69-b786-4084-8dfb-e94e80c63022"
      },
      "source": [
        "evDL = evaluation_DL(x_test , y_test,mlp)\n",
        "evDL"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21/21 [==============================] - 0s 4ms/step - loss: 0.2376 - accuracy: 0.9162\n",
            "[0.23761115968227386, 0.916167676448822]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2MFGycztq9R"
      },
      "source": [
        "modelpath = '/content/drive/MyDrive/PHD Work/Deep/'\n",
        "fName= 'BBC'+ str(K_clusters)+'_0'+str(alpha)[2]\n",
        "modelName=modelpath+fName+'_ep'+str(epochs)+'.h5'\n",
        "mlp.save(modelName)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYNXQD8PttwJ"
      },
      "source": [
        "from keras.models import load_model\n",
        " \n",
        "# load model\n",
        "deep_model = load_model(modelName)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r_ILFbQn4_dT"
      },
      "source": [
        "# **Deleted**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpUmSoZL40fT"
      },
      "source": [
        "epochs=40\n",
        "import csv\n",
        "\n",
        "with open(r'/content/drive/MyDrive/PHD Work/TestFinalResults.csv', 'a', newline='') as csvfile:\n",
        "    fieldnames = ['data','Model','function','clusters','file','a','ML','RF','time','bal']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    writer.writerow({'file':fileName ,'data':dataset, 'Model':modd,'function':kernell,'clusters':K_clusters,'a':alpha,'ML':ev1,'RF':ev2,'time':represnationtime,'bal':bal})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmXsg5byAX3z"
      },
      "source": [
        "## **Deleted code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxU7vUgh_toX"
      },
      "source": [
        "import numpy as np\n",
        "from sys import getsizeof\n",
        "a = final_embeddings\n",
        "b = np.array(a)\n",
        "round(getsizeof(b) / 1024 / 1024,2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2O7CoXvAi3Lj"
      },
      "source": [
        "\n",
        "'''\n",
        "#حساب الحجم وتحويلها الى مصفوفة سبارس\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "f= csr_matrix(final_embeddings)\n",
        "\n",
        "import numpy as np\n",
        "from sys import getsizeof\n",
        "#a = final_embeddings\n",
        "#b = np.array(a)\n",
        "getsizeof(f)\n",
        "\n",
        "####################\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "# Load some categories from the training set\n",
        "\n",
        "# خمسة فئات\n",
        "categories = [\n",
        "    'rec.sport.hockey',\n",
        "    'talk.religion.misc',\n",
        "    'comp.graphics',\n",
        "    'sci.space',\n",
        "    'talk.politics.guns',\n",
        "\n",
        "]\n",
        "#categories=None\n",
        "news_group = fetch_20newsgroups(subset='all', categories=categories, shuffle=True, random_state=42)\n",
        "news_group_data = news_group.data\n",
        "news_group_target_names = news_group.target_names\n",
        "news_group_target = news_group.target\n",
        "# Creating a dataframe from the loaded data\n",
        "data_DF = pd.DataFrame({'class': news_group_target, \n",
        "                        'document': news_group_data})\n",
        "                        \n",
        "##############################استخراج غروب نيوز\n",
        "# loading dataset\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "# Load some categories from the training set\n",
        "# 8 فئات\n",
        "categories1 = [\n",
        "    'rec.sport.hockey',\n",
        "    'talk.religion.misc',\n",
        "    'comp.graphics',\n",
        "    'sci.space',\n",
        "    'talk.politics.guns',\n",
        "    'sci.electronics',\n",
        "    'sci.med',\n",
        "    'rec.sport.baseball'\n",
        "]\n",
        "# خمسة فئات\n",
        "categories = [\n",
        "    'rec.sport.hockey',\n",
        "    'talk.religion.misc',\n",
        "    'comp.graphics',\n",
        "    'sci.space',\n",
        "    'talk.politics.guns',\n",
        "\n",
        "]\n",
        "categories=None\n",
        "\n",
        "news_group = fetch_20newsgroups(subset='all', categories=categories, shuffle=True, random_state=42)\n",
        "news_group_data = news_group.data\n",
        "news_group_target_names = news_group.target_names\n",
        "news_group_target = news_group.target\n",
        "# Creating a dataframe from the loaded data\n",
        "data_DF = pd.DataFrame({'class': news_group_target, \n",
        "                        'document': news_group_data})\n",
        "\n",
        "#####################################\n",
        "# معالجة بيانات رويترز\n",
        "b= [ 13,24,0,1,2,3,8,10]\n",
        "a= [x for x in range(90)]\n",
        "x=[]\n",
        "for i in a:\n",
        "  if i not in b:\n",
        "    x.append(i)\n",
        "len(x)\n",
        "data= data_DF[~data_DF['class'].isin(x)]\n",
        "#data.to_csv(r'/content/drive/MyDrive/PHD Work/Data/Reuters8.csv', index = False)\n",
        "data_DF = pd.read_csv(\"/content/drive/MyDrive/PHD Work/Data/Reuters8.csv\")\n",
        "data_DF.head()\n",
        "\n",
        "\n",
        "######################################3\n",
        "# convert class to numerical values\n",
        "def label_to_num(labeles):\n",
        "  if (labeles=='tech'):\n",
        "    labeles=0\n",
        "  if (labeles=='business'):\n",
        "    labeles=1\n",
        "  if labeles=='entertainment':\n",
        "    labeles=2\n",
        "  if labeles=='sport':\n",
        "    labeles=3\n",
        "  if labeles=='politics':\n",
        "    labeles=4\n",
        "  return labeles\n",
        "\n",
        "BBC_data['category'] = BBC_data['category'].apply(label_to_num)\n",
        "BBC_data.head()\n",
        "##################################################################\n",
        "def Ali_Representation(documents_embeddings , M ,  sim_val , alpha , centers , dictionary):\n",
        "  embeddings = []\n",
        "  counter = 0\n",
        "  IDf = np.zeros(len(centers)).tolist()\n",
        "  for i in documents_embeddings: \n",
        "    v = Doc2Vec(i, M ,sim_val , alpha , centers , dictionary)\n",
        "    ######## حساب عدد مرات ظهور المفهوم في كل نص\n",
        "    for i in v:\n",
        "      if i[M-1] > 0:\n",
        "        IDf[counter] = IDf[counter] + 1\n",
        "      counter = counter+1\n",
        "    counter = 0\n",
        "    embeddings.append(v)\n",
        "  #########  calculate IDF\n",
        "  for i in IDf:\n",
        "    i = math.log(len(embeddings) / (i+1))\n",
        "  ######## Calculate Cf-IDF\n",
        "  for i in range(len(embeddings)): \n",
        "    for j in range(len(embeddings[i])): \n",
        "       embeddings[i][j][M-1] = embeddings[i][j][M-1] * IDf[j]\n",
        "  return embeddings\n",
        "  \n",
        "#############################################\n",
        "#change last weight\n",
        "M=10 # 0 1 2 3 4\n",
        "#saved_emb=changLastCFIDF(saved_embaddings)\n",
        "#saved_emb=deleteLastCFIDF(saved_embaddings)\n",
        "#saved_emb=resize_embedding(5,2,saved_embaddings)\n",
        "saved_emb=deletFeatures(0,0,saved_embaddings)\n",
        "###################################################################\n",
        "\n",
        "if len(groups[0])>1:\n",
        "  del groups[0][0:len(groups[0])-1]\n",
        "else:\n",
        "  l0=groups[0]\n",
        "\n",
        "if len(groups[1])>1:\n",
        "  del groups[1][0:len(groups[1])-1]\n",
        "else:\n",
        "  l1=groups[1]\n",
        "\n",
        "if len(groups[2])>1:\n",
        "  del groups[2][0:len(groups[2])-1]\n",
        "else:\n",
        "  l2=groups[2]\n",
        "\n",
        "if len(groups[3])>1:\n",
        "  del groups[3][0:len(groups[3])-1]\n",
        "else:\n",
        "  del groups[3]\n",
        "\n",
        "if len(groups[4])>1:\n",
        "  del groups[4][0:len(groups[4])-1]\n",
        "else:\n",
        "  l4=groups[4]\n",
        "#print( 'l0=', l0,'l1=',l1  ,' l2 =',l2,' l3 =',l3, 'l4 =',l4 )\n",
        "\n",
        "newlabels= Convert_list_of_list_to_list(groups)\n",
        "newlabels\n",
        "\n",
        "\n",
        "#############################################\n",
        "def changLastCFIDF(saved_embaddings):\n",
        "  numOfDocs=len(saved_embaddings)\n",
        "  for d in saved_embaddings:\n",
        "    for r in d:\n",
        "      l=len(r)\n",
        "      if r[l-1]!=0:\n",
        "        r[l-1]=math.log10( (numOfDocs+1) /r[l-1])\n",
        "  return saved_embaddings\n",
        "###########################################\n",
        "#delet last weght\n",
        "def deleteLastCFIDF(saved_embaddings):\n",
        "  for d in saved_embaddings:\n",
        "    for r in d:\n",
        "      l=len(r)\n",
        "      del r[l-1]\n",
        "  return saved_embaddings\n",
        "##########################################33\n",
        "#delet last weght\n",
        "def deletFeatures(fromF,toF,saved_embaddings):\n",
        "  for d in saved_embaddings:\n",
        "    for r in d:\n",
        "      l=len(r)\n",
        "      del r[fromF:toF]\n",
        "  return saved_embaddings\n",
        "\n",
        "########################################################################################################\n",
        "# function for max similarity selection from each column\n",
        "def max_sim(similarities , M):\n",
        "    #### Removing All Zeros Rows #####\n",
        "    #start_time = time.time()\n",
        "    zero_row = np.zeros((1,M)).tolist()\n",
        "    similarities= [i for i in similarities if i != zero_row[0]]\n",
        "    #t=time.time() - start_time;\n",
        "\n",
        "    ### If no words has similarity degree with the concepts\n",
        "    if len(similarities)==0:\n",
        "      return np.zeros((1,M)).tolist()\n",
        "    else :\n",
        "      similarities = np.array(similarities)\n",
        "      N=similarities.shape[0]   \n",
        "      sim_list=[]\n",
        "      if N==1:\n",
        "          sim_list.append(similarities[0,0:M]) #####################################################\n",
        "          #return sim_list\n",
        "      else:\n",
        "          for x in range(N):\n",
        "              #sort row x in descending order\n",
        "              similarities[x,:].sort()\n",
        "              #reverse row x to get max values first\n",
        "              similarities[x,:]=np.flipud(similarities[x,:])\n",
        "          \n",
        "              #select M values from columns, column by column\n",
        "          for j in range(M):\n",
        "              for i in range(N):\n",
        "                  if len(sim_list)<M:\n",
        "                      sim_list.append(similarities[i,j]) \n",
        "                  else:\n",
        "                      #break if we got M values\n",
        "                      break;\n",
        "    return sim_list\n",
        "#########################################\n",
        "#متجه المستند بوزنين هما تردد المفهوم ومتوسط درجة تشابه كلماته مع المستند قبل تحويله الى ميزة واحدة\n",
        "def Doc2Vec(embeddeings_document,threshold_sim,alpha,centers,dictionary):\n",
        "  M=2\n",
        "  concept_freq = 0\n",
        "  ## intilize doc array\n",
        "  doc2vec = np.zeros((len(centers)).tolist()\n",
        "  for cnt in range(len(centers)):\n",
        "    counter = 0;\n",
        "    sim =[]\n",
        "    sim_array =[]\n",
        "    for emb_word in embeddeings_document:\n",
        "      Smean =Get_similarity_matrix(emb_word , dictionary[cnt]);\n",
        "      if Smean  > alpha:\n",
        "        concept_freq = concept_freq+1 #for tf\n",
        "        sim.append(Smean)\n",
        "        doc2vec[cnt][M-1] = doc2vec[cnt][M-1] + 1;\n",
        "      else:\n",
        "        continue;\n",
        "      counter = counter +1;\n",
        "    # end for doc words with one concepts\n",
        "    meanofSim=0#  المتوسط لدرجات تشابه كلمات المستند مع المفهوم الحالي\n",
        "    if(len(sim)!=0):\n",
        "      meanofSim=sum(sim)/len(sim)\n",
        "    doc2vec[cnt][M-2]=meanofSim\n",
        "    doc2vec[cnt][0:M-2] = sim_array[0:M-2]\n",
        "    del sim\n",
        "\n",
        "  if concept_freq !=0: # the number of occurence for all concept in this document\n",
        "    for i in doc2vec:\n",
        "      i[M-1] = i[M-1]*i[M-2] / concept_freq\n",
        "      #print('concept_freq',concept_freq,' tf',i[M-1])\n",
        "  #get_concept_freq(doc2vec)\n",
        "  return doc2vec\n",
        "\n",
        "###########\n",
        "#كود للاحتياط\n",
        "  #data['vector']=BBC_data['text'].apply(BoET,args=(model,centers , sorted_cons,0.44,2,10))\n",
        "\n",
        "\n",
        "def get_cluster_elements(k_means , embedding_matrix):\n",
        "  # Nice Pythonic way to get the indices of the points for each corresponding cluster\n",
        "  mydict = {i: np.where(k_means.labels_ == i)[0] for i in range(k_means.n_clusters)}\n",
        "  embeddings_samples = []\n",
        "  for i in range(len(mydict)):\n",
        "    temp= []\n",
        "    for j in mydict[i]:\n",
        "      temp.append(embedding_matrix[j])\n",
        "    embeddings_samples.append(temp)\n",
        "    del temp\n",
        "  return embeddings_samples\n",
        "\n",
        "def pre_processing_dataset(data):\n",
        "  tech = []; entertainment=[]; business=[]; politics=[]; sport=[];\n",
        "  All_tokens = [];\n",
        "  for i in range(len(data)):\n",
        "    if data.iloc[i][0] == 'tech':\n",
        "      tech.append(pre_processing(data.iloc[i][1]))\n",
        "    if data.iloc[i][0] == 'entertainment':\n",
        "     entertainment.append(pre_processing(data.iloc[i][1]))\n",
        "    if data.iloc[i][0] == 'business':\n",
        "     business.append(pre_processing(data.iloc[i][1]))\n",
        "    if data.iloc[i][0] == 'politics':\n",
        "     politics.append(pre_processing(data.iloc[i][1]))\n",
        "    if data.iloc[i][0] == 'sport':\n",
        "     sport.append(pre_processing(data.iloc[i][1]))\n",
        "\n",
        "  All_tokens = tech + entertainment +  business + politics + sport\n",
        "  All_tokens = Convert_list_of_list_to_list(All_tokens)\n",
        "\n",
        "  print('number of technolgy samples' , len(tech))\n",
        "  print('number of entertainment samples' , len(entertainment) )\n",
        "  print('number of business samples' , len(business))\n",
        "  print('number of politics samples' , len(politics))\n",
        "  print('number of sport samples' , len(sport))\n",
        "  print('the number of words in Crorpus' , len(All_tokens))\n",
        "   \n",
        "  return tech , entertainment ,  business , politics , sport , All_tokens\n",
        "\n",
        "\n",
        "def ConvertTextLabelToNeumerical(df):\n",
        "  labels = []\n",
        "  for i in df['class']:\n",
        "    if i == 'tech':\n",
        "      labels.append(0)\n",
        "    elif i =='business':\n",
        "      labels.append(1)\n",
        "    elif i == 'sport':\n",
        "      labels.append(2)\n",
        "    elif i =='entertainment':\n",
        "      labels.append(3)\n",
        "    elif i =='politics':\n",
        "      labels.append(4)\n",
        "  return labels\n",
        "  \n",
        "\n",
        "  def ConvertTextLabelToNeumerical(df):\n",
        "  labels = []\n",
        "  for i in df['class']:\n",
        "    labels.append(i)\n",
        "  return labels\n",
        "  '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv20cox_cYam"
      },
      "source": [
        ""
      ]
    }
  ]
}