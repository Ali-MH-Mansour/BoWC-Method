{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ali-MH-Mansour/BoWC-Method/blob/main/codes/ORG_%D0%9C%D0%BE%D0%B4%D1%83%D0%BB%D1%8C_%D0%BE%D0%BF%D1%82%D0%B8%D0%BC%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D0%B8_%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D1%8B_%D0%BA%D0%BB%D0%B0%D1%81%D1%81%D0%B8%D1%84%D0%B8%D0%BA%D0%B0%D1%82%D0%BE%D1%80%D0%B0_%D0%BF%D1%80%D0%B8_%D0%B2%D0%B5%D0%BA%D1%82%D0%BE%D1%80%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D0%B8_%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%B0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Модуль оптимизации работы классификатора при векторизации текста\",\n"
      ],
      "metadata": {
        "id": "F8ixORMH068q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gge2NxpEmGsT"
      },
      "source": [
        "# **Install requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOH5gupuRUiH",
        "outputId": "27e16615-59b7-4046-8c6e-4e1be5fdffc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spherecluster\n",
            "  Downloading spherecluster-0.1.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from spherecluster) (1.23.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from spherecluster) (1.11.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.10/dist-packages (from spherecluster) (1.2.2)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from spherecluster) (7.4.3)\n",
            "Collecting nose (from spherecluster)\n",
            "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20->spherecluster) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20->spherecluster) (3.2.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->spherecluster) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest->spherecluster) (23.2)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->spherecluster) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->spherecluster) (1.1.3)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->spherecluster) (2.0.1)\n",
            "Installing collected packages: nose, spherecluster\n",
            "Successfully installed nose-1.3.7 spherecluster-0.1.7\n",
            "Collecting soyclustering\n",
            "  Downloading soyclustering-0.2.0-py3-none-any.whl (28 kB)\n",
            "Requirement already satisfied: numpy>=1.1 in /usr/local/lib/python3.10/dist-packages (from soyclustering) (1.23.5)\n",
            "Installing collected packages: soyclustering\n",
            "Successfully installed soyclustering-0.2.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "#################################### Importing all Library here  ################################################\n",
        "!pip install spherecluster\n",
        "!pip install soyclustering\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from gensim.models import FastText\n",
        "from soyclustering import SphericalKMeans\n",
        "from sklearn.cluster import KMeans\n",
        "import string\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import array\n",
        "from collections import Counter\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.sparse import csr_matrix\n",
        "##################################\n",
        "import gensim\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "################################\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize, sent_tokenize, pos_tag\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yPskImTLE1Q"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# list of stop words from many resources\n",
        "my_stopwords= [\"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\",\"aab\",'aaab', \"able\", \"about\", \"above\", \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\", \"affected\", \"affecting\", \"affects\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"ain't\", \"aj\", \"al\", \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\", \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appear\", \"appreciate\", \"appropriate\", \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"aren't\", \"arise\", \"around\", \"as\", \"a's\", \"aside\", \"ask\", \"asking\", \"associated\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"b1\", \"b2\", \"b3\", \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"best\", \"better\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\", \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\", \"can't\", \"cause\", \"causes\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"changes\", \"ci\", \"cit\", \"cj\", \"cl\", \"clearly\", \"cm\", \"c'mon\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\", \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"could\", \"couldn\", \"couldnt\", \"couldn't\", \"course\", \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"c's\", \"ct\", \"cu\", \"currently\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"d2\", \"da\", \"date\", \"dc\", \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"didn't\", \"different\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"done\", \"don't\", \"down\", \"downwards\", \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\", \"effect\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"empty\", \"en\", \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\", \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\", \"except\", \"ey\", \"f\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\", \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\", \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\", \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"h2\", \"h3\", \"had\", \"hadn\", \"hadn't\", \"happens\", \"hardly\", \"has\", \"hasn\", \"hasnt\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"hed\", \"he'd\", \"he'll\", \"hello\", \"help\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"here's\", \"hereupon\", \"hers\", \"herself\", \"hes\", \"he's\", \"hh\", \"hi\", \"hid\", \"him\", \"himself\", \"his\", \"hither\", \"hj\", \"ho\", \"home\", \"hopefully\", \"how\", \"howbeit\", \"however\", \"how's\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\", \"ibid\", \"ic\", \"id\", \"i'd\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"i'll\", \"im\", \"i'm\", \"immediate\", \"immediately\", \"importance\", \"important\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\", \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"invention\", \"inward\", \"io\", \"ip\", \"iq\", \"ir\", \"is\", \"isn\", \"isn't\", \"it\", \"itd\", \"it'd\", \"it'll\", \"its\", \"it's\", \"itself\", \"iv\", \"i've\", \"ix\", \"iy\", \"iz\", \"j\", \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"know\", \"known\", \"knows\", \"ko\", \"l\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\", \"less\", \"lest\", \"let\", \"lets\", \"let's\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ll\", \"ln\", \"lo\", \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\", \"may\", \"maybe\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mightn't\", \"mill\", \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\", \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"n\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\", \"ne\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needn\", \"needn't\", \"needs\", \"neither\", \"never\", \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\", \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"nothing\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\", \"nt\", \"ny\", \"o\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\", \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\", \"ord\", \"os\", \"ot\", \"other\", \"others\", \"otherwise\", \"ou\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\", \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\", \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\", \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"pq\", \"pr\", \"predominantly\", \"present\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\", \"pt\", \"pu\", \"put\", \"py\", \"q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\", \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\", \"related\", \"relatively\", \"research\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\", \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"s2\", \"sa\", \"said\", \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"see\", \"seeing\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\", \"several\", \"sf\", \"shall\", \"shan\", \"shan't\", \"she\", \"shed\", \"she'd\", \"she'll\", \"shes\", \"she's\", \"should\", \"shouldn\", \"shouldn't\", \"should've\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"significant\", \"significantly\", \"similar\", \"similarly\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\", \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\", \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\", \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"system\", \"sz\", \"t\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\", \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"thats\", \"that's\", \"that've\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\", \"thereof\", \"therere\", \"theres\", \"there's\", \"thereto\", \"thereupon\", \"there've\", \"these\", \"they\", \"theyd\", \"they'd\", \"they'll\", \"theyre\", \"they're\", \"they've\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\", \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\", \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\", \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"t's\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\", \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\", \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\", \"ut\", \"v\", \"va\", \"value\", \"various\", \"vd\", \"ve\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\", \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"wa\", \"want\", \"wants\", \"was\", \"wasn\", \"wasnt\", \"wasn't\", \"way\", \"we\", \"wed\", \"we'd\", \"welcome\", \"well\", \"we'll\", \"well-b\", \"went\", \"were\", \"we're\", \"weren\", \"werent\", \"weren't\", \"we've\", \"what\", \"whatever\", \"what'll\", \"whats\", \"what's\", \"when\", \"whence\", \"whenever\", \"when's\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"wheres\", \"where's\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\", \"who\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whom\", \"whomever\", \"whos\", \"who's\", \"whose\", \"why\", \"why's\", \"wi\", \"widely\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"won't\", \"words\", \"world\", \"would\", \"wouldn\", \"wouldnt\", \"wouldn't\", \"www\", \"x\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\", \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"you'd\", \"you'll\", \"your\", \"youre\", \"you're\", \"yours\", \"yourself\", \"yourselves\", \"you've\", \"yr\", \"ys\", \"yt\", \"z\", \"zero\", \"zi\", \"zz\"]\n",
        "\n",
        "# preprocessing functions\n",
        "def preprocess_text(text):\n",
        "    # 1. Tokenise to alphabetic tokens\n",
        "    text = remove_numbers(text)\n",
        "    text = remove_punctuation(text)\n",
        "    text = convert_to_lower(text)\n",
        "    text = remove_white_space(text)\n",
        "    text = remove_short_words(text)\n",
        "    tokens = toknizing(text)\n",
        "    # 3. Lowercase and lemmatise\n",
        "    lemmatiser = WordNetLemmatizer()\n",
        "    #tokens = [lemmatiser.lemmatize(t.lower(), pos=pos_map.get(p[0], 'v')) for t, p in pos_tags]\n",
        "    tokens = [lemmatiser.lemmatize(t.lower()) for t in tokens]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "#--------------------------------------------------------------------------------------------\n",
        "\n",
        "def convert_to_lower(text):\n",
        "  return text.lower()\n",
        "#--------------------------------------------------------------------------------------------\n",
        "def remove_numbers(text):\n",
        "  text = re.sub(r'\\d+' , '', text)\n",
        "  return text\n",
        "#--------------------------------------------------------------------------------------------\n",
        "\n",
        "def remove_short_words(text):\n",
        "  text = re.sub(r'\\b\\w{1,2}\\b', '', text)\n",
        "  return text\n",
        "#--------------------------------------------------------------------------------------------\n",
        "\n",
        "def remove_punctuation(text):\n",
        "     punctuations = '''!()[]{};:'\"\\,<>./?@#$%^+&*_~'''\n",
        "     no_punct = \"\"\n",
        "     for char in text:\n",
        "        if char not in punctuations:\n",
        "            no_punct = no_punct + char\n",
        "        elif char==\"-\":\n",
        "            no_punct = no_punct + \" \"\n",
        "     return no_punct\n",
        "#--------------------------------------------------------------------------------------------\n",
        "def remove_white_space(text):\n",
        "  text = text.strip()\n",
        "  return text\n",
        "\n",
        "#--------------------------------------------------------------------------------------------\n",
        "def toknizing(text):\n",
        "  stop_words = set(my_stopwords) #stopwords.words('english')\n",
        "  tokens = word_tokenize(text)\n",
        "  ## Remove Stopwords from tokens\n",
        "  result = [i for i in tokens if not i in stop_words]\n",
        "  return result\n",
        "#--------------------------------------------------------------------------------------------\n",
        "def remove_duplication(text):\n",
        "  return list(set(text))\n",
        "\n",
        "# clustering\n",
        "def spherical_kmeans(n_clusters_, embeddings_matrix,sim_digree):\n",
        "  embeddings_matrix_csr = csr_matrix(embeddings_matrix)\n",
        "  spherical_kmeans = SphericalKMeans( max_similar=sim_digree, init='similar_cut',\n",
        "      n_clusters = n_clusters_)\n",
        "  labels = spherical_kmeans.fit_predict(embeddings_matrix_csr)\n",
        "  centers = spherical_kmeans.cluster_centers_\n",
        "  return labels , centers\n",
        "\n",
        "\n",
        "def clusters_properties(k_means):\n",
        "  print(Counter(k_means.labels_))\n",
        "  centers = k_means.cluster_centers_\n",
        "  '''\n",
        "  Return\n",
        "     - Number of samples for each cluter ,\n",
        "     - The Centers of each cluster\n",
        "  '''\n",
        "  return Counter(k_means.labels_) , centers\n",
        "\n",
        "\n",
        "#-----------------------------------------------------------------------------------\n",
        "\n",
        "# function to extract doc keywords\n",
        "def kw_by_TFIDF(df, topN=200,ngram=(1,1),maxdf=0.9,minDF=1):\n",
        "  doc_as_list_keywords = []\n",
        "  vectorizer = TfidfVectorizer(use_idf=True, max_df=maxdf,min_df=minDF, ngram_range=ngram)\n",
        "  vectors = vectorizer.fit_transform(data_DF['document'])\n",
        "\n",
        "  index_value={i[1]:i[0] for i in vectorizer.vocabulary_.items()}\n",
        "\n",
        "  ngram_keywords = []  # all deoc vectors by tfidf\n",
        "  for row in vectors:\n",
        "    ngram_keywords.append({index_value[column]:value for (column,value) in zip(row.indices,row.data)})\n",
        "\n",
        "  doc_sorted_tfidfs =[]  # list of doc features each with tf weight\n",
        "  #sort each dict of a document\n",
        "  for dn in ngram_keywords:\n",
        "    newD = sorted(dn.items(), key=lambda x: x[1], reverse=True)\n",
        "    newD = dict(newD)\n",
        "    doc_sorted_tfidfs.append(newD)\n",
        "\n",
        "\n",
        "  for doc_tfidf in doc_sorted_tfidfs:\n",
        "\n",
        "    ll = list(doc_tfidf.keys())\n",
        "    doc_as_list_keywords.append(ll[0:topN])\n",
        "\n",
        "  return doc_as_list_keywords\n",
        "\n",
        "\n",
        "# ---------------------------------\n",
        "def represent_documents(documents_as_list_of_tokens, doc_as_kw, model):\n",
        "    dic_w_v = {}\n",
        "    documents_embeddings = []\n",
        "    for i, doc in enumerate(documents_as_list_of_tokens):    #or doc_as_kw | documents_as_list_of_tokens\n",
        "      doc_emb = []\n",
        "      for w in doc:\n",
        "        try :\n",
        "          emb = model.wv[w]  #model.word_vectors[model.dictionary[w]]  #\n",
        "          doc_emb.append(emb)\n",
        "          if w in doc_as_kw[i]:\n",
        "            dic_w_v.update({w : emb})\n",
        "        except:\n",
        "          continue;\n",
        "      documents_embeddings.append(doc_emb)\n",
        "    return documents_embeddings, dic_w_v\n",
        "\n",
        "def data_preperation(dataset , labels, sample):\n",
        "  x_train , x_test , y_train , y_test = train_test_split(dataset , labels , test_size = sample , random_state = 0)\n",
        "  return array(x_train) , array(x_test) , array(y_train) , array(y_test)\n",
        "\n",
        "\n",
        "def embedding_BOWC(documents_embeddings, centers, theta, K_clusters):\n",
        "  doc_emb = []\n",
        "\n",
        "  for i,doc in enumerate(documents_embeddings):\n",
        "    v = np.zeros((K_clusters))\n",
        "    if len(doc)>0: # if the doc contain emebded words\n",
        "      concept_word_sim = cosine_similarity(np.array(centers) , np.array(doc)) # array (concepts * document words)\n",
        "\n",
        "      similar_concept = np.count_nonzero(concept_word_sim> theta, axis=1)\n",
        "      if sum(similar_concept)==0:\n",
        "        CF = np.zeros((K_clusters)).tolist() # if no concept similar to doc words\n",
        "      else:\n",
        "        CF = [cf/sum(similar_concept) for cf in similar_concept]\n",
        "      v =[round(CF[i]*math.exp(concept_word_sim[i].max()),3) for i in range(K_clusters)]\n",
        "\n",
        "    doc_emb.append(v)\n",
        "  return doc_emb\n",
        "\n",
        "#Calculate Cf-IDF\n",
        "def CF_EDF(doc_vectors,IDF):\n",
        "  docL=len(doc_vectors[0])\n",
        "  for i in range(len(doc_vectors)):\n",
        "    for j in range(docL):\n",
        "       doc_vectors[i][j]  = round(doc_vectors[i][j] * IDF[j], 5)\n",
        "  return doc_vectors\n",
        "\n",
        "def get_IDF(DF, N):\n",
        "  EDF =[]\n",
        "  for i in DF:\n",
        "    if i>0: EDF.append(math.exp(-1*(i/N)))\n",
        "    else: EDF.append(0)\n",
        "  return EDF\n",
        "\n",
        "\n",
        "def bowc_vectorizer(theta, documents_embeddings, centers, K_clusters):\n",
        "\n",
        "  TF_SIM = embedding_BOWC(documents_embeddings, centers, theta, K_clusters)\n",
        "\n",
        "  #DF get the number of documents that contain each concept\n",
        "  DF = np.count_nonzero(np.array(TF_SIM)> 0, axis=0)\n",
        "  N = len(TF_SIM)\n",
        "  #---------------- get EDF\n",
        "  EDF = get_IDF(DF, N)\n",
        "  # Multibly CF EDF\n",
        "  final_embeddings = CF_EDF(TF_SIM,EDF)\n",
        "\n",
        "  return final_embeddings\n",
        "\n",
        "#-----------------------------------------------------\n",
        "def classify_data(final_embeddings, svm_kernell):\n",
        "  try:\n",
        "    res =[]\n",
        "    #final_embeddings = [array(i).reshape(-1) for i in final_embeddings]\n",
        "    labels = data_DF['class'].tolist()\n",
        "    x_train , x_test , y_train , y_test = data_preperation(final_embeddings , labels , 0.30)\n",
        "    # kr = [\"poly\",\"linear\", \"rbf\"]\n",
        "\n",
        "    svm =  SVC(kernel=svm_kernell, random_state=44).fit(x_train , y_train)\n",
        "    y_pred=svm.predict(x_test)\n",
        "    F1 = f1_score(y_test,y_pred, average=\"micro\")\n",
        "\n",
        "    return F1\n",
        "\n",
        "  except:\n",
        "    return \"error in training\"\n",
        "\n",
        "\n",
        "def clustering_data(final_embeddings: list, data_DF):\n",
        "    labels =data_DF['class'].tolist()\n",
        "    true_k = np.unique(labels).shape[0]\n",
        "    X_data =array(final_embeddings)\n",
        "    km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100,tol=0.000001)\n",
        "    km.fit(X_data)\n",
        "    H,C,V = metrics.homogeneity_completeness_v_measure(labels, km.labels_)\n",
        "    adj = metrics.adjusted_rand_score(labels, km.labels_)\n",
        "    return V"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload Data"
      ],
      "metadata": {
        "id": "s-pO_c4so8lK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "rnb1TFbGkhct",
        "outputId": "9b0f89cd-5f91-480e-8f7e-34aa3435600a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   class                                           document      class_new\n",
              "0      0  tv future in the hands of viewers with home th...           tech\n",
              "1      1  worldcom boss  left books alone  former worldc...       business\n",
              "2      2  tigers wary of farrell  gamble  leicester say ...          sport\n",
              "3      2  yeading face newcastle in fa cup premiership s...          sport\n",
              "4      3  ocean s twelve raids box office ocean s twelve...  entertainment"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3526fde6-bc3a-4450-b875-94340432b147\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>document</th>\n",
              "      <th>class_new</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>tv future in the hands of viewers with home th...</td>\n",
              "      <td>tech</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>worldcom boss  left books alone  former worldc...</td>\n",
              "      <td>business</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
              "      <td>sport</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
              "      <td>sport</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
              "      <td>entertainment</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3526fde6-bc3a-4450-b875-94340432b147')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3526fde6-bc3a-4450-b875-94340432b147 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3526fde6-bc3a-4450-b875-94340432b147');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-acfeb90f-2586-4c70-bcfd-721f76bb89ec\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-acfeb90f-2586-4c70-bcfd-721f76bb89ec')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-acfeb90f-2586-4c70-bcfd-721f76bb89ec button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Copy the data file 'bbc-text.csv' to the drive, then copy the link and store it in the variable BBC_file_path\n",
        "BBC_file_path=\"/content/bbc-text (6).csv\"\n",
        "\n",
        "data_DF = pd.read_csv(BBC_file_path)\n",
        "\n",
        "# rename columns\n",
        "data_DF.columns.values[0] = \"class\"\n",
        "data_DF.columns.values[1] = \"document\"\n",
        "\n",
        "# Factorize class names to numbers\n",
        "data_DF['class_new'] = data_DF['class']\n",
        "\n",
        "data_DF['class'] = pd.factorize(data_DF['class'])[0]\n",
        "data_DF.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "K_clusters= 200\n",
        "sim_digree=0.3\n",
        "number_of_words_per_cluster=20\n",
        "theta = 0.6\n",
        "topN_KW = 200\n",
        "max_idf  = 0.9\n",
        "words_count= 20 # per cluster"
      ],
      "metadata": {
        "id": "WePzSTy5pjks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aT8qQAUT_Lf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81d4a3ae-287e-4676-917a-b0ead093c7da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The clusters details Counter({194: 46, 189: 44, 17: 42, 85: 41, 42: 41, 5: 36, 136: 35, 163: 33, 62: 33, 67: 29, 108: 27, 119: 27, 120: 26, 96: 26, 109: 25, 44: 25, 199: 24, 87: 24, 72: 24, 26: 24, 73: 24, 143: 23, 43: 23, 22: 23, 160: 23, 125: 22, 59: 22, 111: 22, 158: 22, 140: 22, 63: 22, 144: 21, 13: 21, 81: 21, 174: 21, 89: 21, 28: 21, 196: 20, 162: 20, 168: 20, 167: 20, 155: 20, 128: 19, 191: 19, 50: 19, 135: 19, 21: 19, 92: 19, 64: 19, 98: 19, 169: 19, 115: 18, 70: 18, 11: 18, 93: 18, 110: 18, 161: 18, 121: 17, 94: 17, 141: 17, 179: 17, 79: 17, 187: 17, 78: 16, 152: 16, 61: 16, 55: 16, 177: 16, 29: 16, 68: 16, 148: 16, 95: 15, 183: 15, 77: 14, 105: 14, 25: 14, 147: 14, 132: 14, 116: 14, 53: 13, 151: 13, 103: 13, 8: 13, 58: 13, 47: 13, 131: 13, 165: 13, 190: 13, 10: 13, 166: 13, 186: 12, 156: 12, 36: 12, 34: 12, 193: 12, 153: 12, 173: 12, 66: 12, 164: 12, 1: 12, 71: 12, 106: 11, 142: 11, 15: 11, 12: 11, 37: 11, 86: 11, 198: 11, 154: 11, 56: 11, 122: 11, 180: 11, 32: 11, 133: 11, 40: 10, 130: 10, 33: 10, 69: 10, 182: 10, 126: 10, 49: 10, 150: 10, 39: 10, 117: 10, 195: 10, 114: 9, 27: 9, 14: 9, 19: 9, 176: 9, 3: 9, 35: 9, 138: 9, 100: 9, 197: 9, 101: 9, 57: 8, 80: 8, 46: 8, 6: 8, 170: 8, 88: 8, 188: 8, 107: 8, 30: 8, 159: 7, 54: 7, 83: 7, 145: 7, 31: 7, 97: 7, 7: 7, 146: 7, 185: 6, 9: 6, 45: 6, 38: 6, 41: 6, 0: 6, 16: 6, 104: 6, 18: 6, 84: 6, 137: 5, 76: 5, 181: 5, 90: 5, 113: 5, 157: 5, 74: 5, 24: 5, 178: 5, 175: 5, 123: 5, 139: 5, 184: 5, 20: 4, 172: 4, 192: 4, 65: 4, 82: 4, 102: 4, 23: 4, 52: 4, 112: 4, 51: 3, 48: 3, 134: 3, 91: 3, 149: 3, 4: 3, 118: 3, 129: 2, 75: 2, 124: 2, 171: 2, 2: 1, 99: 1, 60: 1, 127: 1}) \n",
            "\n",
            "CPU times: user 3min 4s, sys: 1.34 s, total: 3min 6s\n",
            "Wall time: 2min 3s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "data_DF['document'] = data_DF.document.apply(lambda x: ' '.join(preprocess_text(x)))\n",
        "documents_as_list_of_tokens = [d.split() for d in data_DF['document']]\n",
        "model = FastText(documents_as_list_of_tokens, vector_size=300, window=15, min_count=10, workers=4,sg=1)\n",
        "doc_as_kw =kw_by_TFIDF(data_DF['document'], topN_KW,(1,1),max_idf,words_count)\n",
        "documents_embeddings, dic_w_v = represent_documents(documents_as_list_of_tokens,doc_as_kw, model)\n",
        "embeddings_matrix = list(dic_w_v.values())\n",
        "labels , centers = spherical_kmeans(K_clusters , embeddings_matrix, sim_digree)\n",
        "print(\"The clusters details\", Counter(labels), \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# text Vectorization\n",
        "final_embeddings = bowc_vectorizer(theta, documents_embeddings, centers, K_clusters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyrMEBz0JAUs",
        "outputId": "4f1eb9fd-ef1d-467d-ae3d-409f46c0b89a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification accuracy by F-measure 0.9580838323353293\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clustering accuracy by v-measure 0.7569820735016624\n",
            "CPU times: user 38.5 s, sys: 23.4 s, total: 1min 1s\n",
            "Wall time: 1min\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# text Classification\n",
        "#rbf linear\n",
        "print('Classification accuracy by F-measure', classify_data(final_embeddings, svm_kernell=\"rbf\"))\n",
        "# text clustering\n",
        "print('Clustering accuracy by v-measure', clustering_data(final_embeddings, data_DF))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRm8Q11EqZCq",
        "outputId": "cfb2804c-ef2a-4966-cfda-5b04d73bb39f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification accuracy by F-measure 0.9580838323353293\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clustering accuracy by v-measure 0.7569820735016624\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}